<!doctype html><html lang=zh-CN data-theme=dark><head><meta charset=UTF-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.121.1"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="多元统计之聚类分析"><meta itemprop=description content="cluster-analysis"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="/img/avatar.jpg"><meta itemprop=keywords content="数理统计"><meta property="og:type" content="article"><meta property="og:title" content="多元统计之聚类分析"><meta property="og:description" content="cluster-analysis"><meta property="og:image" content="/img/avatar.jpg"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="/post/cluster-analysis/"><meta property="og:site_name" content="Patrick's Blog"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="Patrick"><meta property="article:published_time" content="2019-02-17 22:09:27 +0800 CST"><meta property="article:modified_time" content="2024-04-04 09:38:49 +0800 CST"><link type=text/css rel=stylesheet href=https://unpkg.com/@fortawesome/fontawesome-free@6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://unpkg.com/animate.css@3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://unpkg.com/viewerjs@1.11.0/dist/viewer.min.css><link rel=stylesheet href=/css/main.min.7278591c5c8ad9bb2f95d284d0f942ad39102c85f7a0cb49fe948e43b311ce51.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":true,"isHome":false,"isPage":true,"math":{"css":{"file":"dist/katex.min.css","name":"katex","version":"0.16.0"},"js":[{"file":"dist/katex.min.js","name":"katex","version":"0.16.0"},{"alias_name":"katex","file":"dist/contrib/auto-render.min.js","name":"auto-render","version":"0.16.0"}],"render":"katex"},"path":"cluster-analysis","permalink":"/post/cluster-analysis/","title":"多元统计之聚类分析","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>多元统计之聚类分析 - Patrick's Blog</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Patrick's Blog</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Once start, goes forward!</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about/ class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档
<span class=badge>33</span></a></li><li class="menu-item menu-item-commonweal"><a href=/404.html class=hvr-icon-pulse rel=section><i class="fa fa-heartbeat hvr-icon"></i>公益 404</a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#概述>概述</a><ul><li><a href=#类别>类别</a><ul><li><a href=#q型聚类>Q型聚类</a></li><li><a href=#r型聚类>R型聚类</a></li></ul></li><li><a href=#分类与聚类区别>分类与聚类区别</a><ul><li><a href=#分类>分类</a></li><li><a href=#聚类>聚类</a></li></ul></li></ul></li><li><a href=#方法分类>方法分类</a><ul><li><a href=#系统聚类法>系统聚类法</a></li><li><a href=#非系统聚类法>非系统聚类法</a><ul><li><a href=#k-均值聚类法k-means-cluster>K-均值聚类法K-means Cluster</a></li></ul></li></ul></li><li><a href=#相似性的度量>相似性的度量</a><ul><li><a href=#样品相似性的度量>样品相似性的度量</a></li><li><a href=#变量相似性的度量>变量相似性的度量</a><ul><li><a href=#夹角余弦>夹角余弦</a></li><li><a href=#相关系数>相关系数</a></li></ul></li></ul></li><li><a href=#类间距与系统聚类法>类间距与系统聚类法</a><ul><li><a href=#最短距离法>最短距离法</a></li><li><a href=#最长距离法>最长距离法</a></li><li><a href=#中间距离法>中间距离法</a></li><li><a href=#重心法>重心法</a></li><li><a href=#类平均法>类平均法</a></li><li><a href=#可变类平均法>可变类平均法</a></li><li><a href=#可变法>可变法</a></li><li><a href=#离差平方和法ward法>离差平方和法(Ward法)</a></li></ul></li><li><a href=#类间距离的统一性>类间距离的统一性</a></li><li><a href=#类个数的确定>类个数的确定</a></li><li><a href=#系统聚类法在r上的实现>系统聚类法在R上的实现</a><ul><li><a href=#示例>示例</a></li></ul></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=Patrick src=/imgs/img-lazy-loading.gif data-src=/img/avatar.jpg><p class=site-author-name itemprop=name>Patrick</p><div class=site-description itemprop=description>个人学习笔记</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>33</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>4</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>8</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/img-lazy-loading.gif data-src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>
友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://gitee.com/hugo-next/hugo-theme-next title=https://gitee.com/hugo-next/hugo-theme-next target=_blank>Hugo-NexT</a></li></ul></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate=2015-04-05T13:57:58+08:00></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>总访客数：</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>页面浏览：</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=22332></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=121></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate=2023-05-31T21:26:21+08:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=goto-comments class="button goto-comments" title=直达评论><i class="fas fa-comments"></i></div><div id=goto-gtranslate class=button title=多语言翻译><i class="fas fa-globe"></i></div><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><a role=button class="book-mark-link book-mark-link-fixed"></a><script type=text/javascript src=//sidecar.gitter.im/dist/sidecar.v1.js async></script><script type=text/javascript>((window.gitter={}).chat={}).options={room:"hugo-next/community"}</script><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=/post/cluster-analysis/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/img/avatar.jpg"><meta itemprop=name content="Patrick"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="Patrick"><meta itemprop=description content="个人学习笔记"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="多元统计之聚类分析"><meta itemprop=description content="cluster-analysis"></span><header class=post-header><h1 class=post-title itemprop="name headline">多元统计之聚类分析</h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i>
</span><span class=post-meta-item-text title=发表于>发表于：
</span><time title="创建时间：2019-02-17 22:09:27 +0800 CST" itemprop="dateCreated datePublished" datetime="2019-02-17 22:09:27 +0800 CST">2019-02-17
</time></span><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar-check"></i>
</span><span class=post-meta-item-text title=更新于>更新于：
</span><time title=修改时间：2024-04-04T09:38:49+08:00 itemprop=dateModified datetime=2024-04-04T09:38:49+08:00>2024-04-04</time>
</span><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i>
</span><span class=post-meta-item-text title=分类于>分类于：
</span><span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6 itemprop=url rel=index><span itemprop=name>数据科学</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i>
</span><span class=post-meta-item-text>字数：</span>
<span>655</span>
</span><span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i>
</span><span class=post-meta-item-text>阅读：&ap;</span>
<span>4分钟</span>
</span><span class=post-meta-item title=浏览><span class=post-meta-item-icon><i class="far fa-eye"></i>
</span><span class=post-meta-item-text>浏览：
</span><span id=busuanzi_value_page_pv data-path=/post/cluster-analysis/><i class="fa fa-sync fa-spin"></i>
</span></span><span class=post-meta-item title><span class=post-meta-item-icon><i class="far fa-comments"></i>
</span><span class=post-meta-item-text title=评论>评论：
</span><span id=comments-count class=waline-comment-count data-path=/post/cluster-analysis/><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class="post-body autonumber" itemprop=articleBody><h2 id=概述>概述
<a class=header-anchor href=#%e6%a6%82%e8%bf%b0></a></h2><ul><li>聚类分析是研究如何将研究对象按照多个方面的特征进行综合分类的一种统计方法</li><li>聚类分析就是分析如何对样品(或变量)按照他们在性质上的亲疏程度进行量化分类的问题</li><li>聚类分析有效解决了科学研究中多因素、多指标的分类问题</li></ul><h3 id=类别>类别
<a class=header-anchor href=#%e7%b1%bb%e5%88%ab></a></h3><h4 id=q型聚类>Q型聚类
<a class=header-anchor href=#q%e5%9e%8b%e8%81%9a%e7%b1%bb></a></h4><ul><li>对样品进行分类处理</li></ul><h4 id=r型聚类>R型聚类
<a class=header-anchor href=#r%e5%9e%8b%e8%81%9a%e7%b1%bb></a></h4><ul><li>对变量进行分类处理</li></ul><h3 id=分类与聚类区别>分类与聚类区别
<a class=header-anchor href=#%e5%88%86%e7%b1%bb%e4%b8%8e%e8%81%9a%e7%b1%bb%e5%8c%ba%e5%88%ab></a></h3><h4 id=分类>分类
<a class=header-anchor href=#%e5%88%86%e7%b1%bb></a></h4><ul><li>是对当前所研究的问题已知它的类别数目及各类的特征(例如分布规律或来自各类的训练样本)</li><li>目的是要将另一些未知类别的个体正确地归属于其中的某一类</li><li>判别分析就是一种分类技术</li></ul><h4 id=聚类>聚类
<a class=header-anchor href=#%e8%81%9a%e7%b1%bb></a></h4><ul><li>是事先不知道研究的问题应分为几类, 更不知道观测到的个体的具体分类情况。</li><li>目的是通过对观测数据所进行的分析处理, 选定一种度量个体接近程度的统计量, 确定分类数目, 建立一种分类方法, 并按接近程度对观测对象给出合理的分类</li></ul><h2 id=方法分类>方法分类
<a class=header-anchor href=#%e6%96%b9%e6%b3%95%e5%88%86%e7%b1%bb></a></h2><h3 id=系统聚类法>系统聚类法
<a class=header-anchor href=#%e7%b3%bb%e7%bb%9f%e8%81%9a%e7%b1%bb%e6%b3%95></a></h3><ul><li>系统聚类法&ndash;(分层聚类Hierarchical Cluster)系统聚类法是应用最广泛的一种聚类方法</li><li>聚类原则: 都是相近的聚为一类, 即距离最近或者最相似的聚为一类</li><li>基本思想:<ul><li>距离相近的样品(或变量)先聚成类, 距离相远的后聚成类, 过程一直进行下去, 每个样品(或变量)总能聚到合适的类中</li><li>有时为了直观地反映系统聚类过程, 可以把整个分类系统画成一张谱系图(dendrogram)。因此, 系统聚类也称为谱系分析</li></ul></li><li>过程:<ol><li>假设总共有n个样品(或变量), 首先将每个样品(或变量)独自聚成一类, 共有n类; 然后根据所确定的样品(或变量)&ldquo;距离"公式, 形成初始距离阵。之后, 将其中距离较近的两个样品(或变量)聚合为一类, 其他的样品(或变量)仍各自聚为一类</li><li>第二步再根据新合并类与其他类的"距离"计算公式, 再形成的新的距离阵中, 将"距离"最近的两个类进一步再聚成一类</li><li>以上步骤一直进行下去, 最后将所有的样品(或变量)全聚成一类</li></ol></li></ul><h3 id=非系统聚类法>非系统聚类法
<a class=header-anchor href=#%e9%9d%9e%e7%b3%bb%e7%bb%9f%e8%81%9a%e7%b1%bb%e6%b3%95></a></h3><ul><li>非系统聚类法&ndash;(快速聚类法&ndash;K-均值聚类法K-means Cluster)<ul><li>当样本容量很大时, 系统聚类法需要占据足够大的计算机内存空间和计算时间, 因此产生了动态聚类法又称为逐步聚类法</li><li>其基本思想: 开始先粗略地分一下类, 然后按照某种最优的原则修改不合理的分类, 直至类分得比较合理为止</li><li>该方法具有计算量小, 占用计算机内存空间较少, 方法简单, <strong>适用于大样本的Q型聚类分析</strong></li></ul></li></ul><h4 id=k-均值聚类法k-means-cluster>K-均值聚类法K-means Cluster
<a class=header-anchor href=#k-%e5%9d%87%e5%80%bc%e8%81%9a%e7%b1%bb%e6%b3%95k-means-cluster></a></h4><ul><li>K均值法和系统聚类法一样, 都是以距离的远近亲疏为标准进行聚类的</li><li>区别:<ul><li>系统聚类对不同的类数产生一系列的聚类结果</li><li>K均值法只能产生指定类数的聚类结果</li></ul></li></ul><h2 id=相似性的度量>相似性的度量
<a class=header-anchor href=#%e7%9b%b8%e4%bc%bc%e6%80%a7%e7%9a%84%e5%ba%a6%e9%87%8f></a></h2><h3 id=样品相似性的度量>样品相似性的度量
<a class=header-anchor href=#%e6%a0%b7%e5%93%81%e7%9b%b8%e4%bc%bc%e6%80%a7%e7%9a%84%e5%ba%a6%e9%87%8f></a></h3><ul><li>Q型聚类分析，常用距离来测度样品之间的相似程度。两个样品间的相似程度就可用p维空间中的两点距离公式来度量。样本阵如下:</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>$$X=\left[
</span></span><span style=display:flex><span>    \begin{matrix}
</span></span><span style=display:flex><span>        X_{11} &amp; X_{12} &amp; \cdots &amp; X_{1p}\\
</span></span><span style=display:flex><span>        X_{21} &amp; X_{22} &amp; \cdots &amp; X_{2p}\\
</span></span><span style=display:flex><span>        \vdots &amp; \vdots &amp;        &amp; \vdots\\
</span></span><span style=display:flex><span>        X_{n1} &amp; X_{n2} &amp; \cdots &amp; X_{np}
</span></span><span style=display:flex><span>    \end{matrix}
</span></span><span style=display:flex><span>\right]$$
</span></span></code></pre></div><ul><li>因此，任何两个样品$X_i$与$X_j$之间的相似性可以通过矩阵中的第i行与第j行的相似程度来刻划。</li></ul><h3 id=变量相似性的度量>变量相似性的度量
<a class=header-anchor href=#%e5%8f%98%e9%87%8f%e7%9b%b8%e4%bc%bc%e6%80%a7%e7%9a%84%e5%ba%a6%e9%87%8f></a></h3><ul><li>变量间的相似性要从它们的方向趋同性或"相关性"进行考察, 度量方法有: <strong>夹角余弦法</strong>和<strong>相关系数</strong></li></ul><h4 id=夹角余弦>夹角余弦
<a class=header-anchor href=#%e5%a4%b9%e8%a7%92%e4%bd%99%e5%bc%a6></a></h4><ul><li>定义:<ul><li>二维向量a和b的夹角余弦: $cos(\theta) = \frac{a \cdot b}{||a|| \times ||b||} = \frac{(x_1, y_1) \cdot (x_2, y_2)}{\sqrt{x_1^2 + y_1^2} \times \sqrt{x_2^2 + y_2^2}} = \frac{x_1 x_2 + y_1 y_2}{\sqrt{x_1^2 + y_1^2} \times \sqrt{x_2^2 + y_2^2}}$</li><li>两变量$X_i$与$X_j$看作n维空间(n个样品所张成的空间)的两个向量<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>$$
</span></span><span style=display:flex><span>cos(\theta) = \frac{\sum_{i=1}^n (X_i \times Y_i)}{\sqrt{\sum_{i=1}^n (X_i)^2} \times \sqrt{\sum_{i=1}^n (Y_i)^2}} = \frac{a \cdot b}{||a|| \times ||b||}
</span></span><span style=display:flex><span>$$
</span></span></code></pre></div></li></ul></li><li>说明<ul><li>余弦相似度用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小</li><li>余弦值越接近1，就表明夹角越接近0度，也就是两个向量越相似</li></ul></li></ul><h5 id=应用>应用
<a class=header-anchor href=#%e5%ba%94%e7%94%a8></a></h5><ul><li>文本相似度计算<ol><li>对句子A和B分词</li><li>列出所有句子的词</li><li>计算词频</li><li>列出句子A和B的词频向量a和b</li><li>计算a和b的夹角余弦</li></ol></li></ul><h4 id=相关系数>相关系数
<a class=header-anchor href=#%e7%9b%b8%e5%85%b3%e7%b3%bb%e6%95%b0></a></h4><ul><li>概述<ul><li>相关系数经常用来度量变量间的相似性。变量$X_i$与$Y_i$的相关系数定义为:
$$r_{ij} = \frac{\sum_{k=1}^n(X_{ki} - \bar{X_i})(X_{kj} - \bar{X_j})}{\sqrt{\sum_{k=1}^n(X_{ki} - \bar{X_i})^2 \sum_{k=1}^n(X_{kj} - \bar{X_j})^2}}$$</li><li>显然$|r_{ij}| &lt; 1$。事实上, 相关系数是将样本观测数据中心化或标准化后的夹角余弦</li></ul></li><li>注意:<ul><li>无论是夹角余弦还是相关系数, 把他们统称为相似系数, 记为$c_{ij}$<ul><li>当$|c_{ij}| = 1$时, 说明变量$X_i$与$X_j$完全相似</li><li>当$|c_{ij}|$近似于1时, 说明变量$X_i$与$X_j$非常密切</li><li>当$|c_{ij}| = 0$时, 说明变量$X_i$与$X_j$完全不一样</li><li>当$|c_{ij}|$近似于0时, 说明变量$X_i$与$X_j$差别很大</li></ul></li><li>如果需要用距离来测定变量间的亲疏程度, 则变量之间常借助于相似系数来定义距离:
$$d_{ij} = 1 - |c_{ij}| 或者 d_{ij}^2 = 1- c_{ij}^2$$</li></ul></li></ul><h2 id=类间距与系统聚类法>类间距与系统聚类法
<a class=header-anchor href=#%e7%b1%bb%e9%97%b4%e8%b7%9d%e4%b8%8e%e7%b3%bb%e7%bb%9f%e8%81%9a%e7%b1%bb%e6%b3%95></a></h2><ul><li>概述:<ul><li>系统聚类法共8种: 最短距离法, 最长距离法, 中间距离法, 重心法, 类平均法, 可变类平均法, 可变法和离差平方和法</li><li>它们的归类和步骤是一致的, 主要差异: 两类间的距离; 新合并类与其他类间距离的计算方法不同</li></ul></li></ul><h3 id=最短距离法>最短距离法
<a class=header-anchor href=#%e6%9c%80%e7%9f%ad%e8%b7%9d%e7%a6%bb%e6%b3%95></a></h3><ul><li>用$d_{ij}$表示样品$X_i$与$X_j$之间的距离, 用$D_{ij}$表示类$G_i$与$G_j$之间的距离。定义类$G_i$与$G_j$之间的距离为两类最近样品的距离, 即为:
$$D_{ij} = \min\limits_{X_i \in G_i, X_j \in G_j} d_{ij}$$</li></ul><h3 id=最长距离法>最长距离法
<a class=header-anchor href=#%e6%9c%80%e9%95%bf%e8%b7%9d%e7%a6%bb%e6%b3%95></a></h3><ul><li>用$d_{ij}$表示样品$X_i$与$X_j$之间的距离, 用$D_{ij}$表示类$G_i$与$G_j$之间的距离。定义类$G_i$与$G_j$之间的距离为两类最远样品的距离, 即为:
$$D_{ij} = \max\limits_{X_i \in G_i, X_j \in G_j} d_{ij}$$</li></ul><h3 id=中间距离法>中间距离法
<a class=header-anchor href=#%e4%b8%ad%e9%97%b4%e8%b7%9d%e7%a6%bb%e6%b3%95></a></h3><ul><li>取最长距离和最短距离的中线, 则这个中线距离的平方为:
$$D_{kr}^2 = \frac{1}{2}D_{kp}^2 + \frac{1}{2}D_{kq}^2 - \frac{1}{4}D_{pq}^2$$</li></ul><h3 id=重心法>重心法
<a class=header-anchor href=#%e9%87%8d%e5%bf%83%e6%b3%95></a></h3><ul><li>概述:<ul><li>重心法是定义类间距离为两类样品重心(各类样品的均值)的距离的系统聚类方法。重心指标对类有很好的代表性，但利用各样本的信息不充分</li><li>设$G_p$与$G_q$分别有样品$n_p,n_q$个, 其重心分别为$\bar{X_p}和\bar{X_q}$, 则$G_p$与$G_q$之间的距离定义为$X_p和X_q$之间的距离, 用欧式距离来表示, 即
$$D_{pq}^2 = (\bar{X_p} - \bar{X_q})&rsquo;(\bar{X_p} - \bar{X_q})$$</li><li>设将$G_p和G_q$合并为$G_r$, 则$G_r$内样品个数为$n_r = n_p + n_q$, 它的重心是$\bar{X_r} = \frac{1}{n_r}(n_p \bar{X_p} + n_q \bar{X_q})$, 类$G_k$的重心是$\bar{X_k}$, 那么它与新类$G_r$的距离为:</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>$$
</span></span><span style=display:flex><span>\begin{aligned}
</span></span><span style=display:flex><span>      D_{kr}^2 &amp; = (\bar{X_k} - \bar{X_r})&#39;(\bar{X_k} - \bar{X_r}) \\
</span></span><span style=display:flex><span>               &amp; = [\bar{X_k} - \frac{1}{n_r}(n_p \bar{X_p} + n_q \bar{X_q})]&#39;[\bar{X_k} - \frac{1}{n_r}(n_p \bar{X_p} + n_q \bar{X_q})] \\
</span></span><span style=display:flex><span>               &amp; = \frac{n_p}{n_r} D_{kp}^2 + \frac{n_q}{n_r} D_{kq}^2 - \frac{n_p n_q}{n_r^2} D_{pq}^2
</span></span><span style=display:flex><span>\end{aligned}
</span></span><span style=display:flex><span>$$
</span></span></code></pre></div></li></ul><h3 id=类平均法>类平均法
<a class=header-anchor href=#%e7%b1%bb%e5%b9%b3%e5%9d%87%e6%b3%95></a></h3><ul><li>概述:<ul><li>重心法虽然有较好的代表性, 但并未充分利用样本信息。而类平均法定义两类之间的距离平方为两类两两样品之间距离平方的平均数，即为
$$D_{pq}^2 = \frac{1}{n_pn_q} \sum_{X_i \in G_p} \sum_{X_j \in G_q} d_{ij}^2$$</li><li>设聚类的某一步将$G_p和G_q$合并为新类$G_r$, 则类$G_k$与新并类$G_r$的距离为:</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>$$
</span></span><span style=display:flex><span>\begin{aligned}
</span></span><span style=display:flex><span>    D_{kr}^2 &amp; = \frac{1}{n_kn_r} \sum_{X_i \in G_k} \sum_{X_j \in G_r} d_{ij}^2 \\
</span></span><span style=display:flex><span>             &amp; = \frac{1}{n_kn_r} (\sum_{X_i \in G_k} \sum_{X_j \in G_p} d_{ij}^2 + \sum_{X_i \in G_k} \sum_{X_j \in G_q} d_{ij}^2) \\
</span></span><span style=display:flex><span>             &amp; = \frac{n_p}{n_r} D_{kp}^2 + \frac{n_q}{n_r} D_{kq}^2
</span></span><span style=display:flex><span>\end{aligned}
</span></span><span style=display:flex><span>$$
</span></span></code></pre></div></li></ul><h3 id=可变类平均法>可变类平均法
<a class=header-anchor href=#%e5%8f%af%e5%8f%98%e7%b1%bb%e5%b9%b3%e5%9d%87%e6%b3%95></a></h3><ul><li>概述:<ul><li>由于类平均法中没有反映出$G_p和G_q$之间的距离$D_{pq}$的影响, 信息利用不充分。为了充分利用各类距离的信息，可将类平均法和中间距离法进行组合，得到一个组合模型，此组合模型称为可变类平均法</li><li>如果将$G_p和G_q$合并为新类$G_r$, 类$G_k与新并类G_r$的距离公式为:
$$D_{kr}^2 = (1 - \beta) (\frac{n_p}{n_r}D_{kp}^2 + \frac{n_q}{n_r}D_{kq}^2) + \beta D_{pq}^2$$</li><li>之所以称该方法为可变类平均法是因为$\beta$是可变的</li><li>系数$\beta$称为聚类强度系数, $\beta$的取值不同, 聚类的结果就会不同。通常$\beta$取负值时可以使聚类的分辨能力提高; 一般情况下, 取$\beta = - \frac{1}{4}$</li></ul></li></ul><h3 id=可变法>可变法
<a class=header-anchor href=#%e5%8f%af%e5%8f%98%e6%b3%95></a></h3><ul><li>概述:<ul><li>不考虑$G_p和G_q$两类h中各自样品的个数, 而是类同等看待, 则得到可变法
$$D_{kr}^2 = \frac{1 - \beta}{2}(D_{kp}^2 + D_{kq}^2) + \beta D_{pq}^2$$</li></ul></li></ul><h3 id=离差平方和法ward法>离差平方和法(Ward法)
<a class=header-anchor href=#%e7%a6%bb%e5%b7%ae%e5%b9%b3%e6%96%b9%e5%92%8c%e6%b3%95ward%e6%b3%95></a></h3><ul><li>概述:<ul><li>该方法的思想来自于方差分析, 由Ward提出, 如果分类正确, 同类样品的离差平方和应当较小, 类与类的离差平方和较大</li><li>具体做法是先将n个样品各自成一类, 然后每次缩小一类, 每缩小一类, 离差平方和就要增大, <strong>选择使方差增加最小的两类合并</strong>, 直到所有的样品归为一类为止</li><li>设将n个样品分成k类$G_1,G_2,\cdots,G_k$, 用$X_{it}$表示$G_t$中的第i个样品, $n_t$表示$G_t$中样品的个数, $\bar{X_t}$是$G_t$的重心, 则$G_t$的样品离差平方和为:</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>$$S_t = \sum_{i=1}^{n_t}(X_{it} - \bar{X_t})&#39;(X_{it} - \bar{X_t})$$
</span></span></code></pre></div><ul><li>如果$G_p$和$G_q$合并为新类$G_r$, 则类内离差平方和分别为:</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>$$
</span></span><span style=display:flex><span>  S_p = \sum_{i=1}^{n_p}(X_{ip} - \bar{X_p})&#39;(X_{ip} - \bar{X_p}) \\
</span></span><span style=display:flex><span>  S_q = \sum_{i=1}^{n_q}(X_{iq} - \bar{X_q})&#39;(X_{iq} - \bar{X_q}) \\
</span></span><span style=display:flex><span>  S_r = \sum_{i=1}^{n_r}(X_{ir} - \bar{X_r})&#39;(X_{ir} - \bar{X_r})
</span></span><span style=display:flex><span>$$
</span></span></code></pre></div><ul><li>它们反映了各自类内样品的分散程度, 如果$G_p$和$G_q$这两类相距较近, 则合并后所增加的离散平方和$S_r - S_p - S_q$应较小; 否则, 应较大。于是定义$G_p$和$G_q$之间的平方距离为: $D_{pq}^2 = S_r - S_p - S_q$</li><li>如果将$G_p$和$G_q$合并为新类$G_r$, 类$G_k$与新并类$G_r$的距离公式为:
$$D_{kr}^2 = \frac{n_k + n_p}{n_r + n_k} D_{kp}^2 + \frac{n_k + n_q}{n_r + n_k} D_{kq}^2 - \frac{n_k}{n_r + n_k} D_{pq}^2$$</li></ul></li></ul><h2 id=类间距离的统一性>类间距离的统一性
<a class=header-anchor href=#%e7%b1%bb%e9%97%b4%e8%b7%9d%e7%a6%bb%e7%9a%84%e7%bb%9f%e4%b8%80%e6%80%a7></a></h2><ul><li>上述8种系统聚类法的步骤完全一样, 只是距离的递推公式不同。Lance和Williams于1967年给出统一公式, 即将$G_p$和$G_q$合并为新类$G_r$, 类$G_k$与新并类$G_r$的距离公式为:
$$D_{kr}^2 = \alpha_p D_{kp}^2 + \alpha_q D_{kq}^2 + \beta D_{pq}^2 + \gamma |D_{kp}^2 - D_{kq}^2|$$
其中$\alpha_p、\alpha_q、\beta、\gamma$是参数, 不同的系统聚类法, 它们的取不同的数</li><li>系统聚类法参数表</li></ul><table><thead><tr><th>方法</th><th>$\alpha_p$</th><th>$\alpha_q$</th><th>$\beta$</th><th>$\gamma$</th></tr></thead><tbody><tr><td>最短距离法</td><td>1/2</td><td>1/2</td><td>0</td><td>-1/2</td></tr><tr><td>最长距离法</td><td>1/2</td><td>1/2</td><td>0</td><td>1/2</td></tr><tr><td>中间距离法</td><td>1/2</td><td>1/2</td><td>-1/4</td><td>0</td></tr><tr><td>重心法</td><td>$n_p/n_r$</td><td>$n_q/n_r$</td><td>$-\alpha_p \alpha_q$</td><td>0</td></tr><tr><td>类平均法</td><td>$n_p/n_r$</td><td>$n_q/n_r$</td><td>0</td><td>0</td></tr><tr><td>可变类平均法</td><td>$(1 - \beta)n_p/n_r$</td><td>$(1 - \beta)n_q/n_r$</td><td>$\beta(&lt;1)$</td><td>0</td></tr><tr><td>可变法</td><td>$(1 - \beta)/2$</td><td>$(1 - \beta)/2$</td><td>$\beta(&lt;1)$</td><td>0</td></tr><tr><td>离差平方和法</td><td>$\frac{(n_p + n_k)}{(n_r + n_k)}$</td><td>$\frac{(n_q + n_k)}{(n_r + n_k)}$</td><td>$-n_k/(n_k + n_r)$</td><td>0</td></tr></tbody></table><h2 id=类个数的确定>类个数的确定
<a class=header-anchor href=#%e7%b1%bb%e4%b8%aa%e6%95%b0%e7%9a%84%e7%a1%ae%e5%ae%9a></a></h2><ol><li>由适当的阀值确定</li><li>根据数据点的散布图直观地确定类的个数</li></ol><h2 id=系统聚类法在r上的实现>系统聚类法在R上的实现
<a class=header-anchor href=#%e7%b3%bb%e7%bb%9f%e8%81%9a%e7%b1%bb%e6%b3%95%e5%9c%a8r%e4%b8%8a%e7%9a%84%e5%ae%9e%e7%8e%b0></a></h2><h3 id=示例>示例
<a class=header-anchor href=#%e7%a4%ba%e4%be%8b></a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#a6e22e>data</span>(iris)
</span></span><span style=display:flex><span><span style=color:#a6e22e>attach</span>(iris)
</span></span><span style=display:flex><span>class<span style=color:#f92672>=</span><span style=color:#a6e22e>data.frame</span>(<span style=color:#a6e22e>c</span>(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>),<span style=color:#a6e22e>as.vector</span>(<span style=color:#a6e22e>unique</span>(iris[,<span style=color:#ae81ff>5</span>])))
</span></span><span style=display:flex><span><span style=color:#a6e22e>names</span>(class)<span style=color:#f92672>=</span><span style=color:#a6e22e>c</span>(<span style=color:#e6db74>&#34;cla&#34;</span>,<span style=color:#e6db74>&#34;name&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#系统聚类法</span>
</span></span><span style=display:flex><span>d<span style=color:#f92672>=</span><span style=color:#a6e22e>dist</span>(iris[,<span style=color:#ae81ff>-5</span>],method <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;euclidean&#34;</span>,diag <span style=color:#f92672>=</span> T,upper <span style=color:#f92672>=</span> T,p<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>hc<span style=color:#f92672>=</span><span style=color:#a6e22e>hclust</span>(d,method <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;ward&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#a6e22e>plot</span>(hc)
</span></span><span style=display:flex><span><span style=color:#75715e># 分成3类</span>
</span></span><span style=display:flex><span>rt1<span style=color:#f92672>=</span><span style=color:#a6e22e>data.frame</span>(<span style=color:#a6e22e>cutree</span>(hc,k<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>))
</span></span><span style=display:flex><span><span style=color:#a6e22e>names</span>(rt1)<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cla&#34;</span>
</span></span><span style=display:flex><span>rt1<span style=color:#f92672>=</span><span style=color:#a6e22e>merge</span>(rt1,class,by<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cla&#34;</span>)
</span></span><span style=display:flex><span>tab1<span style=color:#f92672>=</span><span style=color:#a6e22e>table</span>(rt1<span style=color:#f92672>$</span>name,iris[,<span style=color:#ae81ff>5</span>])
</span></span><span style=display:flex><span><span style=color:#a6e22e>sum</span>(<span style=color:#a6e22e>diag</span>(<span style=color:#a6e22e>prop.table</span>(tab1)))
</span></span><span style=display:flex><span><span style=color:#75715e># k均值聚类法</span>
</span></span><span style=display:flex><span>km<span style=color:#f92672>=</span><span style=color:#a6e22e>kmeans</span>(iris[,<span style=color:#ae81ff>-5</span>],<span style=color:#ae81ff>3</span>,nstart<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>,algorithm <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Hartigan-Wong&#34;</span>)
</span></span><span style=display:flex><span>rt2<span style=color:#f92672>=</span><span style=color:#a6e22e>data.frame</span>((km<span style=color:#f92672>$</span>cluster))
</span></span><span style=display:flex><span><span style=color:#a6e22e>names</span>(rt2)<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cla&#34;</span>
</span></span><span style=display:flex><span>rt2<span style=color:#f92672>=</span><span style=color:#a6e22e>merge</span>(rt2,class,by<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cla&#34;</span>)
</span></span><span style=display:flex><span>tab2<span style=color:#f92672>=</span><span style=color:#a6e22e>table</span>(rt2<span style=color:#f92672>$</span>name,iris[,<span style=color:#ae81ff>5</span>])
</span></span><span style=display:flex><span><span style=color:#a6e22e>sum</span>(<span style=color:#a6e22e>diag</span>(<span style=color:#a6e22e>prop.table</span>(tab2)))
</span></span></code></pre></div></div><footer class=post-footer><div class=post-tags><a href=/tags/%e6%95%b0%e7%90%86%e7%bb%9f%e8%ae%a1>数理统计</a></div><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-copyright><img src=/imgs/cc/cc.svg width=75 height=75 align=right alt=共享知识><ul><li class=post-copyright-title><strong>文章标题：</strong>
多元统计之聚类分析</li><li class=post-copyright-author><strong>本文作者： </strong>Patrick</li><li class=post-copyright-link><strong>本文链接：</strong>
<a id=post-cr-link href=/post/cluster-analysis/ title=多元统计之聚类分析>/post/cluster-analysis/</a></li><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <i class="fab fa-fw fa-creative-commons"></i><a target=_blank href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/post/discrimination-analysis/ rel=next title=多元统计之判别分析><i class="fa fa-chevron-left"></i> 多元统计之判别分析</a></div><div class="post-nav-prev post-nav-item"><a href=/post/the-common-distribution-of-statistics/ rel=prev title=统计中常用的分布>统计中常用的分布
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div id=comments class=post-comments><div class=comment-head><div class=comment-headline><i class="fas fa-comments fa-fw"></i>
<span>评论交流</span></div></div><div class=comment-wrap><div><div class=comment-loading><i class="fa fa-sync fa-spin"></i></div><div class=waline-container></div></div></div></div></div></main><footer class=footer><div class=footer-inner><div id=gtranslate class=google-translate><i class="fa fa-language"></i><div id=google_translate_element></div></div><div class=copyright>&copy;
<span itemprop=copyrightYear>2015 - 2024
</span><span class=with-love><i class="fa fa-heart"></i>
</span><span class=author itemprop=copyrightHolder>Patrick</span></div><div class=powered-by>由 <a href=https://gohugo.io title=0.121.1 target=_blank>Hugo</a> & <a href=https://github.com/hugo-next/hugo-theme-next title=4.5.3 target=_blank>Hugo NexT.Gemini</a> 强力驱动</div></div></footer><script type=text/javascript src=https://unpkg.com/animejs@3.2.1/lib/anime.min.js defer></script><script type=text/javascript src=https://unpkg.com/viewerjs@1.11.0/dist/viewer.min.js defer></script><script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":true,"save":"manual"},"copybtn":true,"darkmode":true,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"找到 ${hits} 个搜索结果","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"limit":1e3,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"postmeta":{"comments":{"enable":true,"plugin":"waline"},"views":{"enable":true,"plugin":"busuanzi"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"vendor":{"plugins":"unpkg","router":"https://unpkg.com"},"version":"4.5.3","waline":{"cfg":{"emoji":false,"imguploader":false,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":"cmts.yaoyuehome.com","sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.min.b0c78e5a4df586ee46d02716ffa91a4a322e56763e04e91eb2ba052c9469ed02.js defer></script><script type=text/javascript src=/js/math.min.a6ada19a368d85dad9ead2040d86ae561a867fafef89391d1aa2aa5909366509.js defer></script></body></html>