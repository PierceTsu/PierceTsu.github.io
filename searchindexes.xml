<?xml version="1.0" encoding="utf-8" standalone="yes"?><search><entry><title>力扣-准时到达的列车最小时速</title><url>/post/ledtcode-1870/</url><categories><category>数据结构与算法</category></categories><tags><tag>算法</tag></tags><content type="html"> 描述 力扣1870 给你一个浮点数 hour ，表示你到达办公室可用的总通勤时间。要到达办公室，你必须按给定次序乘坐 n 趟列车。另给你一个长度为 n 的整数数组 dist ，其中 dist[i] 表示第 i 趟列车的行驶距离（单位是千米）。每趟列车均只能在整点发车，所以你可能需要在两趟列车之间等待一段时间。例如，第 1 趟列车需要 1.5 小时，那你必须再等待 0.5 小时，搭乘在第 2 小时发车的第 2 趟列车。 返回能满足你准时到达办公室所要求全部列车的 最小正整数 时速（单位：千米每小时），如果无法准时到达，则返回 -1 。 生成的测试用例保证答案不超过 10^7 ，且 hour 的小数点后最多存在两位数字 。 示例1: 输入：dist = [1,3,2], hour = 6 输出：1 解释：速度为 1 时 第 1 趟列车运行需要 1/1 = 1 小时。由于是在整数时间到达，可以立即换乘在第 1 小时发车的列车。 第 2 趟列车运行需要 3/1 = 3 小时。 由于是在整数时间到达，可以立即换乘在第 4 小时发车的列车。第 3 趟列车运行需要 2/1 = 2 小时。 你将会恰好在第 6 小时到达。 示例2: 输入：dist = [1,3,2], hour = 2.7 输出：3 解释：速度为 3 时： 第 1 趟列车运行需要 1/3 = 0.33333 小时。 由于不是在整数时间到达，故需要等待至第 1 小时才能搭乘列车。第 2 趟列车运行需要 3/3 = 1 小时。 由于是在整数时间到达，可以立即换乘在第 2 小时发车的列车。第 3 趟列车运行需要 2/3 = 0.66667 小时。 你将会在第 2.66667 小时到达。 示例2: 输入：dist = [1,3,2], hour = 1.9 输出：-1 解释：不可能准时到达，因为第 3 趟列车最早是在第 2 小时发车。 思路 二分查找能够到达的最小速度
由于时速必须为正整数，因此二分的下界为1； 对于二分的上界，我们考虑 hours 为两位小数，因此对于最后一段路程，最小的时限为 0.01，那么最高的时速要求即为 dist[i]/0.01≤10^7，同时为二分时速的上界。 在二分过程中，假设当前时速为 mid，我们计算对应时速下到达终点的时间t，并与hour 比较以判断能否按时到达。 细节 在代码实现中，需要注意浮点数的处理。
代码示例 public class LC1870MinSpeedOnTime { public static void main(String[] args) { int[] dist = new int[] {1, 3, 2}; // int[] dist = new int[] {1, 1, 100000}; // int[] dist = new int[] {1, 1}; // double hour = 6; double hour = 2.7; // double hour = 1.9; // double hour = 2.01; // double hour = 1.0; int min = minSpeedOnTime(dist, hour); System.out.println(min); } public static int minSpeedOnTime(int[] dist, double hour) { int l = 1, r = (int)1e7; int res = -1; while (l &amp;lt;= r) { int mid = (l + r) / 2; double cur = 0; for (int i = 0; i &amp;lt; dist.length - 1; i++) { cur += Math.ceil(dist[i] * 1d / mid ); } cur += dist[dist.length - 1] * 1d / mid; if (cur &amp;lt;= hour) { res = mid; r = mid - 1; } else { l = mid + 1; } } return res; } }</content></entry><entry><title>牛客-合唱队解法</title><url>/post/niu-ke---he-chang-dui-jie-fa/</url><categories><category>数据结构与算法</category></categories><tags><tag>算法</tag><tag>数据结构</tag></tags><content type="html"><![CDATA[  描述 牛客网题目: 合唱队 N 位同学站成一排，音乐老师要请最少的同学出列，使得剩下的 K 位同学排成合唱队形。 通俗来说，能找到一个同学，他的两边的同学身高都依次严格降低的队形就是合唱队形。
例子 123 124 125 123 121 是一个合唱队形 123 123 124 122不是合唱队形，因为前两名同学身高相等，不符合要求 123 122 121 122不是合唱队形，因为找不到一个同学，他的两侧同学身高递减。 你的任务是，已知所有N位同学的身高，计算最少需要几位同学出列，可以使得剩下的同学排成合唱队形。 注意：不允许改变队列元素的先后顺序且不要求最高同学左右人数必须相等 数据范围：1≤n≤3000 输入描述 用例两行数据，第一行是同学的总数 N ，第二行是 N 位同学的身高，以空格隔开
输出描述 最少需要几位同学出列
示例 示例1 输入： 8 186 186 150 200 160 130 197 200 输出： 4 说明： 由于不允许改变队列元素的先后顺序，所以最终剩下的队列应该为186 200 160 130或150 200 160 130 思路 解法一: 动态规划 分析题目可得，其实就是求最长递增子序列的变种题目，只不过加了一个约束条件，需要左边递增右边递减的情况。
状态定义： left[i]表示该位置结尾的最长递增子序列长度， right[i]表示该位置开头的最长递减子序列长度。 状态初始化：初始长度均为1。 状态转移： 正序遍历时，如果i位置同学身高大于j位置同学，则可以排在j位置同学后面，即left[i]=Math.max(left[i],left[j]+1)。 逆序遍历时，如果i位置同学身高大于j位置同学，则j可排在i同学后面，构成递减子序列，即right[i]=Math.max(right[i],right[j]+1)。 步骤 先找到每一个位置i左侧的最长上升子序列长度left[i] 再找到每一个位置i右侧的最长下降子序列长度right[i] 然后求出所有位置的最长序列长度=左侧最长子序列长度+右侧最长子序列长度-1（因为该位置被算了两次，所以减1） 然后用数目减去最长序列长度就是答案，需要出队的人数 代码示例 public static void dp() { Scanner in = new Scanner(System.in); while (in.hasNext()) { int n = in.nextInt(); int[] arr = new int[n]; for (int i = 0; i &lt; n; i++) { arr[i] = in.nextInt(); } // //计算每个位置左侧的最长递增 int[] left = new int[n]; Arrays.fill(left, 1); for (int i = 1; i &lt; n; i++) { for (int j = 0; j &lt; i; j++) { if (arr[i] &gt; arr[j]) { left[i] = Math.max(left[i], left[j] + 1); } } } // //计算每个位置右侧的最长递减 int[] right = new int[n]; Arrays.fill(right, 1); for (int i = n - 2; i &gt;= 0; i--) { for (int j = i + 1; j &lt; n; j++) { if (arr[i] &gt; arr[j]) { right[i] = Math.max(right[i], right[j] + 1); } } } // 统计每个位置的合唱队形长度最大值 int max = 0; for (int i = 0; i &lt; n; i++) { max = Math.max(max, left[i] + right[i] - 1); } System.out.println(n - max); } } 解法二: 二分查找 步骤 新建left数组和right数组，left[i]表示该位置结尾的最长递增子序列长度，right[i]表示该位置开头的最长递减子序列长度。 然后维护一个递增数组tail1，如果tail1数组为空，或arr[i]大于tail1数组末尾元素，直接将arr[i]放在tail1数组末尾，tail1数组的长度即是当前位置结尾的最长递增子序列长度。否则，二分法找到arr[i]在tail1数组中的位置， 假设该位置为l，则l+1为当前位置结尾的最长递增子序列长度。 对于right数组，逆序遍历arr数组，维护一个递增数组tail2，如果tail2数组为空，或arr[i]大于tail2数组末尾元素，直接将arr[i]放在tail2数组末尾，tail2数组的长度即是当前位置开头的最长递减子序列长度。否则，二分法找到arr[i]在tail2数组中的位置，假设该位置为l，则l+1为当前位置开头的最长递减子序列长度。 最后，遍历left、right数组，统计每个位置的合唱队形长度最大值。 代码示例 public static void binarySearch() { Scanner in = new Scanner(System.in); while (in.hasNext()) { int n = in.nextInt(); int[] arr = new int[n]; for (int i = 0; i &lt; n; i++) { arr[i] = in.nextInt(); } // 记录i结尾的最长递增子序列长度 int[] left = new int[n]; // 记录递增数组 int[] tail1 = new int[n]; int idx = -1; for (int i = 0; i &lt; n; i++) { if (i == 0 || tail1[idx] &lt; arr[i]) { tail1[++idx] = arr[i]; left[i] = idx + 1; } else { // 二分查找 arr[i] 在tail1中的位置 int l = 0, m = 0, r = idx; while (l &lt; r) { m = (l + r) / 2; if (arr[i] &gt; tail1[m]) { l = m + 1; } else { r = m; } } tail1[l] = arr[i]; left[i] = l + 1; } } // 记录i结尾的最长递减子序列长度 int[] right = new int[n]; int[] tail2 = new int[n]; int idx2 = -1; for (int i = n - 1; i &gt;= 0; i--) { if (i == n - 1 || arr[i] &gt; tail2[idx2]) { tail2[++idx2] = arr[i]; right[i] = idx2 + 1; } else { int l = 0, m = 0, r = idx2; while (l &lt; r) { m = (l + r) / 2; if (arr[i] &gt; tail2[m]) { l = m + 1; } else { r = m; } } tail2[l] = arr[i]; right[i] = l + 1; } } int max = 0; for (int i = 0; i &lt; n; i++) { max = Math.max(max, left[i] + right[i] - 1); } System.out.println(n - max); } }   ]]></content></entry><entry><title>牛客-四则运算解法</title><url>/post/niu-ke---si-ze-yun-suan-jie-fa/</url><categories><category>数据结构与算法</category></categories><tags><tag>算法</tag><tag>数据结构</tag></tags><content type="html"><![CDATA[  描述 牛客网题目: 四则运算 输入一个表达式（用字符串表示）, 求这个表达式的值. 保证字符串中的有效字符包括[‘0’-‘9’],‘+’,‘-’, ‘*’,‘/’ ,‘(’， ‘)’,‘[’, ‘]’,‘{’ ,‘}’. 且表达式一定合法.
数据范围：表达式计算结果和过程中满足∣val∣≤1000 ，字符串长度满足 1≤n≤1000
输入描述： 输入一个算术表达式
输出描述： 得到计算结果
思路 使用栈分别存储数字和操作符, 通过出入栈来完成表达式计算.
步骤 遍历字符串表达式; 如果为左括号 (包括大括号, 中括号和小括号), 需要从当前位置遍历找到括号结尾位置, 使用递归方式计算括号中的表达式; 如果为数字则入数字栈, 需要考虑负数和多位数字的情况; 如果为四则远算符号, 分两种情况: 如果符号栈为空栈, 则直接入栈; 如果符号栈不为空, 当前的符号优先级高于栈顶符号, 则直接入栈; 如果当前符号优先级小于或者等于栈顶符号, 从数栈中pop出两个数, 再从符号栈中pop出一个符号, 进行运算, 将结果入数栈, 然后操作符入符号栈; 当表达式扫描完, 顺序的从数栈和符号栈中pop出相应的数和符号, 并运算; 最后数栈中只有一个数字及为表达式的结果. 代码实现 /** * @Description: Calculator * @Author : piercetsu@gmail.com * @Create Date: 2023-05-18 */ public class Calculator { public static void main(String[] args) { Scanner in = new Scanner(System.in); while (in.hasNextLine()) { String exp = in.nextLine(); BigDecimal res = solution(exp); System.out.println(res); } } public static BigDecimal solution(String exp) { Stack&lt;BigDecimal&gt; numStack = new Stack&lt;&gt;(); Stack&lt;Character&gt; opStack = new Stack&lt;&gt;(); BigDecimal num1 = BigDecimal.ZERO, num2 = BigDecimal.ZERO; for (int i = 0; i &lt; exp.length(); i++) { char ch = exp.charAt(i); // 左括号 if (ch == &#39;(&#39; || ch == &#39;[&#39; || ch == &#39;{&#39;) { opStack.push(ch); int end = i; // 找到括号结尾位置 for (int j = i + 1; j &lt; exp.length(); j++) { char k = exp.charAt(j); if (k == &#39;(&#39; || k == &#39;[&#39; || k == &#39;{&#39;) { opStack.push(k); } else if (k == &#39;)&#39; || k == &#39;]&#39; || k == &#39;}&#39;) { opStack.pop(); if (opStack.isEmpty() || (opStack.peek() != &#39;(&#39; &amp;&amp; opStack.peek() != &#39;[&#39; &amp;&amp; opStack.peek() != &#39;{&#39;) ) { end = j; break; } } } // 递归计算括号中的结果 BigDecimal solution = solution(exp.substring(i + 1, end)); numStack.push(solution); // 更新 i 值 i = end; } else if ((i == 0 &amp;&amp; ch == &#39;-&#39;) || (Character.isDigit(ch))) { // 数字, 需要考虑负数和多位数字场景 String num = ch + &#34;&#34;; i ++; while (i &lt; exp.length() &amp;&amp; Character.isDigit(exp.charAt(i))) { num += exp.charAt(i); i++; } numStack.push(new BigDecimal(num)); i--; } else { // 操作符号 +-*/ if (!opStack.isEmpty()) { // 当前符号优先级小于或者等于栈顶符号, 从数栈中pop出两个数, 再从符号栈中pop出一个符号, 进行运算, 将结果入数栈 if (ch == &#39;+&#39; || ch == &#39;-&#39;) { // +- 需要将栈中的全部计算 while (!opStack.isEmpty()) { num1 = numStack.pop(); num2 = numStack.pop(); char op = opStack.pop(); BigDecimal res = calc(num1, num2, op); numStack.push(res); } } else if ((ch == &#39;*&#39; || ch == &#39;/&#39;) &amp;&amp; (opStack.peek() == &#39;*&#39; || opStack.peek() == &#39;/&#39;) ) { num1 = numStack.pop(); num2 = numStack.pop(); char op = opStack.pop(); BigDecimal res = calc(num1, num2, op); numStack.push(res); } } // 操作符栈为空或者当前的符号优先级高于栈顶符号, 则直接入栈 opStack.push(ch); } } // 计算剩余元素结果 while (!opStack.empty()) { num1 = numStack.pop(); num2 = numStack.pop(); char op = opStack.pop(); BigDecimal res = calc(num1, num2, op); numStack.push(res); } return numStack.pop(); } public static BigDecimal calc(BigDecimal num1, BigDecimal num2, char op) { BigDecimal res = BigDecimal.ZERO; switch (op) { case &#39;+&#39;: res = num1.add(num2); break; case &#39;-&#39;: res = num2.subtract(num1); // 顺序 break; case &#39;*&#39;: res = num1.multiply(num2); break; case &#39;/&#39;: // 精度保留8位小数 if (num1.compareTo(BigDecimal.ZERO) != 0) { res = num2.divide(num1, 8, BigDecimal.ROUND_HALF_UP); } break; default: break; } return res; } } 其他解法 使用Java 8的Nashorn JavaScript引擎 代码 @Test public void scriptEngine() throws ScriptException { String exp = &#34;3+2*{1+2*[-4/(8-5)+7]}&#34;; exp = exp.replace(&#34;{&#34;, &#34;(&#34;) .replace(&#34;}&#34;, &#34;)&#34;) .replace(&#34;[&#34;, &#34;(&#34;) .replace(&#34;]&#34;, &#34;)&#34;); ScriptEngineManager scriptEngineManager = new ScriptEngineManager(); ScriptEngine engine = scriptEngineManager.getEngineByName(&#34;nashorn&#34;); System.out.println(engine.eval(exp)); }   ]]></content></entry><entry><title>Flink Window Trigger 使用的正确姿势</title><url>/post/flinkwindowtrigger-shi-yong-de-zheng-que-zi-shi/</url><categories><category>数据科学</category></categories><tags><tag>大数据</tag></tags><content type="html"> 废话 在使用 Flink DataStream API 进行流式数据聚合统计时, sink操作时, 都会用到 Trigger. 以 Flink 1.11.x 版本为例, 查看 Trigger 的实现类主要有以下常用的 7 种: EventTimeTrigger: 事件时间触发器 ProcessTimeTrigger: 处理时间触发器 ContinuousEventTimeTrigger: 连续的事件时间触发器 ContinuousProcessingTimeTrigger: 连续的处理时间触发器 CountTrigger: 窗口数据条数触发器 DeltaTrigger: 窗口Delta指标触发器 PurgingTrigger: 窗口清除触发器 在实际的业务场景中, 使用最多的当属 ContinuousEventTimeTrigger 这个类了. 从一个数据处理延迟告警说起 线上稳定运行了将近两周的 Job, 突然收到告警, 出现了半小时以上的数据处理延迟. 立即查看 Job 的 UI 及监控指标页面, 发现 checkpoint 出现了超时, 定位原因是两周前重新划分了渠道维表大类, 某个渠道类别 keyby 聚合算子因存在数据热点问题, TaskManager 出现了性能处理瓶颈, 导致 Kafka Source 端出现了严重的反压, 数据处理出现了延迟. 解决过程 处理原则就是尽量的打散数据聚合的 key, 均匀的落在每个 Task 上. 套用以往 SparkStreaming 优化的经验来看还是十分的简单, 于是使用 map 算子增加随机的 key, 再 reduce 做预聚合, 最后再按窗口来进行汇总. 不出半小时改完上预发布环境测试, 查看了下数据流一切正常后上线生产, 重跑当日数据. 观察了当天上午每小时的大屏数据折线趋势图, 相比昨天大概有 5% 左右的涨幅, 在有活动的情况下看起来还算正常. 发现问题 下午再次查看大屏数据, 当日的累计成交订单数已经超过前两天之和; 按小时核对明细层数据, 没发现问题, 再看报表聚合层逻辑, 定位问题出现在预聚合计算逻辑. 验证预聚合逻辑 为了验证这个问题, 启动本地 Docker kafka 并灌入乱序数据, 来验证预聚合逻辑. 预聚合逻辑 需求: 按天统计渠道大类的下单数/GMV/用户数, 每秒进行更新 此处简化 …</content></entry><entry><title>Flink QuickStart</title><url>/post/quickstart-of-flink/</url><categories><category>数据科学</category></categories><tags><tag>大数据</tag></tags><content type="html"><![CDATA[  简介 Apache Flink 是一个框架和分布式处理引擎，用于在无边界和有边界数据流上进行有状态的计算。 Flink 能在所有常见集群环境中运行，并能以内存速度和任意规模进行计算。 有界与无界数据流 无界流: 有定义流开始, 未定义流的结束; 持续产生数据, 需要以特定顺序摄取事件 (如事件的发生顺序), 以便能够推断结果的完整性. 有界流: 既定义流的开始, 又定义流的结束; 可以在摄取所有的数据后进行排序计算, 并需要有序摄取, 有界流处理通常被称为批处理. 特点 事件驱动型 流批一体 分层api SQL/Table API (dynamic tables) DataStream API (streams, windows) ProcessFunction (events, state, time) 应用场景 电商和市场营销 数据报表, 广告投放, 业务流程需要 物联网 (IOT) 传感器实时数据采集和显示, 实时报警, 交通运输业 电信业 基站流量调配 银行和金融业 实时结算和通知推送, 实时监测异常行为 Flink vs Spark Streaming Spark Streaming 是微批 (micro-batching) 处理, 运行时需要指定批处理时间, 每次job处理一个批次的DStream (一组一组的小批数据RDD的集合) Flink 是标准的流执行模式, 基本数据模型是DataStream 以及事件 (Event) 序列, 一个事件处理完成后可以直接发往下一个节点进行处理 架构概览 runtime架构: Job 通过DataStream API, DataSet API, SQL和Table API 编写Flink任务, 会生成一个JobGraph (由 source, map, keyBy, window, apply和sink等算子组成) 当JobGraph提交给Flink集群后, 能够以Local, Standalone, Yarn和K8s四种模式运行 JobManager 概述: 控制一个应用程序执行的主进程, 每个应用程序都会被一个不同的JobManager所控制执行 JobManger会先接收到要执行的应用程序, 这个应用包括: 作业图 (JobGraph), 逻辑数据流图 (logical dataflow graph) 和打包了所有的类库及其他资源jar包. JobManager会把JobGraph转换成一个物理层面的数据流图 (ExecutionGraph), 该执行图包含了所有可以并发执行的任务. JobManager会向资源管理器 (ResourceManager) 请求执行任务必要的资源, 也就是任务管理器 (TaskManager) 上的插槽 (slot). 一旦它获取到了足够的资源, 就会将执行图分发到真正运行它们的 TaskManager 上. 而在运行过程中, JonManager会负责所有需要中央协调的操作, 如检查点 (checkpoints) 的协调. 功能: 将JobGraph 转换成Execution Graph, 最终将Execution Graph拿来运行 Schedule组件负责Task的调度 Checkpoint Coordinator组件负责协调整个任务的Checkpoint (包括开始和完成) 通过Actor System 与Task Manager进行通信 其它一些功能, 如Recovery Metadata, 用于进行故障恢复, 可以从Metadaa里面读取数据 TaskManager 概述: Flink中的工作进程. 通常在Flink中会有多个TaskManager运行, 每一个TaskManager都包含一定数量的插槽 (slots), 插槽的数量限制了TaskManager能够执行的任务数量. 启动之后, TaskManager会向资源管理器注册它的插槽, 收到资源管理器的指令后, TaskManager将会将一个或者多个插槽提供给JobManager调用, JobManager就可以向插槽分配任务 (Tasks) 来执行了. 在执行过程中, 一个TaskManager可以跟其他运行同一应用程序的TaskManager交换数据. 组件: Memory &amp; I/O Manager, 即内存I/O的管理 Network Manager, 用来对网络方面进行管理 Actor system, 用来负责网络的通信 ResourceManager 主要负责管理任务管理器 (TaskManager) 的插槽 (slot), TaskManager插槽是Flink中定义的处理资源单元. 提供YARN, Mesos, K8s及standlone部署. 当JobManager申请插槽资源时, ResourceManager会将有空闲插槽的TaskManager分配给JobManager. 如果ResourceManager没有足够的插槽来满足JobManager的请求, 它还可以向资源提供平台发起会话, 以提供启动TaskManager进程的容器. 子组件 SlotManager Dispatcher (分发器) 可跨作业运行, 它为应用提交提供了REST接口. 当一个应用被提交执行时, 分发器就会启动并将应用移交给一个JobManager. Dispatcher也会启动一个Web UI, 用来方便地展示和监控作业执行的信息. Dispatcher在架构中可能并不是必须的, 取决于应用程序提交运行的方式. 安装配置 版本选择 1.9: 开始支持Hive集成, 并未完全兼容 1.10: 完成Blink向Flink的合并 内存管理优化 Batch兼容Hive且生产可用 对SQL的DDL进行增强 支持Python UDF 原生k8s初步集成 SQL Client CLI Beta版本, 仅支持embedded模式 1.11: 新的JobManager内存模型 Blink作为默认的planner standlone 下载 wget https://archive.apache.org/dist/flink/flink-1.10.1/flink-1.10.1-bin-scala_2.11.tgz tar -zxf flink-1.10.1-bin-scala_2.11.tgz 单机standalone 启动/停止 # 启动 ./bin/start-cluster.sh # 停止 ./bin/stop-cluster.sh web页面查看: http://localhost:8081 多机 Standalone 集群 配置 conf/masters: host配置 conf/slaves: host配置 conf/flink-conf.yaml jobmanager.rpc.address: cdh00 启动集群 ./bin/start-cluster.sh HighAvailability（HA）部署和配置 JobManager 是整个系统中最可能导致系统不可用的角色, 在生产业务使用 Standalone 模式，则需要部署配置 HA (flink 1.6+) 配置 # conf/masters 增加 JobManager的host # conf/flink-conf.yaml # 配置high-availability mode high-availability: zookeeper # 配置zookeeper quorum（hostname和端口需要依据对应zk的实际配置） high-availability.zookeeper.quorum: cdh01:2181,cdh02:2181,cdh03:2181 # （可选）设置zookeeper的root目录 high-availability.zookeeper.path.root: /test_dir/test_standalone_root # （可选）相当于是这个standalone集群中创建的zk node的namespace high-availability.cluster-id: /test_dir/test_standalone # JobManager的meta信息放在dfs，在zk上主要会保存一个指向dfs路径的指针 high-availability.storageDir: hdfs:///test_dir/recovery/ Flink on YARN 分两种模式: Job Mode: 每次提交Flink任务都会创建一个专用的Flink集群, 任务完成后资源释放 Session Mode: 在YARN中提前初始化一个Flink集群, 以后所有Flink任务都提交到这个集群 优点: 资源按需使用，提高集群的资源利用率 任务有优先级，根据优先级运行作业 基于 Yarn 调度系统，能够自动化地处理各个角色的 Failover JobManager 进程和 TaskManager 进程都由 Yarn NodeManager 监控 如果 JobManager 进程异常退出，则 Yarn ResourceManager 会重新调度 JobManager 到其他机器 如果 TaskManager 进程异常退出，JobManager 会收到消息并重新向 Yarn ResourceManager 申请资源，重新启动 TaskManager 在Yarn上启动Long Running的Flink集群 (Session CLuster模式) 启动 # 查看命令参数 ./bin/yarn-session.sh -h # 创建一个 Yarn 模式的 Flink 集群 ./bin/yarn-session.sh -n 4 -jm 1024m -tm 4096m 提交任务 # 默认使用 /tmp/.yarn-properties-${user} 文件中保存的上一次创建 Yarn session 的集群信息 ./bin/flink run examples/streaming/WordCount.jar --input hdfs:///test_dir/input_dir/story --output hdfs:///test_dir/output_dir/output # 指定Yarn上的Application ID ./bin/flink run -yid application_1548056325049_0048 examples/streaming/WordCount.jar --input hdfs:///test_dir/input_dir/story --output hdfs:///test_dir/output_dir/output 参数: -n (--container): TaskManager 的数量 -jm (–jobManagerMemory): JobManager 的内存(单位 MB). -tm (–taskManagerMemory): 每个 taskmanager 的内存 (单位 MB). -qu: queue Specify YARN queue -s (--slots): 每个 TaskManager 的 slot 数量, 默认一个 slot 一个 core, 默认每个 taskmanager 的 slot 的个数为 1, 有时候可以多一些 taskmanager, 做冗余. -t (--ship): Ship files in the specified directory (t for transfer) -nm: yarn 的appName yarn ui 上名字). -d: 后台执行. 在Yarn上运行单个 Flink job（Job Cluster模式） 运行单个 Flink Job 后就退出 ./bin/flink run -m yarn-cluster -yn 2 examples/streaming/WordCount.jar --input hdfs:///test_dir/input_dir/story --output hdfs:///test_dir/output_dir/output Yarn 模式下的 HA 配置 yarn-site.xml &lt;!-- Yarn 集群级别 AM 重启的上限 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.am.max-attempts&lt;/name&gt; &lt;value&gt;100&lt;/value&gt; &lt;/property&gt; conf/flink-conf.yaml yarn.application-attempts: 10 # 1+ 9 retries conf/flink-conf.yaml # 配置high-availability mode high-availability: zookeeper # 配置zookeeper quorum（hostname和端口需要依据对应zk的实际配置） high-availability.zookeeper.quorum: cdh01:2181,cdh02:2181,cdh03:2181 # （可选）设置zookeeper的root目录 high-availability.zookeeper.path.root: /test_dir/test_standalone2_root # 删除这个配置 # high-availability.cluster-id: /test_dir/test_standalone2 # JobManager的meta信息放在dfs，在zk上主要会保存一个指向dfs路径的指针 high-availability.storageDir: hdfs:///test_dir/recovery2/ 编译安装CDH版本 下载制作包 # 下载制作包 git clone https://github.com/pkeropen/flink-parcel.git 修改配置文件 flink-parcel.properties #FLINK 下载地址 FLINK_URL=https://archive.apache.org/dist/flink/flink-1.10.1/flink-1.10.1-bin-scala_2.11.tgz #flink版本号 FLINK_VERSION=1.10.1 #扩展版本号 EXTENS_VERSION=BIN-SCALA_2.11 #操作系统版本，以centos为例 OS_VERSION=7 #CDH 小版本 CDH_MIN_FULL=5.2 CDH_MAX_FULL=5.15 #CDH大版本 CDH_MIN=5 CDH_MAX=5 生成parcel文件 ./build.sh parcel 生成csd文件 ./build.sh csd_on_yarn CDH 中安装flink服务 配置flink parcel和csd文件 激活flink 添加flink服务 Flink State 与 CheckPoint state keyed state 只能用于 KeyedStream的函数与操作中 operator state (non-keyed state) 每一个operator state都仅与一个operator的实例绑定 checkpoint state就是checkpoint所做的主要持久化备份的主要数据 常见的source state, 例如记录当前source的offset 执行机制 Checkpoint Coordinator向所有source节点trigger Check-point source 节点向下游广播barrier (实现Chandy-Lamp-ort分布式快照算法核心), 下游task只有接收到所有input的barrier才会执行相应的Checkpoint 当task完成state备份后, 会将备份数据的地址 (state handle) 通知给 Checkpoint coordinator 下游sink节点收集齐上游input barrier之后, 会执行本地快照, 同样sink节点完成自身的checkpoint之后, 会将 state haddle返回通知Coordinator 最后当Checkpoint Coordinator收集齐所有task的state handle, 就认为这一次的checkpoint全局完成了, 向持久化存储中再备份一个Checkpoint meta文件 程序与数据流 所有的Flink程序都是由三部分组成: source, Transformation 和 Sink 流处理API map/flatMap/filter keyBy 聚合操作: 滚动聚合算子 (Rolling Aggregation) sum min max minBy maxBy reduce: 合并当前的元素和上次聚合的结构, 产生一个新的值，返回的流中包含每一次聚合的结果，而不是 只返回最后一次聚合的最终结果。 split 和 select connect 和 coMap/coFlatMap Union UDF MapFunction, FilterFunction, ProcessFunction等 Lambda Functions Rich Functions Source 定义数据源 预定义: 文件: readTextFile, readFile 集合: fromElements, fromCollection, fromParallelCollection, generateSequence Socket: socketTextStream connectors: RabbitMQ/kafka, Apache Bahir 自定义source: 直接或间接实现SourceFunction接口 Sink 对外的输出通过sink来完成 预定义: 文件: writeAsText(), writeAsCsv(), writeUsingOutputFormat, FileOutputFormat socket: writeToSocket console: print, printToErr connectors: hdfs/RabbitMQ/kafka/redis/es/nifi, Apache Bahir 自定义sink: 直接或者间接实现SinkFunction接口 窗口 分类: CountWindow: 按照指定的数据条数生成一个 Window，与时间无关。 TimeWindow: 按照时间生成 Window。 窗口分配器: window()方法, 必须在keyBy之后才能使用. TimeWindow 滚动窗口(Tumbling Windows) 概述: 将数据依据固定的窗口长度对数据进行切片.Flink的默认时间窗口, 根据Processing Time进行窗口划分. 特点: 时间对齐, 窗口长度固定, 没有重叠. 适用场景: 适合做 BI 统计等(做每个时间段的聚合计算). API timeWindow(window_size) 滑动窗口(Sliding Windows) 概述: 滑动窗口是固定窗口的更广义的一种形式, 滑动窗口由固定的窗口长度和滑动间隔组成. 特点: 窗口长度固定, 可以有重叠. 适用场景:对最近一个时间段内的统计 (求某接口最近 5min 的失败率来决定是否要报警). API timeWindow(window_size, sliding_size) 会话窗口(Session Windows) 概述: 由一系列事件组合一个指定时间长度的 timeout 间隙组成, 类似于 web 应用的session, 也就是一段时间没有接收到新数据就会生成新的窗口. 特征: 时间无对齐 CountWindow 按照指定的数据条数生成一个 Window, 与时间无关. 滚动窗口 概述: 默认的 CountWindow 是一个滚动窗口，只需要指定窗口大小即可，当元素数量达到窗口大小时，就会触发窗口的执行。 API countWindow(window_size) 滑动窗口 API countWindow(window_size, sliding_size) window function 类别 增量聚合函数(incremental aggregation functions) 每条数据到来就进行计算，保持一个简单的状态。 典型的增量聚合函数有 ReduceFunction, AggregateFunction。 全窗口函数(full window functions) 先把窗口所有数据收集起来，等到计算的时候会遍历所有数据。 ProcessWindowFunction 就是一个全窗口函数。 其他 trigger() —— 触发器 定义 window 什么时候关闭，触发计算并输出结果 evitor() —— 移除器 定义移除某些数据的逻辑 allowedLateness() —— 允许处理迟到的数据 sideOutputLateData() —— 将迟到的数据放入侧输出流 getSideOutput() —— 获取侧输出流 时间语义与Watermark 时间语义 Event Time: 是事件创建的时间.它通常由事件中的时间戳描述, 例如采集的日志数据中, 每一条日志都会记录自己的生成时间, Flink 通过时间戳分配器访问事件时间戳. Ingestion Time: 是数据进入 Flink 的时间. Processing Time: 是每一个执行基于时间操作的算子的本地系统时间, 与机器相关, 默认的时间属性就是 Processing Time. Watermark 作用: 用于处理乱序事件的，而正确的处理乱序事件，通常用 Watermark 机制结合 window 来实现。 遇到一个时间戳达到了窗口关闭时间，不应该立刻触发窗口计算，而是等待一段时间，等迟到的数据来了再关闭窗口 Timestamp 分配和 Watermark生成 方式: 在SourceFunction中, 通过collectWithTimestamp方法发送一条数据或者emitWatermark去产生一条watermark (表示接下来不会再有时间戳小于等于这个数值记录) 调用DataStream.assiginTimestampAndWatermarks Watermark 传播 策略 watermark会以广播的形式在算子之间传播 若接受到Long.MAX_VALUE这个数值的watermark, 相当于是终止标志 对于有多个输入的算子watermark原则是: 单输入取其大, 多输入取小 (类似木桶效应) 特点: 幂等 不足: 时钟不同步将会带来性能开销, TimeService设置过期 Table API 与 SQL Flink 对批处理和流处理，提供了统一的上层 API Table API 是一套内嵌在 Java 和 Scala 语言中的查询API，它允许以非常直观 的方式组合来自一些关系运算符的查询 Flink 的 SQL 支持基于实现了 SQL 标准的 Apache Calcite (SQL 解析工具) DataStream 与 Table 互转 // DataStream 转为 Table tableEnv.fromDataStream(dataStream) // Table 转为 DataStream (需要指定类型) 追加模式 tableEnv.toAppendStream[Row](resultTable) // Table 转为 DataStream (需要指定类型) 撤回模式: 聚合操作后的方式 tableEnv .toRetractStream[(String, Long)](aggResultTable) SQL client 当前的SQL客户端仅支持嵌入式模式 # 启动, 暂不支持远程连接 ./bin/sql-client.sh embedded Flink CEP 简介: CEP 的意思是复杂事件处理，例如：起床–&gt;洗漱–&gt;吃饭–&gt;上班等一系列串联起来的事件流形成的模式称为 CEP。如果发现某一次起床后没有刷牙洗脸亦或是吃饭就直接上班，就可以把这种非正常的事件流匹配出来进行分析，看看今天是不是起晚了. 应用场景 风险控制: 对用户异常行为模式进行实时检测，当一个用户发生了不该发生的行为，判定这个用户是不是有违规操作的嫌疑。 策略营销: 用预先定义好的规则对用户的行为轨迹进行实时跟踪，对行为轨迹匹配预定义规则的用户实时发送相应策略的推广。 运维监控: 灵活配置多指标、多依赖来实现更复杂的监控模式。 原理 Flink CEP 内部是用 NFA（非确定有限自动机）来实现的，由点和边组成的一个状态图，以一个初始状态作为起点，经过一系列的中间状态，达到终态。 点分为起始状态、中间状态、最终状态三种. 边分为 take、ignore、proceed 三种. take：必须存在一个条件判断，当到来的消息满足 take 边条件判断时，把这个消息放入结果集，将状态转移到下一状态。 ignore：当消息到来时，可以忽略这个消息，将状态自旋在当前不变，是一个自己到自己的状态转移。 proceed：又叫做状态的空转移，当前状态可以不依赖于消息到来而直接转移到下一状态. 组件 Event Stream pattern 定义 pattern 检测 参考 release-notes flink-docs-release flink-parcel Demo：基于 Flink SQL 构建流式应用 Flink CEP   ]]></content></entry><entry><title>Oracle GoldenGate 实时同步Oracle数据到Kafka安装及配置</title><url>/post/sync-oracle-data-to-kafka-in-real-time/</url><categories><category>数据科学</category></categories><tags><tag>大数据</tag></tags><content type="html"><![CDATA[  简介 MySQL实时同步到Kafk可以采用canal, Oracle实时同步到Kafka可以采用OGG, 配置过程比canal略复杂 OGG即Oracle GoldenGate是Oracle的同步工具, 能够实现大量交易数据的实时捕捉、变换和投递，实现源数据库与目标数据库的数据同步，保持亚秒级的数据延迟。 相关进程 GoldenGate主要包含Manager进程、Extract进程、Pump进程、Replicat进程 OGG相关目录 dirchk：检查点文件，记录了该进程的检查点信息 dirdat：trail日志文件，存放收取接手的日志文件 dirdef：用来存放通过DEFGEN工具生成的源或目标端数据定义文件 dirpcs：用来存放进程状态文件 dirprm：用来存放参数文件，该进程所配置的参数（edit param 进程组名 就是配置该文件） dirrpt：用来存放进程报告（report）文件，可以查看该进程运行时的报错信息等（view report 进程组名 就是看该文件） dirsql：用来存放SQL脚本文件 dirtmp：当事物所需要的内存超过已分配内存时，缺省存在此目录 源端安装配置 下载解压 # 查询所需要的版本: https://edelivery.oracle.com/osdc/faces/SoftwareDelivery # 下载 Oracle GoldenGate 11.2.1.0.3 for Oracle on Linux x86-64 tar xf fbo_ggs_Linux_x64_ora11g_64bit.tar -C /opt/ogg # 使oracle用户有ogg的权限，后面有些需要在oracle用户下执行才能成功 chown -R oracle:oinstall /opt/ogg 配置 # /home/oracle/.bash_profile export OGG_HOME=/opt/ogg export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/usr/lib export PATH=$OGG_HOME:$PATH source .bash_profile # 测试 ggsci oracle打开归档模式 sqlplus / as sysdba # 查看当前是否为归档模式, Disabled为关闭 archive log list # Database log mode	No Archive Mode # Automatic archival	Disabled # Archive destination	USE_DB_RECOVERY_FILE_DEST # Oldest online log sequence 18 # Current log sequence	20 # 执行 conn / as sysdba # 以DBA身份连接数据库 shutdown immediate # 立即关闭数据库 startup mount # 启动实例并加载数据库，但不打开 alter database archivelog; # 更改数据库为归档模式 alter database open; # 打开数据库 alter system archive log start; # 启用自动归档 # 查看 archive log list # Database log mode	Archive Mode # Automatic archival	Enabled # Archive destination	USE_DB_RECOVERY_FILE_DEST # Oldest online log sequence 18 # Next log sequence to archive 20 # Current log sequence	20 Oracle打开日志相关: OGG基于辅助日志等进行实时传输，故需要打开相关日志确保可获取事务内容，通过下面的命令查看该状态 select force_logging, supplemental_log_data_min from v$database; -- FORCE_ SUPPLEMENTAL_LOG -- ------ ---------------- -- NO NO -- 修改命令 alter database force logging; alter database add supplemental log data; -- 再次查看 select force_logging, supplemental_log_data_min from v$database; Oracle 创建复制用户 # 首先root用户建立相关文件夹，并赋予权限 cd $ORACLE_BASE mkdir -p oggdata/orcl -- 执行sql create tablespace oggtbs datafile &#39;/ora/oracle/oggdata/orcl/oggtbs01.dbf&#39; size 1000M autoextend on; create user ogg identified by ogg default tablespace oggtbs; grant dba to ogg; OGG初始化 ggsci # 在当前目录创建文件夹 create Oracle创建测试表 create user test_ogg identified by test_ogg default tablespace users; grant dba to test_ogg; conn test_ogg/test_ogg; create table test_ogg(id int ,name varchar(20),primary key(id)); 目标端 (kafka) 配置 下载解压 # 查询所需要的版本: https://edelivery.oracle.com/osdc/faces/SoftwareDelivery # Oracle GoldenGate for Big Data 12.3.0.1.0 on Linux x86-64 tar -xf ggs_Adapters_Linux_x64.tar -C /opt/ogg/ 配置 # vim /etc/profile export OGG_HOME=/opt/ogg export LD_LIBRARY_PATH=$JAVA_HOME/jre/lib/amd64:$JAVA_HOME/jre/lib/amd64/server:$JAVA_HOME/jre/lib/amd64/libjsig.so:$JAVA_HOME/jre/lib/amd64/server/libjvm.so:$OGG_HOME/lib export PATH=$OGG_HOME:$PATH # 测试 source /etc/profile cd $OGG_HOME ggsci 初始化目录 cd $OGG_HOME create subdirs OGG源端配置 配置OGG的全局变量 su oracle cd $OGG_HOME ggsci # 配置用户 dblogin userid ogg password ogg # 编辑 edit param ./globals # 添加 oggschema ogg 配置管理器mgr # 编辑 edit param mgr # 添加 PORT 7809 DYNAMICPORTLIST 7810-7909 AUTORESTART EXTRACT *,RETRIES 5,WAITMINUTES 3 PURGEOLDEXTRACTS ./dirdat/*,usecheckpoints, minkeepdays 3 # PORT 即mgr的默认监听端口； # DYNAMICPORTLIST 动态端口列表，当指定的mgr端口不可用时，会在这个端口列表中选择一个，最大指定范围为256个； # AUTORESTART 重启参数设置表示重启所有EXTRACT进程，最多5次，每次间隔3分钟； # PURGEOLDEXTRACTS 即TRAIL文件的定期清理 添加复制表 add trandata test_ogg.test_ogg info trandata test_ogg.test_ogg 配置extract进程 # 编辑 edit param extkafka # 添加 extract extkafka dynamicresolution SETENV (ORACLE_SID = &#34;orcl&#34;) SETENV (NLS_LANG = &#34;american_america.AL32UTF8&#34;) GETUPDATEBEFORES NOCOMPRESSDELETES NOCOMPRESSUPDATES userid ogg,password ogg exttrail /opt/ogg/dirdat/to table test_ogg.test_ogg; # 第一行指定extract进程名称； # dynamicresolution动态解析； # SETENV设置环境变量，这里分别设置了Oracle数据库以及字符集； # userid ggs,password ggs即OGG连接Oracle数据库的帐号密码，这里使用2.5中特意创建的复制帐号； # exttrail定义trail文件的保存位置以及文件名，注意这里文件名只能是2个字母，其余部分OGG会补齐；table即复制表的表名，支持*通配，必须以 &#34;;&#34; 结尾 # 添加extract进程 add extract extkafka,tranlog,begin now # 添加trail文件的定义与extract进程绑定 add exttrail /opt/ogg/dirdat/to,extract extkafka # 启停命令 # stop extkafka # start extkafka 配置pump进程: pump进程本质上来说也是一个extract，只不过他的作用仅仅是把trail文件传递到目标端，配置过程和extract进程类似，只是逻辑上称之为pump进程 # 编辑 edit param pukafka # 添加 extract pukafka passthru dynamicresolution userid ogg,password ogg rmthost cdh01 mgrport 7809 rmttrail /opt/ogg/dirdat/to table test_ogg.test_ogg; # 第一行指定extract进程名称； # passthru即禁止OGG与Oracle交互，我们这里使用pump逻辑传输，故禁止即可； # dynamicresolution动态解析； # userid ogg,password ogg即OGG连接Oracle数据库的帐号密码 # rmthost和mgrhost即目标端(kafka)OGG的mgr服务的地址以及监听端口； # rmttrail即目标端trail文件存储位置以及名称。 # 分别将本地trail文件和目标端的trail文件绑定到extract进程 add extract pukafka,exttrailsource /opt/ogg/dirdat/to add rmttrail /opt/ogg/dirdat/to,extract pukafka 配置define文件: Oracle与MySQL，Hadoop集群（HDFS，Hive，kafka等）等之间数据传输可以定义为异构数据类型的传输，故需要定义表之间的关系映射 # 编辑 edit param test_ogg # 添加 defsfile /opt/ogg/dirdef/test_ogg.test_ogg userid ogg,password ogg table test_ogg.test_ogg; # 退出 quit # 进行OGG主目录下执行以下命令 ./defgen paramfile dirprm/test_ogg.prm # 将生成的/opt/ogg/dirdef/test_ogg.test_ogg发送的目标端ogg目录下的dirdef里 scp -r /opt/ogg/dirdef/test_ogg.test_ogg root@cdh01:/opt/ogg/dirdef/ OGG目标端配置 开启kafka服务 配置管理器mgr cd $OGG_HOME ggsci # 编辑 edit param mgr # 添加 PORT 7809 DYNAMICPORTLIST 7810-7909 AUTORESTART EXTRACT *,RETRIES 5,WAITMINUTES 3 PURGEOLDEXTRACTS ./dirdat/*,usecheckpoints, minkeepdays 3 配置checkpoint: checkpoint即复制可追溯的一个偏移量记录，在全局配置里添加checkpoint表即可 # 编辑 edit param ./GLOBALS # 配置 CHECKPOINTTABLE test_ogg.checkpoint 配置replicate进程 # 编辑 edit param rekafka # 添加 REPLICAT rekafka sourcedefs /opt/ogg/dirdef/test_ogg.test_ogg TARGETDB LIBFILE libggjava.so SET property=dirprm/kafka.props REPORTCOUNT EVERY 1 MINUTES, RATE GROUPTRANSOPS 10000 MAP test_ogg.test_ogg, TARGET test_ogg.test_ogg; # REPLICATE rekafka定义rep进程名称； # sourcedefs即在4.6中在源服务器上做的表映射文件； # TARGETDB LIBFILE即定义kafka一些适配性的库文件以及配置文件，配置文件位于OGG主目录下的dirprm/kafka.props； # REPORTCOUNT即复制任务的报告生成频率； # GROUPTRANSOPS为以事务传输时，事务合并的单位，减少IO操作；MAP即源端与目标端的映射关系 配置kafka.props # cd /opt/ogg/dirprm/ &amp;&amp; vim kafka.props # handler类型 gg.handlerlist=kafkahandler gg.handler.kafkahandler.type=kafka # Kafka生产者配置文件 gg.handler.kafkahandler.KafkaProducerConfigFile=custom_kafka_producer.properties # kafka的topic名称，无需手动创建 # gg.handler.kafkahandler.topicMappingTemplate=test_ogg（新版topicName属性的设置方式） gg.handler.kafkahandler.topicName=test_ogg # 传输文件的格式，支持json，xml等 gg.handler.kafkahandler.format=json gg.handler.kafkahandler.format.insertOpKey = I gg.handler.kafkahandler.format.updateOpKey = U gg.handler.kafkahandler.format.deleteOpKey = D gg.handler.kafkahandler.format.truncateOpKey=T gg.handler.kafkahandler.format.includePrimaryKeys=true # OGG for Big Data中传输模式，即op为一次SQL传输一次，tx为一次事务传输一次 gg.handler.kafkahandler.mode=op # 类路径 gg.classpath=dirprm/:/opt/cloudera/parcels/KAFKA/lib/kafka/libs/*:/opt/ogg/:/opt/ogg/lib/* # vim custom_kafka_producer.properties # kafkabroker的地址 bootstrap.servers=cdh01:9092,cdh02:9092,cdh03:9092 acks=1 # 压缩类型 compression.type=gzip # 重连延时 reconnect.backoff.ms=1000 value.serializer=org.apache.kafka.common.serialization.ByteArraySerializer key.serializer=org.apache.kafka.common.serialization.ByteArraySerializer batch.size=102400 linger.ms=10000 添加trail文件到replicate进程 cd $OGG_HOME ggsci add replicat rekafka exttrail /opt/ogg/dirdat/to,checkpointtable test_ogg.checkpoint 测试 创建topic kafka-topics --create --zookeeper cdh01:2181 --replication-factor 3 --partitions 3 --topic test_ogg 启动所有进程 # 在源端和目标端的OGG命令行下使用start [进程名]的形式启动所有进程。 # 启动顺序按照源mgr——目标mgr——源extract——源pump——目标replicate来完成。 # 全部需要在ogg目录下执行ggsci目录进入ogg命令行。 # 源端 start mgr start extkafka start pukafka # 目标端 start mgr start rekafka 查看: 通过info all 或者info [进程名] 查看状态，所有的进程都为RUNNING才算成功 # 源端 info all # 目标端 info all # 日志查看 less ggser.log ggsci &gt; view report rekafka 测试同步更新效果 源端执行sql -- 源端执行sql conn test_ogg/test_ogg insert into test_ogg values(1,&#39;test&#39;); commit; update test_ogg set name=&#39;zhangsan&#39; where id=1; commit; delete test_ogg where id=1; commit; 查看文件状态 # 源端 ls -l /opt/ogg/dirdat/to* # 目标端 ls -l /opt/ogg/dirdat/to* 查看kafka中数据 kafka-console-consumer --bootstrap-server cdh01:9092,cdh02:9092,cdh03:9092 --topic test_ogg --from-beginning # {&#34;table&#34;:&#34;TEST_OGG.TEST_OGG&#34;,&#34;op_type&#34;:&#34;I&#34;,&#34;op_ts&#34;:&#34;2020-10-30 17:43:30.247674&#34;,&#34;current_ts&#34;:&#34;2020-10-30T17:43:35.632000&#34;,&#34;pos&#34;:&#34;00000000000000001061&#34;,&#34;primary_keys&#34;:[&#34;ID&#34;],&#34;after&#34;:{&#34;ID&#34;:1,&#34;NAME&#34;:&#34;test&#34;}} # {&#34;table&#34;:&#34;TEST_OGG.TEST_OGG&#34;,&#34;op_type&#34;:&#34;I&#34;,&#34;op_ts&#34;:&#34;2020-10-30 17:44:05.247770&#34;,&#34;current_ts&#34;:&#34;2020-10-30T17:44:10.707000&#34;,&#34;pos&#34;:&#34;00000000000000001201&#34;,&#34;primary_keys&#34;:[&#34;ID&#34;],&#34;after&#34;:{&#34;ID&#34;:2,&#34;NAME&#34;:&#34;orcal&#34;}} # {&#34;table&#34;:&#34;TEST_OGG.TEST_OGG&#34;,&#34;op_type&#34;:&#34;I&#34;,&#34;op_ts&#34;:&#34;2020-10-30 17:44:31.247814&#34;,&#34;current_ts&#34;:&#34;2020-10-30T17:44:35.728000&#34;,&#34;pos&#34;:&#34;00000000000000001343&#34;,&#34;primary_keys&#34;:[&#34;ID&#34;],&#34;after&#34;:{&#34;ID&#34;:3,&#34;NAME&#34;:&#34;db&#34;}} # {&#34;table&#34;:&#34;TEST_OGG.TEST_OGG&#34;,&#34;op_type&#34;:&#34;I&#34;,&#34;op_ts&#34;:&#34;2020-10-30 17:45:06.247722&#34;,&#34;current_ts&#34;:&#34;2020-10-30T17:45:11.755000&#34;,&#34;pos&#34;:&#34;00000000000000001480&#34;,&#34;primary_keys&#34;:[&#34;ID&#34;],&#34;after&#34;:{&#34;ID&#34;:4,&#34;NAME&#34;:&#34;ok&#34;}} # {&#34;table&#34;:&#34;TEST_OGG.TEST_OGG&#34;,&#34;op_type&#34;:&#34;I&#34;,&#34;op_ts&#34;:&#34;2020-10-30 17:45:06.247722&#34;,&#34;current_ts&#34;:&#34;2020-10-30T17:45:11.757000&#34;,&#34;pos&#34;:&#34;00000000000000001617&#34;,&#34;primary_keys&#34;:[&#34;ID&#34;],&#34;after&#34;:{&#34;ID&#34;:5,&#34;NAME&#34;:&#34;sd&#34;}} # {&#34;table&#34;:&#34;TEST_OGG.TEST_OGG&#34;,&#34;op_type&#34;:&#34;U&#34;,&#34;op_ts&#34;:&#34;2020-10-30 17:45:54.247822&#34;,&#34;current_ts&#34;:&#34;2020-10-30T17:45:59.790000&#34;,&#34;pos&#34;:&#34;00000000000000001871&#34;,&#34;primary_keys&#34;:[&#34;ID&#34;],&#34;before&#34;:{&#34;ID&#34;:1,&#34;NAME&#34;:&#34;test&#34;},&#34;after&#34;:{&#34;ID&#34;:1,&#34;NAME&#34;:&#34;oracle&#34;}} # {&#34;table&#34;:&#34;TEST_OGG.TEST_OGG&#34;,&#34;op_type&#34;:&#34;D&#34;,&#34;op_ts&#34;:&#34;2020-10-30 17:50:37.247804&#34;,&#34;current_ts&#34;:&#34;2020-10-30T17:50:42.948000&#34;,&#34;pos&#34;:&#34;00000000000000001990&#34;,&#34;primary_keys&#34;:[&#34;ID&#34;],&#34;before&#34;:{&#34;ID&#34;:4,&#34;NAME&#34;:&#34;ok&#34;}} # {&#34;table&#34;:&#34;TEST_OGG.TEST_OGG&#34;,&#34;op_type&#34;:&#34;D&#34;,&#34;op_ts&#34;:&#34;2020-10-30 17:50:37.247804&#34;,&#34;current_ts&#34;:&#34;2020-10-30T17:50:42.949000&#34;,&#34;pos&#34;:&#34;00000000000000002127&#34;,&#34;primary_keys&#34;:[&#34;ID&#34;],&#34;before&#34;:{&#34;ID&#34;:2,&#34;NAME&#34;:&#34;orcal&#34;}} 参考 利用ogg实现oracle到kafka的增量数据实时同步 Using the Kafka Handler   ]]></content></entry><entry><title>单双因素方差分析及Python实现</title><url>/post/anova-by-python/</url><categories><category>数据科学</category></categories><tags><tag>数理统计</tag></tags><content type="html"> 概述 方差分析 (analysis of variance, ANOVA) 在20世纪20年代发展起来的一种统计方法, 由英国统计学家费希尔在进行试验设计时为解释试验数据而首先引入. 方差分析就是通过检验个总体的均值是否相等来判断分类型自变量对数值型因变量是否有显著影响。 从表面上看, 方差分析是检验多个总体均值是否相等的方法, 但本质上是它所研究的是分类型自变量对数值型因变量的影响. 例如, 变量之间有没有关系, 关系的强度如何等. 根据所分析的分类型自变量多少, 可以分为单因素方差分析 (one-way analysis of variance) 和 双因素方差分析 (two-way analysis of variance) . 方差分析的基本思想和原理 在判断均值之间是否有差异时需要借助方差, 即通过对数据误差来源的分析来判断不同总体的均值是否相等, 进而分析自变量对因变量是否有显著影响. 在方差分析中, 数据的误差使用平方和来表示. 方差分析中的基本假定 每个总体都应该服从正态分布 各个总体的方差$\sigma^2$必须相同. 观测值是独立的. 问题一般提法 设因素有$k$个水平, 每个水平的均值分别用$\mu_1, \mu_2, \dotsb, \mu_k$表示, 要检验$k$个水平 (总体) 的均值是否相等, 需要提出如下假设: $H_0: \mu_1 = \mu_2 = \dotsb = \mu_k$ 自变量对因变量没有显著影响 $H_1: \mu_1 , \mu_2 , \dotsb , \mu_k$不全相等 自变量对因变量没有显著影响 单因素方差分析 单因素方差分析研究的是一个分类型自变量对一个数值型因变量的影响. 分析步骤 提出假设 构造检验的统计量 计算各样本的均值 $$\bar{x_i} = \frac{\sum_{j=1}^{n_i} x_{ij}}{n_i}, i =1,2,\dotsb, k$$
式中, $n_i$为第$i$个总体的样本量; $x_{ij}$为第$i$个总体的第$j$个观测值. 计算全部观测值的总均值 $$\bar{\bar{x}} = \frac{\sum_{i=1}^k \sum_{j=1}^{n_i} x_{ij}}{n} = \frac{\sum_{i=1}^k n_i \bar{x_i}}{n}$$
式中, …</content></entry><entry><title>机器学习中的特征工程</title><url>/post/featureenginering-of-machinelearning/</url><categories><category>数据科学</category></categories><tags><tag>机器学习</tag></tags><content type="html"><![CDATA[  概述 机器学习将数据拟合到数学模型中来获得结论或者做出预测。这些模型吸纳特征作为输入。特征就是原始数据某方面的数学表现。在机器学习流水线中特征位于数据和模型之间。 特征工程是一项从数据中提取特征，然后转换成适合机器学习模型的格式的艺术。这是机器学习流水线关键的一步，因为正确的特征可以减轻建模的难度，并因此使流水线能输出更高质量的结果。 数据和特征决定了机器学习的上限, 而模型和算法只是逼近这个上限而已. 流程: 分类 特征使用 数据选择 可用性 特征获取 特征来源 特征存储 特征处理 数据清洗 特征预处理 特征监控 现有特征 新特征 数据清洗 数据样本采集(抽样) 样本要具有代表性 样本比例要平衡以及样本不平衡时如何处理 考虑全量数据 缺失值 缺失值的比例超过50%以上时, 此字段通常舍弃不用, 不做任何填补. 另一种处理, 将此字段的值根据是否缺失, 生成指示变量, 将原字段舍弃, 并仅使用此指示变量作为输入变量. 异常值(空值)处理 识别异常值和重复值 Pandas: isnull()/duplicated() 直接丢弃(包括重复值) Pandas: drop()/dropna()/drop_duplicated() 当是否有异常当做一个新的属性, 替代原值 Pandas: fillna() 集中值指代: 除异常值以外的均值, 中位数, 众数等 Pandas: fillna() 边界值指代: 连续值中使用四分位间距确定的上下边界来确定超过上下边界的数 Pandas: fillna() 插值 Pandas: interpolate()&ndash;Series 特征预处理 分类 特征选择 特征变换 对数化, 离散化, 数据平滑, 归一化(标准化), 数值化, 正规化 特征降维 特征衍生 特征选择 概述: 剔除与标注不相关或者冗余的特征 规约思路: 过滤思想 离散与连续数据类型处理 包裹思想 遍历特征子集 嵌入思想 建立简单回归模型 特征变换 对指化 离散化 概述: 将连续变量分成几段(bins) 方法: 等频 等距 自因变量优化 优点: 可使数据精简, 降低数据的复杂度, 让数据更容易被解释. 可支持许多无法处理数值型属性的分类算法, 如贝叶斯分类算法, 以关联规则为基础的分类算法. 可提高分类器的稳定性, 进而提升分类模型的准确度. 可找出条件属性在目标属性上的趋势(Trend), 有助于未来的解读. 无量纲化 分类: 区间缩放, 标准化和归一化 使用时机 1、在后续的分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA、LDA这些需要用到协方差分析进行降维的时候，同时数据分布可以近似为正太分布，标准化方法(Z-score standardization)表现更好. 2. 在不涉及距离度量、协方差计算、数据不符合正太分布的时候，可以使用区间缩放法或其他归一化方法。比如图像处理中，将RGB图像转换为灰度图像后将其值限定在[0 255]的范围. 区间缩放(对列向量处理) Min-Max 数据缩放到(0,1)之间 $$x&rsquo; = \frac{x - x_{\min}}{x_{\max} - x_{\min}}$$ 示例 import numpy as np from sklearn.preprocessing import MinMaxScaler # 归一化 MinMaxScaler().fit_transform(np.array([1,4,10,15,21]).reshape(-1,1)) # array([[0. ], # [0.15], # [0.45], # [0.7 ], # [1. ]]) MinMaxScaler().inverse_transform(result) # 将归一化后的结果逆转 # 当X中特征太多时, 使用partial_fit作为训练接口 标准化(对列向量处理) 标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。 Z-score $$x&rsquo;= \frac{x - \bar{x}}{\sigma}$$ 示例 import numpy as np from sklearn.preprocessing import StandardScaler # 标注化 StandardScaler().fit_transform(np.array([1,1,1,1,0,0,0,0]).reshape(-1,1)) # array([[ 1.], # [ 1.], # [ 1.], # [ 1.], # [-1.], # [-1.], # [-1.], # [-1.]]) 归一化(对行向量处理) 归一化目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”. 公式 $$x&rsquo;= \frac{x_{i}}{\sqrt{\sum_{j=1}^{n} |x_{j}|^{2}}}$$ 示例 from sklearn.datasets import load_iris #导入IRIS数据集 iris = load_iris() #特征矩阵 iris.data #目标向量 iris.target from sklearn.preprocessing import Normalizer #归一化，返回值为归一化后的数据 Normalizer().fit_transform(iris.data) 数值化 数据分类: 定类: 数值化-独热(One-Hot Encode) 定序: 数值化-标签化 定距: 归一化 定比 示例 # 使用pandas import pandas as pd # onehot_cols 为需要进行哑变量的数组列 pd.get_dummies(whole_df[onehot_cols], dummy_na = True) # 使用sklearn from sklearn.preprocessing import LabelEncoder,OneHotEncoder LabelEncoder().fit_transform(np.array([&#34;Down&#34;, &#34;Up&#34;, &#34;Up&#34;, &#34;Down&#34;]).reshape(-1,1)) # array([0, 1, 1, 0]) lb_encoder = LabelEncoder() lb_tran = lb_encoder.fit_transform(np.array([&#34;Red&#34;, &#34;Yellow&#34;, &#34;Blue&#34;, &#34;Green&#34;])) oht_encoder = OneHotEncoder().fit(lb_tran.reshape(-1,1)) oht_encoder.transform(lb_encoder.transform(np.array([&#34;Yellow&#34;, &#34;Blue&#34;, &#34;Green&#34;, &#34;Green&#34;, &#34;Red&#34;])).reshape(-1,1)) oht_encoder.transform(lb_encoder.transform(np.array([&#34;Yellow&#34;, &#34;Blue&#34;, &#34;Green&#34;, &#34;Green&#34;, &#34;Red&#34;])).reshape(-1,1)).toarray() # array([[0., 0., 0., 1.], # [1., 0., 0., 0.], # [0., 1., 0., 0.], # [0., 1., 0., 0.], # [0., 0., 1., 0.]]) one-hot encoding与dummy encoding one-hot encoding: 将离散型特征的每一种取值都看成一种状态，若你的这一特征中有N个不相同的取值，那么我们就可以将该特征抽象成N种不同的状态，one-hot编码保证了每一个取值只会使得一种状态处于“激活态”，也就是说这N种状态中只有一个状态位值为1，其他状态位都是0。 dummy encoding: 任意的将一个状态位去除。 最好是选择正则化 + one-hot编码；哑变量编码也可以使用，不过最好选择前者。虽然哑变量可以去除one-hot编码的冗余信息，但是因为每个离散型特征各个取值的地位都是对等的，随意取舍未免来的太随意。 正规化(规范化) 常用 L1距离: $$x&rsquo;= \frac{x_i}{\sum_{j=1}^n|x_j|}$$ L2距离: $$x&rsquo;= \frac{x_i}{\sqrt{\sum_{j=1}^n|x_j|^2}}$$ 场景: 直接用在特征上(比较少) 用在每个对象的各个特征的表示(特征矩阵的行) 模型的参数上(回归模型使用较多) 示例 from sklearn.preprocessing import Normalizer # L1距离 Normalizer(norm=&#39;l1&#39;).fit_transform(np.array([1,1,3,-1,2]).reshape(-1,1)) # array([[ 1.], # [ 1.], # [ 1.], # [-1.], # [ 1.]]) # L2距离 Normalizer(norm=&#39;l2&#39;).fit_transform(np.array([[1,1,3,-1,2]])) # array([[ 0.25, 0.25, 0.75, -0.25, 0.5 ]]) 连续变量压缩 主成分分析, 因子分析, 变量聚类 特征降维 PCA, 奇异值分解等线性降维 PCA: 求特征协方差矩阵 求协方差的特征值和特征向量 将特征值按照大小排序, 选择其中最大的k个 将样本点投影到选取的特征向量上 LDA降维 概述 线性判别式分析(Liner Discriminant Analysis) 核心思想: 投影变化后同一标注内距离尽可能小 不同标注间距离尽可能大 分类变量压缩技术 水平聚类, WOE打分 IV(Information Value)值与WOE(Weight of Evidence)打分 特征衍生 常用方法: 四则运算: 加减乘除 求导与高阶求导 人工归纳 参考 feature-engineering-book   ]]></content></entry><entry><title>机器学习中的评估指标</title><url>/post/evaluation-of-machinelearning/</url><categories><category>数据科学</category></categories><tags><tag>机器学习</tag></tags><content type="html"><![CDATA[  分类问题 当正负样本分布极不均衡时, 准确率将失去意义, 通常使用AUC作为指标 混淆矩阵（Confusion Matrix） 矩阵中的每一行代表实例的预测类别，每一列代表实例的真实类别。 真正(True Positive , TP)：被模型预测为正的正样本。 假正(False Positive , FP)：被模型预测为正的负样本。 假负(False Negative , FN)：被模型预测为负的正样本。 真负(True Negative , TN)：被模型预测为负的负样本。 真正率(True Positive Rate,TPR)：TPR=TP/(TP+FN)，即被预测为正的正样本数 /正样本实际数。 召回率 假正率(False Positive Rate,FPR) ：FPR=FP/(FP+TN)，即被预测为正的负样本数 /负样本实际数。 假负率(False Negative Rate,FNR) ：FNR=FN/(TP+FN)，即被预测为负的正样本数 /正样本实际数。 真负率(True Negative Rate,TNR)：TNR=TN/(TN+FP)，即被预测为负的负样本数 /负样本实际数/2 sklearn相应的包 sklearn.metrics.confusion_matrix from sklearn.metrics import confusion_matrix # y_pred是预测标签 y_pred, y_true =[1,0,1,0], [0,0,1,0] confusion_matrix(y_true=y_true, y_pred=y_pred) # array([[2, 1], # [0, 1]], dtype=int64) 准确率(Accuracy) 分类正确的样本个数占总样本的比例. sklearn相应的包 sklearn.metrics.accuracy_score from sklearn.metrics import accuracy_score # y_pred是预测标签 y_pred, y_true=[1,2,3,4], [2,2,3,4] accuracy_score(y_true=y_true, y_pred=y_pred) # 0.75 精确率(Precision): 预测正确的正样本占所有预测为正样本的比例 所有分正确的正样本/所有预测为正类的样本数. $$Precision = \frac{TP}{TP + FP}$$ 也叫查准率 sklearn相应的包 sklearn.metrics.precision_score from sklearn.metrics import precision_score # y_pred是预测标签 y_pred, y_true =[1,0,1,0], [0,0,1,0] precision_score(y_true=y_true, y_pred=y_pred) # 0.5 召回率 (Recall): 预测正确的正样本占所有正样本比例 所有分正确的正样本/所有的正样本数. $$Recall = \frac{TP}{TP + FN}$$ 也叫查全率 sklearn sklearn.metrics.recall_score from sklearn.metrics import recall_score # y_pred是预测标签 y_pred, y_true =[1,0,1,0], [0,0,1,0] recall_score(y_true=y_true, y_pred=y_pred) # 1.0 F1 score 又称平衡分数, 定义为精确率和召回率的调和平均数 $$F_1 \ score = \frac{2 * Precision * Recall}{Precision + Recall}$$ sklearn相应的包 sklearn.metrics.f1_score from sklearn.metrics import f1_score # y_pred是预测标签 y_pred, y_true =[1,0,1,0], [0,0,1,0] f1_score(y_true=y_true, y_pred=y_pred) # classification_report可以直接输出各个类的precision recall f1-score support from sklearn.metrics import classification_report # y_pred是预测标签 y_pred, y_true =[1,0,1,0], [0,0,1,0] print(classification_report(y_true=y_true, y_pred=y_pred)) 增益（Gain）和提升（Lift）图 ROC曲线 横轴: 负正类率(false postive rate FPR=FP/(FP+TN))特异度, 划分实例中所有负例占所有负例的比例；(1-Specificity) 纵轴: 真正类率(true postive rate TPR=TP/(TP+FN))灵敏度, Sensitivity(正类覆盖率), 即召回率 sklearn相应的包 sklearn.metrics.roc_curve, sklearn.metrics.auc import matplotlib.pyplot as plt from sklearn.metrics import roc_curve, auc # y_test：实际的标签, dataset_pred：预测的概率值。 fpr, tpr, thresholds = roc_curve(y_test, dataset_pred) roc_auc = auc(fpr, tpr) #画图，只需要plt.plot(fpr,tpr),变量roc_auc只是记录auc的值，通过auc()函数能计算出来 plt.plot(fpr, tpr, lw=1, label=&#39;ROC(area = %0.2f)&#39; % (roc_auc)) plt.xlabel(&#34;FPR (False Positive Rate)&#34;) plt.ylabel(&#34;TPR (True Positive Rate)&#34;) plt.title(&#34;Receiver Operating Characteristic, ROC(AUC = %0.2f)&#34;% (roc_auc)) plt.show() AUC（Area Under Curve） AUC即为ROC曲线下的面积(ROC的积分), 通常大于0.5小于1. AUC值(面积)越大的分类器，性能越好. sklearn相应的包 sklearn.metrics.roc_auc_score from sklearn.metrics import roc_auc_score # y_test：实际的标签, dataset_pred：预测的概率值。 roc_auc_score(y_test, dataset_pred) PR曲线 横坐标: 精确率P 纵坐标: 召回率R 评价标准和ROC一样，先看平滑不平滑（蓝线明显好些）。一般来说，在同一测试集，位于上面的线比下面的好. 当P和R的值接近时，F1值最大. 曲线绘制 import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns from sklearn.metrics import precision_recall_fscore_support as prfs def evaluate_pr(res, inf=0.0, sup=1.0): y_vali = res.copy() pres = [] recs = [] thrs = [] for thred in np.arange(inf, sup, 0.01): y_vali[&#39;pred&#39;] = y_vali[&#39;proba&#39;] &gt; thred p, r, f, _ = prfs(y_vali[&#39;real&#39;], y_vali[&#39;pred&#39;], average=&#39;binary&#39;) pres.append(p) recs.append(r) thrs.append(thred) sns.lineplot(pres, recs) proba = model.predict_proba(X)[:,1] res = input_train[[]].copy() res[&#39;real&#39;] = y res[&#39;proba&#39;] = proba evaluate_pr(res) 多分类 precision_recall_fscore_support: 计算每个分类的precision, recall, fscore和support 回归问题 在sklearn中, 通常函数以_score结尾返回一个值来最大化, 越高越好; 函数 _error或_loss结尾返回一个值来 minimize（最小化）, 越低越好. 平均绝对误差（MAE） 平均绝对误差MAE（Mean Absolute Error）又被称为l1 平均绝对误差是非负值，模型越好MAE越接近零. 公式 $$MAE = \frac{1}{m}\sum_{i=0}^m|y_i - \hat{y}_i|$$ sklearn相应包 sklearn.metrics.mean_absolute_error from sklearn.metrics import mean_absolute_error y_true, y_pred = [3, -0.5, 2, 7], [2.5, 0.0, 2, 8] mean_absolute_error(y_true, y_pred) # 0.5 平均平方误差（MSE） 平均平方误差MSE（Mean Squared Error）又被称为l2 本质是在残差平方和(RSS)的基础上除以了样本总量，得到了每个样本量上的平均误差. 均方误差是非负值，模型越好MSE越接近零. 公式 $$MSE = \frac{1}{m}\sum_{i=0}^m(y_i - \hat{y}_i)^2$$ sklearn相应包 sklearn.metrics.mean_squared_error from sklearn.metrics import mean_squared_error y_true, y_pred = [3, -0.5, 2, 7], [2.5, 0.0, 2, 8] mse = mean_squared_error(y_true, y_pred) # 0.375 rmse = np.sqrt(mse) # 0.6123724356957945 均方根误差（RMSE） 均方根误差RMSE (Root Mean Squared Errort), 即MSE开方. 公式 $$RMSE = \sqrt{\frac{1}{m}\sum_{i=0}^m(y_i - \hat{y}_i)^2}$$ 均方对数误差(MSLE) 均方对数误差MSLE (mean squared logarithmic error) 均方对数误差是非负值，模型越好MSLE越接近零. 公式 $$MSLE = \frac{1}{m}\sum_{i=1}^m(log(y_i + 1) - log(\hat{y_i + 1})^2$$ sklearn对应包 sklearn.metrics.mean_squared_log_error 中值绝对误差(MedAE) 中值绝对误差MedAE（median absolute error） 中值绝对误差是非负值，模型越好MSE越接近零. 公式 $$MedAE = median(|y_i - \hat{y}_i|, \cdots, |y_m - \hat{y}_m|)$$ sklearn对应包 sklearn.metrics.mean_squared_log_error 可释方差得分 (EVS) 解释变异（ Explained variance）是根据误差的方差计算得到. 最佳模型的可释方差分数值为1，模型越差值越小. 公式: $$EVS = 1 - \frac{var(y_i - \hat{y_i})}{var(y_i)}$$ sklearn相关包 sklearn.metrics.explained_variance_score 决定系数(Coefficient of Determination) R2 决定系数(r2_score) 判断回归方程的拟合程度. 最佳模型的R^{2}决定系数分数值为1，常数模型值为0，模型越差值越小. 公式 $$ R^2 = 1 - \frac{\sum_{i=0}^m(y_i - \hat{y}i)^2}{\sum{i=0}^m(y_i - \bar{y}i)^2} = 1 - \frac{RSS}{\sum{i=0}^m(y_i - \bar{y}_i)^2} $$ sklearn相关包 sklearn.metrics.r2_score from sklearn.metrics import r2_score y_true, y_pred = [3, -0.5, 2, 7], [2.5, 0.0, 2, 8] r2_score(y_true, y_pred) 聚类问题 聚类结果, 追求&quot;簇内相似度&quot;(intra-cluster similarity)高, 且&quot;簇间相似度&quot;(inter-cluster similarity)低. 聚类性能度量大致有两类: 将聚类结果与某个&quot;参考模型&quot;(reference model)进行比较, 称为&quot;外部指标&quot;(external index). 直接参考聚类结果而不利用任何参考模型, 称为&quot;内部指标&quot;(internal index). 外部指标 对数据集$D = {x_1, x_2, \cdots, x_m}$, 假定通过聚类给出的簇划分为$C = {C_1, C_2, \cdots, C_k }$, 参考模型给出的簇划分为$$C^* = \{C_1^*, C_2^*, \cdots, C_s^*\}$$. 相应地, 令$\lambda$与$$\lambda^*$$分别表示与$C$和$$C^*$$对应的簇标记向量, 将样本两两配对考虑, 定义 $$ a = |SS|, SS = \{(x_i,x_j) | \lambda_i = \lambda_j, \lambda_i^* = \lambda_j*, i &lt; j \} \\ b = |SD|, SD = \{(x_i,x_j) | \lambda_i = \lambda_j, \lambda_i* \ne \lambda_j^*, i &lt; j \} \\ c = |DS|, DS = \{(x_i,x_j) | \lambda_i \ne \lambda_j, \lambda_i^* = \lambda_j^*, i &lt; j \} \\ d = |DD|, DD = \{(x_i,x_j) | \lambda_i \ne \lambda_j, \lambda_i^* \ne \lambda_j^*, i &lt; j \} $$ 其中 集合SS: 包含了在$C$中属于相同簇, 同时在$C^*$中也属于相同簇的样本对 集合SD: 包含了在$C$中属于相同簇, 同时在$C^*$中属于不同簇的样本对 集合DS: 包含了在$C$中属于不同簇, 同时在$C^*$中属于相同簇的样本对 集合DD: 包含了在$C$中属于不同簇, 同时在$C^*$中属于不同簇的样本对 由于每个样本对$(x_i, x_j) (i &lt; j)$仅能出现在一个集合中, 因此有$a + b + c + d = m(m - 1)/2$ 常用外部指标 Jacccard系数(Jaccard Coeffient, 简称JC) $$JC = \frac{a}{a + b + c}$$ FM指数(Fowlkes and Mallows Index, 简称FMI) $$FMI = \sqrt{\frac{a}{a + b} \cdot \frac{a}{a + c}}$$ Rand指数(Rand Index, 简称RI) $$RI = \frac{2(a + d)}{m(m-1)}$$ 上述性能指数的结果值均在$[0, 1]$区间, 值越大越好. 互信息（Mutual Information) 两个随机变量的互信息（Mutual Information，简称MI）或转移信息（transinformation）是变量间相互依赖性的量度. sklearn相应包 FMI: fowlkes_mallows_score RI: sklearn.metrics.adjusted_rand_score MI: sklearn.metrics.adjusted_mutual_info_score 内部指标 考虑聚类结果的簇划分$C = {C_1, C_2, \cdots, C_k}$, 有以下定义 $avg(C)$为簇$C$内样本间的平均距离 $diam(C)$对应簇内样本间的最远距离 $d_{min}(C_i, C_j)$对应于簇$C_i$与簇$C_j$最近样本间的距离 $d_{cen(C_i, C_j)}$对应簇$C_i$与簇$C_j$中心点的距离 常用内部指标 DB指数(Davies-Bouldin Index, 简称DBI) $$DBI = \frac{1}{k}\sum_{i=1}^k\max_{ j\ne i}(\frac{avg(C_i) + avg(C_j)}{d_{cen}(C_i, C_j)})$$ DBI的可能最小值为0, 值越小越好. Dunn指数(Dunn Index, 简称DI) $$DI = \min_{1 \le i \le k}{\min_{j \ne i}(\frac{d_{min}(C_i,C_j)}{\max_{1 \le l \le k} diam(C_l)})}$$ DI值越大越好 轮廓系数（Silhouette coefficient） 结合了聚类的凝聚度（Cohesion）和分离度（Separation）, 用于评估聚类的效果。该值处于(-1,1)之间. 其中值越接近1表示样本与自己所在的簇中的样本很相似，并且与其他簇中的样本不相似;当样本点与簇外的样本更相似的时候，轮廓系数就为负; 当轮廓系数为0时, 则代表两个簇中的样本相 似度一致，两个簇本应该是一个簇. 公式 $$s(i) = \frac{b(i) - a(i)}{max{a(i) , b(i)}}$$ a(i)为样本i与簇内其它样本的平均距离, b(i)为样本i与其它某簇样本的平均距离, 多个簇b(i)取最小. sklearn相应包 DBI: sklearn.metrics.davies_bouldin_score sklearn.metrics.silhouette_score, 返回是一个数据集中, 所有样本的轮廓系数均值. sklearn.metrics.silhouette_score_samples，它的参数与轮廓系数一致，但返回的是数据集中每个样本自己的轮廓系数. 关联问题 假设$I={I_{1},I_{2},\ldots ,I_{m}}$, 是项的集合。给定一个交易数据库$D={t_{1},t_{2},\ldots ,t_{n}}$，其中每个事务（Transaction）t是I的非空子集，即$t\subseteq I$，每一个交易都与一个唯一的标识符TID（Transaction ID）对应。关联规则是形如$X \Rightarrow Y$的蕴涵式，其中$X,Y\subseteq I$且$X\cap Y=\emptyset$ ， X和Y分别称为关联规则的先导(antecedent或left-hand-side, LHS)和后继(consequent或right-hand-side, RHS) 。关联规则$X\Rightarrow Y$在D中的支持度（support）是D中事务包含$X\cup Y$的百分比，即概率$P(X\cup Y)$；包含X的事务中同时包含Y的百分比，即条件概率$P\left(Y|X\right)$。 支持度(Support) 表示项目X, Y同时在总数据集中出现的概率, 其计算公式为 $$support(X =&gt; Y) = \frac{T(X \cup Y)}{N}$$ 指D中N个交易记录中同时出现X和Y的交易记录所占的比例. 置信度(Confidence) 指在先导项X已经发生的情况下, 后续项Y也发生的概率, 即包含X的交易记录中同时也包含Y的交易记录所占的比例, 计算公式为: $$confidence(X =&gt; Y) = \frac{support(X \cup Y)}{support(X)}$$ 提升度 表示含有X的条件下同时含有Y的概率, 与无论含不含X, 含有Y的概率之比, 计算公式 $$confidence(X =&gt; Y) /support(Y)$$ 购买X的情况下, 购买Y的概率大于购买Y的概率, 则具有提升作用. 参考 机器学习 周志华著 model-evaluation   ]]></content></entry><entry><title>数据仓库建模分层设计</title><url>/post/the-modeling-of-data-warehouse/</url><categories><category>数据科学</category></categories><tags><tag>大数据</tag></tags><content type="html"> 概述 数据仓库 （DataWarehouse）是一个面向主题的（Subject Oriented）、集成的（Integrate）、相对稳定的（Non-Volatile）、反映历史变化（Time Variant）的数据集合, 用于支持管理决策. 将所有的业务数据经汇总处理, 构成数据仓库(DW) 趋势 数据仓库是伴随着企业信息化发展起来的, 在企业信息化的过程中, 随着信息化工具的升级和新工具的应用, 数据量变的越来越大, 数据格式越来越多, 决策要求越来越苛刻, 数据仓库技术也在不停的发展. 实时数据仓库以满足实时化&amp;amp;自动化决策需求. 大数据&amp;amp;数据湖以支持大量&amp;amp;复杂数据类型（文本、图像、视频、音频）. 数据库VS仓库 存储: 数据库面向业务存储, 采用关系建模, 组织规范 仓库面向主题(较高层次上对分析对象数据的一个完整并且一致的描述)存储, 采用维度建模, 可能冗余, 相对变化大, 数据量大 场景: 数据库针对应用(OLTP) 仓库针对分析(OLAP) 关系建模VS维度建模 关系模型: 主要应用与 OLTP 系统中，为了保证数据的一致性以及避免 冗余，所以大部分业务系统的表都是遵循第三范式的. 维度模型: 关系模型虽然冗余少，但是在大规模数据，跨表分析统计查询过程中，会造成多表关联，这会大大降低执行效率。所以通常我们采用维度模型建模，把相关各种表整理成两种: 事实表和维度表两种. 维度建模模型 在维度建模的基础上又分为三种模型: 星型模型 雪花模型 星座模型 维度设计过程 选择业务过程: 对应企业数据仓库总线矩阵的一行. 声明粒度: 粒度用于确定某一事实表中的行为表示什么, 建议从原子级别的粒度开始设计. 确认维度: 包含应用所需要的用于过滤及分类事实的描述型属性. 确认事实: 所有事实只允许与声明的粒度保持一致. 维度表VS事实表 维度表 一般是对事实的描述信息. 每一张维表对应现实世界中的一个对象或者概念. 例如:用户、商品、日期、地区等. 可以简单地理解维度表包含了事实表中指定属性的相关详细信息. 比如商品维度表和用户维度表. 特点: 维度表的范围很宽 (具有多个属性, 列比较多) 与事实表相比, 行数相对较小 内容相对固定: 编码表 事实表 事实表中的每行数据代表一个业务事件 (下单、支付、退款、评价等). 事实这个术语表示的是业务事件的度量值 (可统计次数、个数、件数、金额等)，例如，订单事件中的下单金额. 每一个事实表的行包括: 具有可加性的数值型的度量值 与维表相连接的外键 通常具有两个和两个以上的外键 外键之间表示维表之间多对多的关系 特点: 非常大 内容相对窄, 列数少 经常发生变化, 每天会新增很多 数据仓库建设方法论 分层设计 数据分层是数据仓库设计中十分重要的一个环节, 优秀的分层设计能够让整个数据体系更易理解和使用. 一般根据具体的业务进行合理的分层设计 通常采用四层划分方式: ODS原始数据层, DWD明细数据层, DWM数据中间层 (包括DWS数据服务层和DWT数据主题层) 和ADS数据应用层 ODS (Operation Data Store) 原始数据层 保持数据原貌不做任何修改，起到备份数据的作用。 数据采用压缩 (如LZO压缩)，减少磁盘存储空间(例如:原始数据 100G，可以压缩到 10G 左右) 创建分区表，防止后续的全表扫描 DWD (Data Warehouse Detail) 明细数据层 对ODS层数据进行提取、清洗 (去除空值, 脏数据, 异常值)、降维、解密脱敏等. 粒度是一行信息代表一次行为. 需构建维度模型，一般采用星型模型，呈现的状态一般为星座模型. 该层的数据应该是一致的、准确的、干净的数据，即对源系统数据进行了清洗（去除了杂质）后的数据 DWM (Data Warehouse Middle) 数据中间层 数据中间层, 包括DWS和DWT层. DWS (Data Warehouse Service) 数据服务层 以DWD为基础, 按天进行轻度汇总. 粒度是一行信息代表一天的行为. DWT (Data Warehouse Topic) 数据主题层 以DWS为基础, 按主题进行汇总. 粒度是一行信息代表积累的行为. ADS (Application Data Store) 数据应用层 应用层, 为各种统计报表提供数据. 面向主题 从具体的业务出发, 是分析的宏观领域, 比如供应商主题、商品主题、客户主题和仓库主题. 反范式数据模型 以事实表和维度表组成的星型/雪花/星座数据模型. 为多维数据分析服务 数据报表 数据立方体，上卷、下钻、切片、旋转等分析功能. 命名规范 表命名规范 ODS层命名: ods_表名 DWD层命名: dwd_dim/fact_表名 DWS层命名: dws_表名 DWT层命名: dwt_表名 ADS层命名: ads_表名 临时表命名: xxx_tmp 字段命名规范 同名同义 参考 数据仓库工具箱-维度建模权威指南 (第3版) 数据仓库的架构与设计 有赞数据仓库实践之路 美团点评基于 Flink 的实时数仓建设实践</content></entry><entry><title>机器学习之决策树</title><url>/post/decision-tree-of-machinelearning/</url><categories><category>数据科学</category></categories><tags><tag>机器学习</tag></tags><content type="html"> 概述 是一种基本的分类与回归方法, 模型呈树形结构. 学习步骤: 特征选择 决策树的生成 决策树的修剪 决策树模型与学习 特征选择 信息增益 相关概念 熵(entropy) 定义: 表示随机变量不确定性的度量. 公式: 设X是一个取有限个值得离散随机变量, 其概率分布为: $$P(X = x_i) = p_i, \quad i=1,2,&amp;hellip;,n$$ 则随机变量X的熵定义为 $$H(x) = -\sum_{i=1}^np_i logp_i \tag{3.1.1.1}$$ 式(3.1.1.1)中的对数以2或以e为底, 熵单位分别称作比特(bit)或纳特(nat). 由定义可知, 熵只依赖于X的分布, 而与X的取值无关, 所以也可以将X的熵记作H(p), 即 $$H(p) = -\sum_{i=1}^np_i logp_i \tag{3.1.1.2}$$ 性质: 熵越大, 随机变量的不确定性就越大, 从定义可证 $$0 \le H(p) \le logn$$ 经验熵(empirical entropy): 熵中的概率由数据估计(特别是极大似然估计)得到的计算值. 条件熵(conditional entropy) 定义: 设有随机变量(X , Y), 其联合概率分布为 $$P(X = x_i, Y = y_j) = p_{ij}, \quad i=1,2,\cdots,n; j=1,2,\cdots, m$$ 条件熵H(Y|X) 表示在已知随机变量X的条件下随机变量Y的不确定性. 定义为X给定条件下Y的条件概率分布的熵对X的数学期望 $$H(Y|X) = \sum_{i=1}^np_iH(Y|X=x_i), \quad p_i = P(X = x_i), i=1,2,\cdots,n$$ 经验条件熵(empirical entropy): 条件熵中的概率由数据估计(特别是极大似然估计)得到的计算值. 信息增益(information gain): 表示得知特征X的信息而使得类Y的信息的不确定性减少的程度. 定义: 特征A对训练数据集D的信息增益g(D,A), 定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差, 即 $$g(D,A) = H(D) - H(D|A)$$ 一般地, 熵H(Y)与条件熵H(Y|X)之差称为互信息(mutual information) 信息增益比(information gain ratio): 定义: 特征A对训练数据集D的信息增益比$g_R(D, A)$, 定义为其信息增益g(D, A)与训练数据集D的经验熵H(D)之比: $$g_R(D, A) = \frac{g(D, A)}{H(D)}$$ 决策树的生成 ID3算法 概述: 核心是在决策树各个节点上应用信息增益准则选择特征, 递归地构建决策树. 相当于用极大似然法进行概率模型的选择. 相关概念: 不纯度: 决策树的每个叶子节点中都会包含一组数据，在这组数据中，如果有某一类标签占有较大的比例，我们就说叶子节点“纯”，分枝分得好。某一类标签占的比例越大，叶子就越纯，不纯度就越低，分枝就越好。 如果没有哪一类标签的比例很大，各类标签都相对平均，则说叶子节点”不纯“，分枝不好，不纯度高。 分类型决策树在叶子节点上的决策规则是少数服从多数. 不纯度基于节点来计算，树中的每个节点都会有一个不纯度，并且子节点的不纯度一定是低于父节点的，也就是说，在同一棵决策树上，叶子节点的不纯度一定是最低的。 优缺点: 缺点: 在选择根节点和各内部节点中的分支属性时, 采用信息增益作为评价标准. 信息增益的缺点是倾向于选择取值较多的属性, 在有些情况下这类属性可能不会提供太多有价值的信息. 不能处理具有连续值的属性, 也不能处理具有缺数据的属性 没有对决策树进行修剪的过程, 噪声比较大. C4.5的生成算法 概述: 与ID3算法相似, C4.5算法对ID3算法进行了改进, 在生成过程中, 用信息增益比来选择特征. C5.0算法 概述: C5.0算法是C4.5算法的修订版, 适用于处理大数据集, 引入了分支度(Information Value) $$IV = -\sum_{i=1}^n(c_i/t)log_2(c_i/t)$$ 信息增益率为: $$g_R(D,A) = \frac{g(D,A)}{IV}$$ 优缺点: 优点: 可以处理数值型数据 CART 算法 概述: 分类与回归树(classification and regression tree, CART), 同样由特征选择, 树的生成及剪枝组成, 既可用于分类也可以用于回归. 分类树与回归树的区别在于叶节点. 如果目标字段是类别型的, 则建立的模型为分类树; 如果目标字段是数值型的, 则建立的模型就是回归树. 决策树的生成就是递归地构建二叉决策树的过程. 回归树用平方误差最小化准则 分类树用基尼指数(Gini index)最小化准则 $$Gini(p) = \sum_{k=1}^Kp_k(1 - p_k) = 1- \sum_{k=1}^Kp_k^2$$ 回归树的生成 分类树的生成 决策树的剪枝(pruning) 如果一个分支, 叶节点的错误率之和大于将该分支砍去所产生的错误率, 则对此分支进行修剪. 决策树算法异同 C5.0通过计算预测错误率来剪枝, 而CART算法通过验证数据来剪枝. 相关库 Python from sklearn.tree import DecisionTreeClassifier</content></entry><entry><title>多元统计之因子分析</title><url>/post/factor-analysis/</url><categories><category>数据科学</category></categories><tags><tag>数理统计</tag></tags><content type="html"><![CDATA[  概述 研究观测变量变动的共同原因和特殊原因, 从而达到简化变量结构目的的多元统计方法 应用 寻求变量的基本结构, 简化变量系统 用于分类, 根据因子得分值, 在因子轴所构成的空间中将变量或者样本进行分类(能够分析样品间差异的原因) 类型 R型因子分析 Q型因子分析 因子分析模型 因子分析的数学模型 R型因子分析模型 概述:
R型因子分析是将每一个变量都表示成公共因子的线性函数与特殊因子之和, 即 $$X_i = a_{i1}F_1 + a_{i2}F_2 + \cdots + a_{im}F_m + \epsilon_i, \quad (i=1,2,\cdots, p) \tag{2.1.1-1}$$ 上式中的 $F_1,F_2,\cdots,F_m$ 称为公共因子, $\epsilon_i$ 称为 $X_i$ 的特殊因子。该模型可用矩阵表示为：
$$X = AF + \epsilon$$ $$\begin{bmatrix} X_1 \ X_2 \ \vdots \ X_p \end{bmatrix} = \begin{bmatrix} \epsilon_1 \ \epsilon_2 \ \vdots \ \epsilon_p \end{bmatrix} + A_{p \times m}\begin{bmatrix} F_1 \ F_2 \ \vdots \ F_m \end{bmatrix} \tag{2.1.1-2} $$
构造模型满足:
$m \le p$ $Cov(F, \epsilon) = 0$, 即公共因子与特殊因子是不相关的 $D_F = D(F) = I_m$, 即各个公共因子不相关且方差为1 各个特殊因子不相关, 方差不要求相等 $$D_{\epsilon} = D(\epsilon) = \begin{bmatrix} \sigma_1^2 &amp; &amp; &amp; 0 \ &amp; \sigma_2^2 &amp; &amp; \ &amp; &amp; \ddots &amp; \ 0 &amp; &amp; &amp; \sigma_p^2 \end{bmatrix}$$
公共因子(潜在因子)是不可观测变量且只存在于某种理论意义之中, 可理解为在高维空间中的互相垂直的m个坐标轴。虽然潜在变量不能直接测得, 但它一定与某些可测变量有着某种程度的关联。
Q型因子分析(因子得分) 概述: 类似地, Q型因子分析数学模型可表示为: $$X_i = a_{i1}F_1 + a_{i2}F_2 + \cdots + a_{im}F_m + \epsilon_i, \quad (i=1,2,\cdots, n) \tag{2.1.1-3}$$ Q型因子分析与R型因子分析模型的差体现在$X_1, X_2, \cdots, X_n$表示的是n个样品。 主成分分析与因子分析的异同 相同点: R型或Q型因子分析都用公因子F代替X, 一般要求$m&lt;p, m&lt;n$, 因此因子分析与主成分分析一样, 也是一种降低变量维度数的方法。 因子分析求解过程同主成分分析类似, 也是从一个协方差阵出发的。 区别: 主成分分析的数学模型本质上是一种线性变化, 将原始坐标变换到变异程度大的方向, 突出数据变异的方向, 归纳重要信息。 因子分析从本质上看是从显在变量去&quot;提炼&quot;潜在因子的过程。并且因子的形式也不是唯一确定的。一般来说, 作为&quot;自变量&quot;的因子$F_1,F_2,\cdots,F_m$是不可直接观测的。 因子载荷阵 因子载荷阵不唯一的原因 变量X的协差阵$\Sigma$的分解式为 $$ \begin{aligned} D(X) &amp; = D(AF + \epsilon) = E[(AF + \epsilon)(AF + \epsilon)&rsquo;] \ &amp; = AE(FF&rsquo;)A&rsquo; + AE(F\epsilon&rsquo;) + E(\epsilon F&rsquo;)A&rsquo; + E(\epsilon\epsilon&rsquo;) \ &amp; = AD(F)A&rsquo; + D(\epsilon) \ &amp; = AA&rsquo; + D(\epsilon) \end{aligned} $$
如果X为标准化的随机向量, 则$\Sigma$就是相关矩阵$R = (\rho_{ij}), 即R = AA&rsquo; + D_{\epsilon}$
对于$m \times m$的正交矩阵T, 令$A^* = AT, F^* = T&rsquo;F$, 模型可表示为: $$X = A^*F^* + \epsilon$$ 由于
$$Cov(F^, \epsilon) = E(F^\epsilon&rsquo;) = T&rsquo;E(F\epsilon&rsquo;) = 0 \ D(F^*) = T&rsquo;D(F)T = T&rsquo;T = I_{m \times m}$$
所以仍满足模型的条件 同样$\Sigma$也可以分解为$$\Sigma = A^*A^{*'} + D_{\epsilon}$$
因子载荷阵的统计意义 因子载荷$a_{ij}$的统计意义
对于因子模型 $$X_i = a_{i1}F_1 + a_{i2}F_2 + \cdots + a_{ij}F_j + \cdots + a_{im}F_m + \epsilon_i \quad (i=1,2,\cdots,p)$$
可得$X_i与F_j$的协方差为: $$\begin{aligned} Cov(X_i,F_j) &amp; = Cov(\sum_{k=1}^ma_{ik}F_k + \epsilon, F_j) \ &amp; = Cov(\sum_{k=1}^ma_{ik}F_k, F_j) + Cov(\epsilon_i, F_j) \ &amp; = a_{ij} \end{aligned}$$
若对$X_i$做了标准化处理, $X_i$的标准差为1, 且$F_j$的标准差为1, 有 $$ r_{X_i,F_j} = \frac{Cov(X_i,F_j)}{\sqrt{D(X_i)}\sqrt{D(F_j)}} = Cov(X_i,F_j) = a_{ij} $$ 那么, 对于标准化后的$X_i$, $a_{ij}$是$X_i$与$F_j$的相关系数, 表示$X_i$依赖$F_j$的分量(比重)。因此统计学上也称其为权。 历史原因, 心理学家将它叫做载荷, 表示第i个变量在第j个公共因子上的载荷, 反应了第i个变量在第j个公共因子上的相对重要性。 变量共同度的统计意义
由因子模型, 知 $$\begin{aligned} D(X_i) &amp; = a_{i1}^2D(F_1) + a_{i2}^2D(F_2) + \cdots + a_{im}^2D(F_m) + D(\epsilon_i) \ &amp; = a_{i1}^2 + a_{i2}^2 + \cdots + a_{im}^2 + D(\epsilon_i) \ &amp; = h_i^2 + \sigma_i^2 \end{aligned}$$
变量$X_i$的共同度为: $h_i^2 = \sum_{j=1}^ma_{ij}^2 \quad i=1,2,\cdots,p$ 如果对$X_i$作了标准化处理, 有 $$1 = h_i^2 + \sigma_i^2$$ 公因子$F_i$的方差贡献率$g_j^2$的统计意义
设因子载荷矩阵为$A$, 称第j列元素的平方和, 即: $$g_j^2 = \sum_{i=1}^pa_{ij}^2 \quad j=1,2,\cdots,m$$ 为公共因子$F_j$对X的贡献, 即$g_j^2$表示同一个公共因子$F_j$对各变量所提供的方差贡献之总和, 反映该因子对全部变量的总影响 它是衡量每一个公共因子相对重要性的一个尺度。公共因子$F_j$的相对重要性可用$g_j^2/h$来衡量, 它称为公共因子$F_j$对X的贡献率。 因子分析的目的就是要由原始随机向量的协差阵$\Sigma$或相关阵$R$求出因子分析模型的解, 即求出载荷阵$A$和特性方差阵$D_\epsilon$, 并给公因子赋予有实际背景的解释。 因子载荷矩阵求解 主成分法 概述: 在进行因子分析之前先对数据进行一次主成分分析, 然后把前几个主成分作为未旋转的公共因子。 该方法所得的特殊因子$\epsilon_1,\epsilon_2,\cdots,\epsilon_p$之间并不相互独立, 不完全符合因子模型的假设前提。 当共同度较大时, 特殊因子所起的作用较小, 特殊因子之间的相关性所带来的影响几乎可以忽略。 求解方法 假定从相关阵出发求解主成分, 设有p个变量, 则可以找出p个主成分$Y_1,Y_2,\cdots,Y_p$(按由大到小顺序排列), 则主成分与原始变量之间存在如下关系式:
$$ \begin{cases} Y_1 = \gamma_{11}X_1 + \gamma_{12}X_2 + \cdots + \gamma_{1p}X_p \ Y_2 = \gamma_{21}X_1 + \gamma_{22}X_2 + \cdots + \gamma_{2p}X_p \ \cdots\cdots \ Y_p = \gamma_{p1}X_1 + \gamma_{p2}X_2 + \cdots + \gamma_{pp}X_p \ \end{cases} \tag{3.1.1-1} $$
式中, $\gamma_ij$为随机变量X的相关矩阵的特征根所对应的特征向量的分量, 因为特征向量之间彼此正交, 从X到Y的转换关系式可逆的, 则可得出由Y到X的转换关系为:
$$ \begin{cases} X_1 = \gamma_{11}Y_1 + \gamma_{21}Y_2 + \cdots + \gamma_{p1}Y_p \ X_2 = \gamma_{12}Y_1 + \gamma_{22}Y_2 + \cdots + \gamma_{p2}Y_p \ \cdots\cdots \ X_p = \gamma_{1p}Y_1 + \gamma_{2p}Y_2 + \cdots + \gamma_{pp}Y_p \ \end{cases} \tag{3.1.1-2} $$
对上面每一等式只保留前m个主成分而把后面的部分用$\epsilon_i$代替, 则式(3.1.1-2)转化为:
$$ \begin{cases} X_1 = \gamma_{11}Y_1 + \gamma_{21}Y_2 + \cdots + \gamma_{m1}Y_m + \epsilon_1 \ X_2 = \gamma_{12}Y_1 + \gamma_{22}Y_2 + \cdots + \gamma_{m2}Y_m + \epsilon_2 \ \cdots\cdots \ X_p = \gamma_{1p}Y_1 + \gamma_{2p}Y_2 + \cdots + \gamma_{mp}Y_m + \epsilon_p \ \end{cases} \tag{3.1.1-3} $$
该式与因子模型(2.1.1-2)相一致, 并且$Y_i=(i=1,2,\cdots,m)$之间相互独立。将$Y_i$转化成合适的公共因子, 只需将主成分$Y_i$变成方差为1的变量, 即除以其标准差(特征根的平方根), 则可令$F_i = Y_i/\sqrt{\lambda_i}, \quad a_{ij} = \gamma_{ji}$, 式(3.1.1-3)变为:
$$ \begin{cases} X_1 = a_{11}F_1 + a_{12}F_2 + \cdots + a_{1m}F_m + \epsilon_1 \ X_2 = a_{21}F_1 + a_{22}F_2 + \cdots + a_{2m}F_m + \epsilon_2 \ \cdots\cdots \ X_p = a_{p1}F_1 + a_{p2}F_2 + \cdots + a_{pm}F_m + \epsilon_p \end{cases} \tag{3.1.1-4} $$
这与因子模型完全一致, 这样就得到了载荷矩阵$A$和一组初始公共因子(未旋转)。
一般设$\lambda_1,\lambda_2,\cdots,\lambda_p \quad (\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_p)$为样本相关矩阵R的特征根, $\gamma_1,\gamma_2,\cdots,\gamma_p$为对应的标准化正交化特征向量。设$m &lt; p$, 则因子载荷矩阵A的一个解为:
$$\widehat{A} = (\sqrt{\lambda_1}\gamma_1, \sqrt{\lambda_2}\gamma_2,\cdots,\sqrt{\lambda_m}\gamma_m) \tag{3.1.1-5}$$ 共同度的估计为: $$\widehat{h}i^2 = \widehat{a}{i1}^2 + \widehat{a}{i2}^2 + \cdots + \widehat{a}{im}^2 \tag{3.1.1-6}$$
公因子的数目m, 当用主成分进行因子分析时, 可以借鉴确定主成分个数的准则。一般具体问题具体分析, 总之要使所选取的公共因子能够合理地描述原始变量相关阵的结构, 同时要有利于因子模型的解释。
主轴因子法 概述: 求解思路与主成分发类似, 均从分析矩阵的结构入手, 不同的地方在于, 主成分发是在所有的p个主成分能解释标准化原始变量所有方差的基础上进行分析的, 而主轴因子法中, 假定m个公共因子只能解释原始变量的部分方差, 利用公共因子方差(或共同度)来代替相关矩阵主对角线上的元素1, 并以新得到的这个矩阵(称为调整相关矩阵)为出发点, 对其分别求解特征根与特征向量并得到因子解。 求解方法 在因子模型(2.1.1-1)中, 不难得到如下关于X的相关矩阵R的关系式: $$R = AA&rsquo; + \Sigma_\epsilon$$ 式中A为因子载荷矩阵, $\Sigma_\epsilon$为一对角阵, 其对角元素为相应特殊因子的方差。则称$$R^* = R -\Sigma_\epsilon = AA'$$为调整相关矩阵, 显然$$R^*$$的主对角元素不再是1, 而是共同度$h_i^2$。分别求解$R^*$的特征值与标准正交特征向量, 进而求出因子载荷矩阵A。此时, $$R^*$$有m个正的特征值。设$$\lambda_1^* \ge \lambda_2^* \ge \cdots \ge \lambda_m^*$$为$$R^*$$的特征根, $$\gamma_1^*, \gamma_2^*, \cdots, \gamma_m^*$$为对应的标准正交话特征向量。m &lt; p, 则因子载荷矩阵A的一个主轴因子解为:
$$\widehat{A} = (\sqrt{\lambda_1^}\gamma_1^, \sqrt{\lambda_2^}\gamma_2^,\cdots,\sqrt{\lambda_m^}\gamma_m^) \tag{3.1.1.7}$$
注意到, 上面的分析是以首先得到调整相关矩阵$R^$为基础的, 而实际上, $R^$与共同度(或相对的剩余方差)都是未知的, 需要先进行估计。一般先给出一个初始估计, 然后估计出载荷矩阵A后再给出较好的共同度或剩余方差的估计。初始估计的方法有很多, 可尝试对原始变量先进行一次主成分分析, 给出初始估计值。
极大似然法 如果假定公共因子F和特殊因子$\epsilon$服从正态分布, 则能够得到因子载荷和特殊因子方差的极大似然估计。设$X_1,X_2,\cdots,X_p$为来自正态总体$N(\mu,\Sigma)$的随机样本, 其中$\Sigma = AA&rsquo; + \Sigma_\epsilon$。从似然函数的理论知
$$ L(\mu, \Sigma) = \frac{1}{(2\pi)^{np/2}|\Sigma|^{n/2}}e^{-1/2tr{\Sigma^{-1}[\sum_{j=1}^n(X_j - \bar{X})(X_j - \bar{X})&rsquo; + n(\bar{X} - \mu)(\bar{X} - \mu)&rsquo;]}} \tag{3.3.1-1} $$
它通过$\Sigma$依赖于A和$\Sigma_\epsilon$, 但式(3.3.1)不能唯一确定A, 为此, 添加如下条件:
$$ A&rsquo;\Sigma_\epsilon^{-1}A = \Lambda \tag{3.3.1-2} $$
其中$\Lambda$是一个对角阵, 用数值极大化的方法可以得到极大似然估计$\widehat{A}$和$\widehat{\Sigma_\epsilon}$。极大似然估计$\widehat{A}, \widehat{\Sigma_\epsilon}和\widehat{\mu} = \bar{X}$, 将使$\widehat{A}&rsquo;\widehat{\Sigma}_\epsilon^{-1}\widehat{A}$为对角阵, 使公式(3.3.1-2)达到最大。
公因子重要性的分析 因子旋转 概述 不管用何种方法确定初始因子载荷矩阵A, 它们都不是唯一的。 因所得的初始因子解，各主因子的典型代表变量不是很突出，容易使因子的意义含糊不清，不便于对实际问题进行分析，出于这种考虑，可以对初始公共因进行线性组合，即进行因子旋转。 分类: 正交旋转: 由初始载荷矩阵A右乘一正交阵得到, 经过正交旋转得到的新公共因子仍然保持彼此独立的性质 斜交旋转: 放弃了因子之间彼此独立的限制, 形式更简洁, 其实际意义更容易理解。 说明： 对于一个具体问题做因子旋转，有时需要多次才能得到满意效果。每一次旋转后，矩阵各列平方的相对方差之和总会比上一次有所增加。如此继续下去，当总方差的改变不大时，就可以停止旋转，这样就得到新的一组公共因子及相应的因子载荷矩阵，使得其各列元素的相对方差之和最大。 因子得分概述 因子得分就是公共因子$F_1,F_2,\cdots,F_m$在每一个样品点上的得分。 计算公式 因子模型中，公共因子的个数少于原始变量的个数，且是不可观测的隐变量，载荷矩阵A不可逆, 因而不能直接求得公共因子用原始变量表示的精确线性组合。可采用回归的思想求出线性组合系数的估计值，即建立如下以公共因子为因变量，原始变量为自变量的回归方程： $$F_j = \beta_{j1}X_1 + \beta_{j2}X_2 + \cdots + \beta_{jp}X_p, \quad j=1,2,\cdots,m \tag{4.2.1-1}$$ 此处因为原始变量与公共因子变量均为标准化变量, 所以回归模型中不存在常数项。在最小二乘意义下, 可以得到F的估计值: $$\widehat{F} = A&rsquo;R^{-1}X \tag{4.2.1-2}$$ 式中, A为因子载荷矩阵, R为原始变量的相关阵, X为原始变量向量。 因子分析步骤与逻辑框图 步骤 根据研究问题选取原始变量 对原始变量进行标准化并求其相关阵, 分析变量之间的相关性 求解初始公共因子及因子载荷矩阵 因子旋转 计算因子得分 根据因子得分值进行进一步分析 逻辑框图   ]]></content></entry><entry><title>多元统计之主成分分析</title><url>/post/principle-component-analysis/</url><categories><category>数据科学</category></categories><tags><tag>数理统计</tag></tags><content type="html"><![CDATA[  概述 主成分分析就是设法将原来指标重新组合成一组新的互相无关的几个综合指标来代替原来指标, 同时根据实际需要从中取几个较少的综合指标尽可能多地反映原来指标的信息。 这种将多个指标转化为少数互相无关的综合指标的统计方法叫做主成分分析或称主分量分析。 几何意义 代数观点: p个原始变量的一些特殊的线性组合 几何意义: 这些线性组合通过把由$X_1,X_2,\cdots,X_p$构成的坐标系旋转而产生的新坐标系。这样的新坐标轴使其通过样本变差最大的方向(或者说具有最大的样本方差)。 数学推导 设$X = (X_1,\cdots,X_p)&rsquo;$为一个p维随机向量, 并假定存在二阶矩, 其均值向量与协差阵分别记为: $$\mu = E(X), \quad \Sigma = D(X)$$ 考虑如下线性变换 $$ \begin{cases} Y_1 = t_{11}X_1 + t_{12}X_2 + \cdots + t_{1p}X_p = T&#39;_1X \\ Y_2 = t_{21}X_1 + t_{22}X_2 + \cdots + t_{2p}X_p = T&#39;_2X \\ \cdots \cdots \\ Y_p = t_{p1}X_1 + t_{p2}X_2 + \cdots + t_{pp}X_p = T&#39;_pX \end{cases} $$ 用矩阵表示为$Y = (Y_1,Y_2,\cdots,Y_p)&rsquo;$, 其中$Y = T&rsquo;X, T=(T_1,T_2,\cdots,T_p)$ 希望找到一组新的变量$Y_1,\cdots,Y_m(m \le p)$, 这组新的变量要求充分地反映原变量$X_1,\cdots,X_p$的信息, 而且相互独立。 对于$Y_1,\cdots,Y_m$有 $$ D(Y_i) = D(T&#39;_iX) = T&#39;_iD(X)T&#39;&#39;_i = T&#39;_i\Sigma T_i \quad i=1,2,\cdots,m \\ Cov(Y_i,Y_k) = Cov(T&#39;_iX,T&#39;_kX) = T&#39;_iCov(X,X)T&#39;&#39;_k = T&#39;_i\Sigma T_k \quad i,k=1,2,\cdots,m $$ 则问题转化为在新的变量$Y_1,\cdots,Y_m$相互独立的条件下, 求得$T_i$使得$D(Y_i)$达到最大。 由于$D(Y_i) = T&rsquo;_i\Sigma T_i$是向量T的增函数, 添加约束条件$T&rsquo;_iT_i=1$(即将线性组合$Y=T&rsquo;X$的系数标准化即单位化), 问题变为在约束条件下求最大的问题。 第一主成分满足$T&rsquo;_1T_1=1$, 使得$D(Y_1) = T&rsquo;_1\Sigma T_1$达到最大的$Y_1 = T&rsquo;_1X$。 第二主成分满足$T&rsquo;_2T_2=1, Cov(Y_2,Y_1) = Cov(T&rsquo;_2X, T&rsquo;_1X) = 0$使得$D(Y_2) = T&rsquo;_2\Sigma T_2$达到最大的$Y_2=T&rsquo;_2X$。 第k主成分满足$T&rsquo;_kT_k=1, Cov(Y_k,Y_i) = Cov(T&rsquo;_kX, T&rsquo;_iX) = 0 \quad (i &lt; k)$使得$D(Y_k) = T&rsquo;_k\Sigma T_k$达到最大的$Y_k = T&rsquo;_kX$。 求第一主成分, 构造目标函数为: $$\quad \varphi_1(T_1, \lambda) = T&rsquo;_1\Sigma T_1 - \lambda(T&rsquo;_1T_1 - 1) \tag{1.2-1}$$ 求导有: $$\quad \frac{\partial \varphi_1}{\partial T_1} = 2\Sigma T_1 - 2\lambda T_1 = 0 \tag{1.2-2}$$ 即: $$\quad (\Sigma - \lambda I)T_1 = 0 \tag{1.2-3}$$ 两边左乘$T&rsquo;_1$得到: $$\quad T&rsquo;_1\Sigma T_1 = \lambda \tag{1.2-4}$$ 由于X的协方差阵$\Sigma$为非负, 其特征方程(1.2-3)均大于零, 不妨设$\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_p \ge 0$。由(1.2-4)知$Y_1$的方差为$\lambda$。那么, $Y_1$的最大方差值为$\lambda_1$, 其相应的单位化特征向量为$T_1$。 求第二主成分, 由(1.2-2)有$\Sigma T_1 = \lambda T_1,$则$Cov(Y_2,Y_1) = T&rsquo;_2\Sigma T_1 = \lambda T&rsquo;_2T_1$。如果$Y_2$与$Y_1$相互独立, 既有$T&rsquo;_2T_1 = 0$或$T&rsquo;_1T_2 = 0$。构造第二主成分的目标函数, 即 $$\varphi_2(T_2, \lambda, \rho) = T&rsquo;_2\Sigma T_2 - \lambda(T&rsquo;_2T_2 - 1) - 2\rho(T&rsquo;_1T_2)$$ 对目标函数求导有: $$\frac{\partial\varphi_2}{\partial T_2} = 2\Sigma T_2 - 2\lambda T_2 - 2\rho T_1 = 0$$ 用$T&rsquo;_1$左乘有$T&rsquo;_1\Sigma T_2 - \lambda T&rsquo;_1T_2 - \rho T&rsquo;_1T_1 = 0$ 由于$T&rsquo;_1T_2 = 0, T&rsquo;_1\Sigma T_2 = 0$, 那么$\rho T&rsquo;_1T_1 = 0$, 即有$\rho = 0$ 从而$(\Sigma - \lambda I)T_2 = 0$, 且$T&rsquo;_2\Sigma T_2 = \lambda \quad (1.2-5)$ 这样说明, 如果X的协差阵$\Sigma$的特征根为$\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_p \ge 0$, 由(1.2-5)知$Y_2$的最大方差值为第二大特征根$\lambda_2$, 其相应的单位化的特征向量为$T_2$。 针对一般情形, 第k主成分应该是在$T&rsquo;_kT_k = 1$且$T&rsquo;_iT_k = 0$或$T&rsquo;_kT_i = 0(i &lt; k)$的条件下, 使得$D(Y_k) = T&rsquo;_k\Sigma T_k$达到最大的$Y_k = T&rsquo;_kX$。构造目标函数为: $$\varphi_k(T_k, \lambda, \rho_i) = T&rsquo;_k\Sigma T_k - \lambda(T&rsquo;kT_k - 1) - \sum{i=1}^{k-1}\rho_i(T&rsquo;iT_k) \tag{1.2-6}$$ 对目标函数求导有: $$\frac{\partial\varphi_k}{\partial T_k} = 2\Sigma T_k - 2\lambda T_k - 2\sum{i=1}^{k-1}\rho_iT_i = 0 \tag{1.2-7}$$ 用$T&rsquo;_k$左乘(1.2-7)有$T&rsquo;_i\Sigma T_k - \lambda T&rsquo;_iT_k - T&rsquo;i(\sum{i=1}^{k-1}\rho_iT_i) = 0$即$\rho_iT&rsquo;_iT_i = 0$, 那么$\rho_i = 0 \quad (i=1,2,\cdots,k-1)$。从而$(\Sigma - \lambda I)T_k = 0 \quad (1.2-8)$而且$T&rsquo;_k\Sigma T_k = \lambda \quad (1.2-8)$ 对于X的协差阵$\Sigma$的特征根为$\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_p \ge 0$, 由(1.2-7)和(1.2-8)知$Y_k$的最大方差值为第k大特征根$\lambda_k$, 其相应的单位化的特征向量为$T_k$。 综上所述, 设$X = (X_1, \cdots, X_p)$的协差矩阵为$\Sigma$, 其特征根为$\lambda_1 \ge \cdots \ge \lambda_k$, 相应的单位化的特征向量为$T_1,T_2,\cdots,T_p$。那么, 由此所确定的主成分为$Y_1 = T&rsquo;_1X, \cdots, Y_m = T&rsquo;_mX$, 其方差分别为$\Sigma$的特征根。 性质 设$Y = (Y_1,Y_2,\cdots,Y_p)&rsquo;$是X的主成分, 由$\Sigma$的所有特征根构成的对角阵为 $$ \Lambda = \begin{bmatrix} \lambda_1 &amp; &amp; 0 \\ &amp; \ddots \\ 0 &amp; &amp; \lambda_p \end{bmatrix} $$ 主成分可表示为 $Y = T&rsquo;X$ 性质一 主成分的协方差矩阵是对角阵 性质二 主成分的总方差等于原始变量的总方差 性质三 主成分$Y_k$与原始变量$X_i$的相关系数为: $$ \rho(Y_k,X_i) = \frac{\sqrt{\lambda_k}}{\sqrt{\sigma_{ii}}}t_{ki} \tag{1.3.3-1} $$ 并称之为因子负荷量(或因子载荷量) 性质四 $$ \sum_{i=1}^p\rho^2(Y_k,X_i)\cdot\sigma_{ii} = \lambda_k \quad (k = 1,2,\cdots,p) \tag{1.3.4-1} $$
主成分的偏差贡献率与信息提取率 在选取主成分时, 不仅要考虑累计贡献率, 还应考虑信息提取率。 主成分的偏差贡献率 称 $$\varphi_k = \lambda_k / \sum_{k=1}^p\lambda_k$$ 为第k个主成分$Y_k$的贡献率。第一主成分的贡献率最大, 其他依次减弱。 若只取$m(&lt;p)$个主成分, 则称 $$\psi_m = \sum_{k=1}^m\lambda_k/\sum_{k=1}^p\lambda_k$$ 为主成分$Y_1,\cdots,Y_m$的累计贡献率, 累计贡献率表明$Y_1,\cdots,Y_m$综合$X_1,X_2,\cdots,X_p$的能力。通常取m, 使得累计贡献率达到一个较高的百分数(如85%以上)。 信息提取率 任一原始变量$X_i$可用主成分表示为: $$X_i = t_{1i}Y_1 + t_{2i}Y_2 + \cdots + t_{pi}Y_p$$ 其中各个主成分之间互不相关, 所以原始变量的$X_i$的方差为: $$Var(X_i) = t_{1i}^2\lambda_1 + t_{2i}^2\lambda_2 + \cdots + t_{pi}^2\lambda_p = \sum_{j=1}^pt_{ji}^2\lambda_j = \sigma_{ii}$$ 定义比率: $$\Omega_i = \sum_{j=1}^mt_{ji}^2\lambda_j/\sigma_{ii}$$ 为原始变量$X_i$的信息提取率。 实际应用 出发点 为消除量纲的不同可能带来的一些不合理的影响, 常常需要事先对变量标准化。 标准化处理, 即令: $$X_i^* = \frac{X_i - E(X_i)}{\sqrt{D(X_i)}} \quad i=1,\cdots,p$$ $$X^* = (X_1^*, \cdots, X_p^*)'$$的协方差矩阵就是X的相关系数矩阵R. 标准化的实质 标准化后各原始指标的方差均为1, 抹杀了数据本身离散度的信息。对于同度量或相似量级的变量还是使用原始的协方差求解主成分为宜。 注意 若各指标的数量级相差悬殊或者有不同量纲, 较合理做法: 使用原始变量的R代替$\Sigma$做主成分分析; 使用标准化变量的相关矩阵或者协差阵做主成分分析, 二者结果相同。 从原始变量的相关阵求得的主成分与用原始变量的协差阵求得的主成分一般不相同, 有时差异很大。 步骤 将原始数据标准化 建立变量的相关系数阵 求$$R^*$$的特征根为$$\lambda_1^* \ge \cdots \lambda_p^* \ge 0$$, 相应的特征向量为$$T_1^*, T_2^*, \cdots, T_p^*$$ 由累积方差贡献率确定主成分的个数(m), 并写出主成分为: $$Y_i = (T_i^*)'X \quad (i=1,2,\cdots,m)$$ 综合评价 一般在进行综合评价时, 需要给各指标赋予权重, 再进行综合评价。 如果是用主成分分析进行综合评价, 则在评价前需要给主成分赋予权重, 此时, 可以以方差贡献率作为参考来确定权重。 设$Y_1,Y_2,\cdots,Y_p$是所求出的p个主成分, 它们的特征分布分别是$\lambda_1,\lambda_2,\cdots,\lambda_p$, 将特征根&quot;归一化&quot;, 既有: $$w_i = \lambda_i/\sum_{i=1}^m\lambda_i \quad i=1,2,\cdots,p$$ 记为$W = (w_1,w_2,\cdots,w_p)&rsquo;$, 由$Y = T&rsquo;X$, 构造综合评价函数为: $$Z = w_1Y_1 + w_2Y_2 + \cdots + w_pY_p = W&rsquo;Y = W&rsquo;T&rsquo;X = (TW)&lsquo;X \tag{3.2-1}$$ 令$TW = w^*_{k \times 1}$, 带入(3.2-1)有 $$Z = (w^*)&#39;X \tag{3.2-2}$$ 综合评价函数是对原始指标的线性综合, 从计算主成分到对之加权, 经过两次线性运算后得到综合评价函数。 主成分得分 指将第t个样品(标准化后的数据)的值$X_{(t)} = (X_{t1}, X_{t2}, \cdots, X_{tp})&rsquo;$带入第$i$个主成分$Y_i = T&rsquo;_iX$, 得到的值称为第$t$个样品在第$i$个主成分的得分。 R应用 mark =read.table(&#34;eg6.1.csv&#34;, sep = &#34;,&#34;, header = T) # 计算列与列的相关系数 R = round(cor(mark), 3) # 使用相关矩阵计算 pca = princomp(mark, cor = T) summary(pca, loadings = T) #取前2个主成分，分别为课程差异因子和课程均衡因子 pre = round(predict(pca), 3) pca$scores screeplot(pca, types = &#34;lines&#34;) load = loadings(pca) plot(load[,1:2],xlim=c(-0.6,0.6),ylim=c(-0.6,0.6)) text(load[,1],load[,2],adj=c(0.5,-0.5)) abline(h=0) abline(v=0) # 主成分得分 score=as.matrix(pca$scores) sd=as.vector(pca$sdev) weight=sd^2/sum(sd^2) tscore=score%*%weight outcome=data.frame(pca$scores,tscore) 主成分回归(PCR) 多元线性回归 样本回归模型: $$Y = (I_n \quad X) {\beta_0 \choose \beta} + \mu = \beta_0I_n + X\beta + \mu$$ 模型系数向量估计量为: $${\beta_0 \choose \beta} = [(I_nX)&rsquo;(I_nX)]^{-1}(I_nX)&lsquo;Y$$ 主成分回归的原理 最小二乘估计的假定前提之一: 自变量不相关, 即自变量之间不存在共线性问题。但在经济管理中, 许多经济变量之间都存在相关关系。 解决办法 先对模型中的自变量进行主成分分析, 并提取一定的主成分; 然后用因变量对所取的主成分进行回归; 最后, 再将因变量对所提取的主成分的回归方程转化为对原自变量的回归方程, 即为主成分回归 对上述回归模型中的自变量$X_1,X_2,\cdots,X_p$的样本观测数据, 若进行主成分分析, 则可得k个主成分分别为: $$ \begin{cases} Z_1 = t_{11}X_1 + t_{12}X_2 + \cdots + t_{1p}X_p \\ Z_2 = t_{21}X_1 + t_{22}X_2 + \cdots + t_{2p}X_p \\ \cdots \\ Z_k = t_{k1}X_1 + t_{k2}X_2 + \cdots + t_{kk}X_k \end{cases} $$ 记第i个样品在第j个主成分上的得分为$Z_{ij}$, 并记全部样本的主成分得分矩阵为$Z = (z_{ij})_{n \times k}$, 即有: $$Z = \begin{bmatrix} z_{11} &amp; z_{12} &amp; \cdots &amp; z_{1k} \\ z_{21} &amp; z_{22} &amp; \cdots &amp; z_{2k} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ z_{n1} &amp; z_{n2} &amp; \cdots &amp; z_{nk} \end{bmatrix} \quad T = \begin{bmatrix} t_{11} &amp; t_{12} &amp; \cdots &amp; t_{1k} \\ t_{21} &amp; t_{22} &amp; \cdots &amp; t_{2k} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ t_{n1} &amp; t_{n2} &amp; \cdots &amp; t_{nk} \end{bmatrix} $$ 若主成分分析是根据样本协方差S做出来的, 则主成分的分矩阵为:$Z = X_0T$ 若主成分分析是根据样本相关矩阵R做出来的, 则主成分的分矩阵为:$$Z^* = X^*T^*$$ 若将主成分得分矩阵(经过中心化或标准化变换)看作是自变量的样本观测矩阵, 建立因变量Y对k个主成分的线性回归方程: $$Y = r_0 + r_1z_1 + \cdot + r_kz_k + \mu \\ 或者 \quad Y = r_0^* + r_1^*z_1^* + \cdots + r_k^*z_k^* + \mu$$ 此回归方程的因变量与原回归方程的因变量相同, 所以随机误差也相同, 并且$E(\mu) = 0, E(\mu^2) = \sigma^2$, 即可使用最小二乘法进行系数的估计。 模型系数的估计量为: $${\hat{r}_0 \choose \hat{r}} = {n \quad I&#39;_nZ \choose Z&#39;I&#39;_n \quad Z&#39;Z}^{-1}{I_nY \choose Z&#39;Y} \\ 或者 {\hat{r}_0^* \choose \hat{r}^*} = {n \quad I&#39;_nZ^* \choose Z^{*&#39;}I&#39;_n \quad Z^{*&#39;}Z}^{-1}{I_nY \choose Z^{*&#39;}Y} $$ 主成分回归的系数向量估计值与相应的原自变量回归系数向量估计值之间的关系: $$\hat{\beta} = T\hat{r} \quad 或者 \quad \hat{\beta}^* = T^*\hat{r}^*$$ 根据上述系数值即可得出因变量与原始变量的回归模型。   ]]></content></entry><entry><title>多元统计之判别分析</title><url>/post/discrimination-analysis/</url><categories><category>数据科学</category></categories><tags><tag>数理统计</tag></tags><content type="html"><![CDATA[  概述 判别分析是判别样品所属类型的一种分析方法，是在分类确定的条件下，根据某一研究对象的各种特征值判别其类型归属问题的一种多变量统计分析方法。 判别分析于聚类分析的功能差不多，区别在于，聚类分析之前，没有人知道具体的是怎么分的类，分了哪几大类。而判别分析是已经把类别给分好，要做的是把没有分好类的数据观测，按照之前分好的类再进行分类。这里不同于生活中常见的分类先有具体的分类逻辑（这里叫做判别函数）。所以判别分的难点在于先由分好类的数据观测找到一个或者多个判别函数，然后对未进行分类的观测按照该判别公式进行分类。 分类 按判别的总体数来区分: 两个总体判别分析和多总体判别分析 按区分不同总体所用的数学模型来分: 线性判别和非线性判别 常用的几种判别分析方法: 距离判别法、Fisher判别法、Bayes判别法和逐步判别法 按判别时所处理的变量方法不同: 逐步判别和序贯判别等 距离判别法 马氏距离 定义点到总体G的马氏距离为 $$D^2(X, G) = (X - \mu)&amp;rsquo;\Sigma^{-1}(X - \mu)$$ $当\Sigma = I(单位矩阵)时, 即为欧氏距离的情形$ 距离判别的思想和方法 两个总体的距离判别 问题: 设有协方差矩阵相等的两个总体$G_1$和$G_2$, 其均值分别是$\mu_1$和$\mu_2$, 对于一个新的样品X, 判断它来自哪个总体 思路: 求新样品到两个总体的马氏距离的差值$W(X)$, 若$W(X) \le 0, 则X \in G_1; 反之X \in G_2$, 其中$\bar{\mu} = \frac{\mu_1 + \mu_2}{2}$是两个总体均值的平均值, $\alpha = \Sigma^{-1}(\mu_1 - \mu_2)$ $$ \begin{aligned} W(X) &amp;amp; = D^2(X, G_1) - D^2(X, G_2) \\ &amp;amp; = -2(X - \frac{\mu_1 + \mu_2}{2})&amp;#39;\Sigma^{-1}(\mu_1 - \mu_2) \\ &amp;amp; = -2(X - \bar{\mu})&amp;#39;\alpha \\ &amp;amp; = -2\alpha&amp;#39;(X - \bar{\mu}) \end{aligned} \tag{2.2.1-1} $$  …  ]]></content></entry><entry><title>多元统计之聚类分析</title><url>/post/cluster-analysis/</url><categories><category>数据科学</category></categories><tags><tag>数理统计</tag></tags><content type="html"><![CDATA[  概述 聚类分析是研究如何将研究对象按照多个方面的特征进行综合分类的一种统计方法 聚类分析就是分析如何对样品(或变量)按照他们在性质上的亲疏程度进行量化分类的问题 聚类分析有效解决了科学研究中多因素、多指标的分类问题 类别 Q型聚类 对样品进行分类处理 R型聚类 对变量进行分类处理 分类与聚类区别 分类 是对当前所研究的问题已知它的类别数目及各类的特征(例如分布规律或来自各类的训练样本) 目的是要将另一些未知类别的个体正确地归属于其中的某一类 判别分析就是一种分类技术 聚类 是事先不知道研究的问题应分为几类, 更不知道观测到的个体的具体分类情况。 目的是通过对观测数据所进行的分析处理, 选定一种度量个体接近程度的统计量, 确定分类数目, 建立一种分类方法, 并按接近程度对观测对象给出合理的分类 方法分类 系统聚类法 系统聚类法&ndash;(分层聚类Hierarchical Cluster)系统聚类法是应用最广泛的一种聚类方法 聚类原则: 都是相近的聚为一类, 即距离最近或者最相似的聚为一类 基本思想: 距离相近的样品(或变量)先聚成类, 距离相远的后聚成类, 过程一直进行下去, 每个样品(或变量)总能聚到合适的类中 有时为了直观地反映系统聚类过程, 可以把整个分类系统画成一张谱系图(dendrogram)。因此, 系统聚类也称为谱系分析 过程: 假设总共有n个样品(或变量), 首先将每个样品(或变量)独自聚成一类, 共有n类; 然后根据所确定的样品(或变量)&ldquo;距离&quot;公式, 形成初始距离阵。之后, 将其中距离较近的两个样品(或变量)聚合为一类, 其他的样品(或变量)仍各自聚为一类 第二步再根据新合并类与其他类的&quot;距离&quot;计算公式, 再形成的新的距离阵中, 将&quot;距离&quot;最近的两个类进一步再聚成一类 以上步骤一直进行下去, 最后将所有的样品(或变量)全聚成一类 非系统聚类法 非系统聚类法&ndash;(快速聚类法&ndash;K-均值聚类法K-means Cluster) 当样本容量很大时, 系统聚类法需要占据足够大的计算机内存空间和计算时间, 因此产生了动态聚类法又称为逐步聚类法 其基本思想: 开始先粗略地分一下类, 然后按照某种最优的原则修改不合理的分类, 直至类分得比较合理为止 该方法具有计算量小, 占用计算机内存空间较少, 方法简单, 适用于大样本的Q型聚类分析 K-均值聚类法K-means Cluster K均值法和系统聚类法一样, 都是以距离的远近亲疏为标准进行聚类的 区别: 系统聚类对不同的类数产生一系列的聚类结果 K均值法只能产生指定类数的聚类结果 相似性的度量 样品相似性的度量 Q型聚类分析，常用距离来测度样品之间的相似程度。两个样品间的相似程度就可用p维空间中的两点距离公式来度量。样本阵如下: $$X=\left[ \begin{matrix} X_{11} &amp; X_{12} &amp; \cdots &amp; X_{1p}\\ X_{21} &amp; X_{22} &amp; \cdots &amp; X_{2p}\\ \vdots &amp; \vdots &amp; &amp; \vdots\\ X_{n1} &amp; X_{n2} &amp; \cdots &amp; X_{np} \end{matrix} \right]$$ 因此，任何两个样品$X_i$与$X_j$之间的相似性可以通过矩阵中的第i行与第j行的相似程度来刻划。 变量相似性的度量 变量间的相似性要从它们的方向趋同性或&quot;相关性&quot;进行考察, 度量方法有: 夹角余弦法和相关系数 夹角余弦 定义: 二维向量a和b的夹角余弦: $cos(\theta) = \frac{a \cdot b}{||a|| \times ||b||} = \frac{(x_1, y_1) \cdot (x_2, y_2)}{\sqrt{x_1^2 + y_1^2} \times \sqrt{x_2^2 + y_2^2}} = \frac{x_1 x_2 + y_1 y_2}{\sqrt{x_1^2 + y_1^2} \times \sqrt{x_2^2 + y_2^2}}$ 两变量$X_i$与$X_j$看作n维空间(n个样品所张成的空间)的两个向量 $$ cos(\theta) = \frac{\sum_{i=1}^n (X_i \times Y_i)}{\sqrt{\sum_{i=1}^n (X_i)^2} \times \sqrt{\sum_{i=1}^n (Y_i)^2}} = \frac{a \cdot b}{||a|| \times ||b||} $$ 说明 余弦相似度用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小 余弦值越接近1，就表明夹角越接近0度，也就是两个向量越相似 应用 文本相似度计算 对句子A和B分词 列出所有句子的词 计算词频 列出句子A和B的词频向量a和b 计算a和b的夹角余弦 相关系数 概述 相关系数经常用来度量变量间的相似性。变量$X_i$与$Y_i$的相关系数定义为: $$r_{ij} = \frac{\sum_{k=1}^n(X_{ki} - \bar{X_i})(X_{kj} - \bar{X_j})}{\sqrt{\sum_{k=1}^n(X_{ki} - \bar{X_i})^2 \sum_{k=1}^n(X_{kj} - \bar{X_j})^2}}$$ 显然$|r_{ij}| &lt; 1$。事实上, 相关系数是将样本观测数据中心化或标准化后的夹角余弦 注意: 无论是夹角余弦还是相关系数, 把他们统称为相似系数, 记为$c_{ij}$ 当$|c_{ij}| = 1$时, 说明变量$X_i$与$X_j$完全相似 当$|c_{ij}|$近似于1时, 说明变量$X_i$与$X_j$非常密切 当$|c_{ij}| = 0$时, 说明变量$X_i$与$X_j$完全不一样 当$|c_{ij}|$近似于0时, 说明变量$X_i$与$X_j$差别很大 如果需要用距离来测定变量间的亲疏程度, 则变量之间常借助于相似系数来定义距离: $$d_{ij} = 1 - |c_{ij}| 或者 d_{ij}^2 = 1- c_{ij}^2$$ 类间距与系统聚类法 概述: 系统聚类法共8种: 最短距离法, 最长距离法, 中间距离法, 重心法, 类平均法, 可变类平均法, 可变法和离差平方和法 它们的归类和步骤是一致的, 主要差异: 两类间的距离; 新合并类与其他类间距离的计算方法不同 最短距离法 用$d_{ij}$表示样品$X_i$与$X_j$之间的距离, 用$D_{ij}$表示类$G_i$与$G_j$之间的距离。定义类$G_i$与$G_j$之间的距离为两类最近样品的距离, 即为: $$D_{ij} = \min\limits_{X_i \in G_i, X_j \in G_j} d_{ij}$$ 最长距离法 用$d_{ij}$表示样品$X_i$与$X_j$之间的距离, 用$D_{ij}$表示类$G_i$与$G_j$之间的距离。定义类$G_i$与$G_j$之间的距离为两类最远样品的距离, 即为: $$D_{ij} = \max\limits_{X_i \in G_i, X_j \in G_j} d_{ij}$$ 中间距离法 取最长距离和最短距离的中线, 则这个中线距离的平方为: $$D_{kr}^2 = \frac{1}{2}D_{kp}^2 + \frac{1}{2}D_{kq}^2 - \frac{1}{4}D_{pq}^2$$ 重心法 概述: 重心法是定义类间距离为两类样品重心(各类样品的均值)的距离的系统聚类方法。重心指标对类有很好的代表性，但利用各样本的信息不充分 设$G_p$与$G_q$分别有样品$n_p,n_q$个, 其重心分别为$\bar{X_p}和\bar{X_q}$, 则$G_p$与$G_q$之间的距离定义为$X_p和X_q$之间的距离, 用欧式距离来表示, 即 $$D_{pq}^2 = (\bar{X_p} - \bar{X_q})&rsquo;(\bar{X_p} - \bar{X_q})$$ 设将$G_p和G_q$合并为$G_r$, 则$G_r$内样品个数为$n_r = n_p + n_q$, 它的重心是$\bar{X_r} = \frac{1}{n_r}(n_p \bar{X_p} + n_q \bar{X_q})$, 类$G_k$的重心是$\bar{X_k}$, 那么它与新类$G_r$的距离为: $$ \begin{aligned} D_{kr}^2 &amp; = (\bar{X_k} - \bar{X_r})&#39;(\bar{X_k} - \bar{X_r}) \\ &amp; = [\bar{X_k} - \frac{1}{n_r}(n_p \bar{X_p} + n_q \bar{X_q})]&#39;[\bar{X_k} - \frac{1}{n_r}(n_p \bar{X_p} + n_q \bar{X_q})] \\ &amp; = \frac{n_p}{n_r} D_{kp}^2 + \frac{n_q}{n_r} D_{kq}^2 - \frac{n_p n_q}{n_r^2} D_{pq}^2 \end{aligned} $$ 类平均法 概述: 重心法虽然有较好的代表性, 但并未充分利用样本信息。而类平均法定义两类之间的距离平方为两类两两样品之间距离平方的平均数，即为 $$D_{pq}^2 = \frac{1}{n_pn_q} \sum_{X_i \in G_p} \sum_{X_j \in G_q} d_{ij}^2$$ 设聚类的某一步将$G_p和G_q$合并为新类$G_r$, 则类$G_k$与新并类$G_r$的距离为: $$ \begin{aligned} D_{kr}^2 &amp; = \frac{1}{n_kn_r} \sum_{X_i \in G_k} \sum_{X_j \in G_r} d_{ij}^2 \\ &amp; = \frac{1}{n_kn_r} (\sum_{X_i \in G_k} \sum_{X_j \in G_p} d_{ij}^2 + \sum_{X_i \in G_k} \sum_{X_j \in G_q} d_{ij}^2) \\ &amp; = \frac{n_p}{n_r} D_{kp}^2 + \frac{n_q}{n_r} D_{kq}^2 \end{aligned} $$ 可变类平均法 概述: 由于类平均法中没有反映出$G_p和G_q$之间的距离$D_{pq}$的影响, 信息利用不充分。为了充分利用各类距离的信息，可将类平均法和中间距离法进行组合，得到一个组合模型，此组合模型称为可变类平均法 如果将$G_p和G_q$合并为新类$G_r$, 类$G_k与新并类G_r$的距离公式为: $$D_{kr}^2 = (1 - \beta) (\frac{n_p}{n_r}D_{kp}^2 + \frac{n_q}{n_r}D_{kq}^2) + \beta D_{pq}^2$$ 之所以称该方法为可变类平均法是因为$\beta$是可变的 系数$\beta$称为聚类强度系数, $\beta$的取值不同, 聚类的结果就会不同。通常$\beta$取负值时可以使聚类的分辨能力提高; 一般情况下, 取$\beta = - \frac{1}{4}$ 可变法 概述: 不考虑$G_p和G_q$两类h中各自样品的个数, 而是类同等看待, 则得到可变法 $$D_{kr}^2 = \frac{1 - \beta}{2}(D_{kp}^2 + D_{kq}^2) + \beta D_{pq}^2$$ 离差平方和法(Ward法) 概述: 该方法的思想来自于方差分析, 由Ward提出, 如果分类正确, 同类样品的离差平方和应当较小, 类与类的离差平方和较大 具体做法是先将n个样品各自成一类, 然后每次缩小一类, 每缩小一类, 离差平方和就要增大, 选择使方差增加最小的两类合并, 直到所有的样品归为一类为止 设将n个样品分成k类$G_1,G_2,\cdots,G_k$, 用$X_{it}$表示$G_t$中的第i个样品, $n_t$表示$G_t$中样品的个数, $\bar{X_t}$是$G_t$的重心, 则$G_t$的样品离差平方和为: $$S_t = \sum_{i=1}^{n_t}(X_{it} - \bar{X_t})&#39;(X_{it} - \bar{X_t})$$ 如果$G_p$和$G_q$合并为新类$G_r$, 则类内离差平方和分别为: $$ S_p = \sum_{i=1}^{n_p}(X_{ip} - \bar{X_p})&#39;(X_{ip} - \bar{X_p}) \\ S_q = \sum_{i=1}^{n_q}(X_{iq} - \bar{X_q})&#39;(X_{iq} - \bar{X_q}) \\ S_r = \sum_{i=1}^{n_r}(X_{ir} - \bar{X_r})&#39;(X_{ir} - \bar{X_r}) $$ 它们反映了各自类内样品的分散程度, 如果$G_p$和$G_q$这两类相距较近, 则合并后所增加的离散平方和$S_r - S_p - S_q$应较小; 否则, 应较大。于是定义$G_p$和$G_q$之间的平方距离为: $D_{pq}^2 = S_r - S_p - S_q$ 如果将$G_p$和$G_q$合并为新类$G_r$, 类$G_k$与新并类$G_r$的距离公式为: $$D_{kr}^2 = \frac{n_k + n_p}{n_r + n_k} D_{kp}^2 + \frac{n_k + n_q}{n_r + n_k} D_{kq}^2 - \frac{n_k}{n_r + n_k} D_{pq}^2$$ 类间距离的统一性 上述8种系统聚类法的步骤完全一样, 只是距离的递推公式不同。Lance和Williams于1967年给出统一公式, 即将$G_p$和$G_q$合并为新类$G_r$, 类$G_k$与新并类$G_r$的距离公式为: $$D_{kr}^2 = \alpha_p D_{kp}^2 + \alpha_q D_{kq}^2 + \beta D_{pq}^2 + \gamma |D_{kp}^2 - D_{kq}^2|$$ 其中$\alpha_p、\alpha_q、\beta、\gamma$是参数, 不同的系统聚类法, 它们的取不同的数 系统聚类法参数表 方法 $\alpha_p$ $\alpha_q$ $\beta$ $\gamma$ 最短距离法 1/2 1/2 0 -1/2 最长距离法 1/2 1/2 0 1/2 中间距离法 1/2 1/2 -1/4 0 重心法 $n_p/n_r$ $n_q/n_r$ $-\alpha_p \alpha_q$ 0 类平均法 $n_p/n_r$ $n_q/n_r$ 0 0 可变类平均法 $(1 - \beta)n_p/n_r$ $(1 - \beta)n_q/n_r$ $\beta(&lt;1)$ 0 可变法 $(1 - \beta)/2$ $(1 - \beta)/2$ $\beta(&lt;1)$ 0 离差平方和法 $\frac{(n_p + n_k)}{(n_r + n_k)}$ $\frac{(n_q + n_k)}{(n_r + n_k)}$ $-n_k/(n_k + n_r)$ 0 类个数的确定 由适当的阀值确定 根据数据点的散布图直观地确定类的个数 系统聚类法在R上的实现 示例 data(iris) attach(iris) class=data.frame(c(1,2,3),as.vector(unique(iris[,5]))) names(class)=c(&#34;cla&#34;,&#34;name&#34;) #系统聚类法 d=dist(iris[,-5],method = &#34;euclidean&#34;,diag = T,upper = T,p=2) hc=hclust(d,method = &#34;ward&#34;) plot(hc) # 分成3类 rt1=data.frame(cutree(hc,k=3)) names(rt1)=&#34;cla&#34; rt1=merge(rt1,class,by=&#34;cla&#34;) tab1=table(rt1$name,iris[,5]) sum(diag(prop.table(tab1))) # k均值聚类法 km=kmeans(iris[,-5],3,nstart=20,algorithm = &#34;Hartigan-Wong&#34;) rt2=data.frame((km$cluster)) names(rt2)=&#34;cla&#34; rt2=merge(rt2,class,by=&#34;cla&#34;) tab2=table(rt2$name,iris[,5]) sum(diag(prop.table(tab2)))   ]]></content></entry><entry><title>统计中常用的分布</title><url>/post/the-common-distribution-of-statistics/</url><categories><category>数据科学</category></categories><tags><tag>数理统计</tag></tags><content type="html"><![CDATA[  概述 相关概念 随机变量: 设随机试验的样本空间为S(e) = {e}. X = X(e)是定义在样本空间S上的实值单值函数. 称X = X(e)为随机变量. 离散型随机变量: 有些随机变量, 它全部可能取到的值是有限个或可列无限多个, 这种随机变量称为离散型随机变量. 三种重要的离散型随机变量 (0-1)分布 定义: 设随机变量X只可能取到0与1两个值, 它的分布律是 $$P{X=k} = p^k(1-p)^{1-k}, \quad k = 0,1 \quad (0 &lt; p &lt; 1)$$ 则称X服从以p为参数的(0-1)分布或两点分布. 伯努利试验, 二项分布(Binomial distribution) 概念: 设试验E只有两个可能的结果: $A及\bar{A}$, 则称E为伯努利(Bernoulli)试验. 设$P(A) = p \quad (0 &lt; p &lt; 1)$, 此时$P(\bar{A} = 1-p)$. 将E独立重复地进行n次, 则称这一串重复的独立试验为n重伯努利试验. 二项分布(Binomial distribution) 定义: ${n \choose k}p^kq^{n - k}$刚好是二项式$(p + q)^n$的展开式中出现p^k的那一项, 我们称随机变量X服从参数为n, p的二项分布, 并记为$X \sim b(n,p)$, 其中${n \choose k} = \frac{n!}{k!(n -k)!}, \quad k= 0,1,\cdots,n$ 性质 特别, 当n = 1时, 二项分布$P{X = k} = p^kq^{1 -k}, \quad k=0,1$转化为(0-1)分布 泊松分布(Poissson distribution) 概述: 用来描述在一指定时间范围内或在指定的面积或体积之内某一事件出现的次数的分布. 定义: 设随机变量X所有可能的取值为$0, 1, 2, \cdots,$而取各个值的概率为 $$P{X = k} = \frac{\lambda^ke^{-\lambda}}{k!}, \quad k=0,1,2,\cdots$$ 其中$\lambda &gt; 0$是常数. 则称X服从参数$\lambda$的泊松分布, 记为$X \sim \pi(\lambda)$ 三种重要的连续型随机变量 均匀分布 定义: 若连续型随机变量X具有概率密度 $$f(x) = \begin{cases} \frac{1}{b - a}, \quad a &lt; x &lt; b, \\ 0, \quad 其他, \end{cases}$$ 则称X在区间(a, b)上服从均匀分布. 记为$X \sim U(a,b).$ 指数分布 定义: 若连续型随机变量X的概率密度为 $$f(x) = \begin{cases} \frac{1}{\theta}e^{-x/\theta}, \quad x &gt; 0, \\ 0, 其他 \end{cases}$$ 其中$\theta &gt; 0$为常数, 则称X服从参数为$\theta$的指数分布. 正态分布(Normal distribution) 定义: 若连续型随机变量X的概率密度为 $$f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x - \mu)^2}{2\sigma^2}}, \quad -\infty &lt; x &lt; \infty$$ 其中$\mu, \sigma(\sigma&gt;0)$为常数, 则称X服从参数为$\mu, \sigma$的正态分布或高斯(Gauss)分布, 记为$X \sim N(\mu, \sigma^2).$ X的分布函数为 $$F(x) = \frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^x e^{-\frac{(t - \mu)^2}{2\sigma^2}}dt$$ 当$\mu = 0, \sigma = 1$时, 称随机变量X服从标准正态分布 其概率密度和分布函数分别用$\varphi(x), \varPhi(x)$表示 $$ \varphi(x) = \frac{1}{\sqrt{2\pi}}e^{-t^2/2} \\ \varPhi(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^xe^{-t^2/2}dt \\ 易知\quad \varPhi(-x) = 1 - \varPhi(x) $$ 区间概率 $$ \begin{aligned} &amp; P\{\mu - \sigma &lt; X &lt; \mu + \sigma\} = \varPhi(1) - \varPhi(-1) = 2\varPhi(1) - 1 = 68.26\% \\ &amp; P\{\mu - 2\sigma &lt; X &lt; \mu + 2\sigma\} = \varPhi(2) - \varPhi(-2) = 95.44\% \\ &amp; P\{\mu - 3\sigma &lt; X &lt; \mu + 3\sigma\} = \varPhi(3) - \varPhi(-3) = 99.74\% \end{aligned} $$ 尽管正态变量的取值范围是$(-\infty, +\infty)$, 但它的值落在$(\mu - 3\sigma, \mu + 3\sigma)$几乎是肯定的事. 这就是&quot;$3\sigma$&ldquo;法则. 性质 若$X \sim N(\mu, \sigma^2)$, 则$Z = \frac{X - \mu}{\sigma} \sim N(0,1)$ 抽样分布 卡方分布(chi-square distribution) 概述: 设$X_1,X_2,\cdots,X_n$是来自总体N(0,1)的样本, 则称统计量 $$\chi^2 = X_1^2 + X_2^2 + \cdots + X_n^2$$ 服从自由度为n的$\chi^2$分布, 记为$\chi^2 \sim \chi^2(n).$ 此处自由度为包含独立变量的个数. $\chi^2(n)$分布的概率密度为 $$f(y) = \begin{cases} \frac{1}{2^{n/2}\Gamma(n/2)}y^{n/2 - 1}e^{-y/2}, \quad y&gt;0, \\ 0, \quad 其他 \end{cases}$$ 性质: $\chi^2$分布的可加性: 设$\chi_1^2 \sim \chi^2(n_1), \chi_2^2 \sim \chi^2(n_2)$, 并且$\chi_1^2, \chi_2^2$相互独立, 则有 $$\chi_1^2 + \chi_2^2 \sim \chi^2(n_1 + n_2)$$ $\chi_2$分布的数学期望和方差: 若$\chi^2 \sim \chi^2(n)$, 则有 $$E(\chi^2) = n, \quad D(\chi^2) = 2n$$ $\chi^2$分布的分位点: 对于给定的正数$\alpha, 0 &lt; \alpha &lt; 1$, 称 满足条件 $$P{\chi^2 &gt; \chi_\alpha^2(n)} = \int_{\chi_\alpha^2(n)}^\infty f(y)dy = \alpha$$ 的点$\chi_\alpha^2(n)$为$\chi^2(n)$分布上的分位点. t分布(学生氏分布) 概述: 设$X \sim N(0,1), Y \sim \chi^2(n)$, 且X, Y相互独立, 则称随机变量 $$t = \frac{X}{\sqrt{Y/n}}$$ 服从自由度为n的t分布. 记为$t \sim t(n)$ t(n)分布的概率密度函数为 $$h(t) = \frac{\Gamma[(n + 1)/2]}{\sqrt{\pi n}\Gamma(n/2)}(1 + \frac{t^2}{n})^{-(n + 1)/2}, \quad -\infty &lt; t &lt; \infty$$ 性质 t分布的分位点: 对于给定的$\alpha, 0 &lt; \alpha &lt; 1,$称满足条件 $$P{t &gt; t_\alpha(n)} = \int_{t_\alpha(n)}^\infty h(t)dt = \alpha$$ 的点$t_\alpha(n)$为t(n)分布上的分位点. F分布 概述: 设$U \sim \chi^2(n_1), V \sim \chi^2(n_2)$, 且$U,V$相互独立, 则称随机变量 $$F = \frac{U/n_1}{U/n_2}$$ 服从自由度为$(n_1,n_2)$的F分布, 记为$F \sim F(n_1,n_2)$ $F(n_1, n_2)$分布的概率密度为 $$\varphi(y) = \begin{cases} \frac{\Gamma[(n_1 + n_2)/2](n_1 + n_2)^{n_1/2}y^{n_1/2 - 1}}{\Gamma(n_1/2)\Gamma(n_2/2)[1 + (n_1y/n_2)]^{(n_1 + n_2)/2}}, \quad y&gt;0 \\ 0, \quad others \end{cases}$$ 性质 若$F \sim F(n_1,n_2),$则 $$\frac{1}{F} \sim F(n_2, n_1)$$ F的分位点: 对于给定的$\alpha, 0 &lt; \alpha &lt; 1,$则称满足条件 $$P{F &gt; F_\alpha(n_1,n_2)} = \int_{F_\alpha(n_1,n_2)}^\infty \varphi(y)dy = \alpha$$ 的点$F_\alpha(n_1,n_2)$为$F(n_1,n_2)$分布的上$\alpha$分位点. 参考 统计学(第7版) 中国人民大学出版社 概率论与数理统计(第四版) 高等教育出版社   ]]></content></entry><entry><title>Spark程序相关调优整理</title><url>/post/optimization-of-spark-program/</url><categories><category>数据科学</category></categories><tags><tag>大数据</tag></tags><content type="html"> 普通调优 jvm调优 数据倾斜 数据倾斜只会发生在 shuffle 过程中 在进行 shuffle 的时候，必须将各个节点上相同的 key 拉取到某个节点上的一个 task 来进行处理，比如按照 key 进行聚合或 join 等操作。此时如果某个 key 对应的数据量特别大的话，就会发生数据倾斜。 shuffle调优 Spark 是根据 shuffle 类算子来进行 stage 的划分 触发 shuffle 操作的算子: distinct、groupByKey、reduceByKey、 aggregateByKey、join、cogroup、repartition 等 可以通过 Spark Web UI 来查看当前运行到了第几个 stage, 看一下当前这个stage 各个 task 分配的数据量，从而进一步确定是不是 task 分配的数据不均匀 导致了数据倾斜。 参数调优 spark.shuffle.file.buffer 默认值: 32k 参数说明: 该参数用于设置 shuffle write task 的 BufferedOutputStream 的buffer 缓冲大小。将数据写到磁盘文件之前，会先写入 buffer 缓冲中，待缓冲 写满之后，才会溢写到磁盘. 调优建议: 如果作业可用的内存资源较为充足的话，可以适当增加这个参 数的大小(比如 64k)，从而减少 shuffle write 过程中溢写磁盘文件的次数，也 就可以减少磁盘 IO 次数，进而提升性能。在实践中发现，合理调节该参数， 性能会有 1%~5%的提升. spark.reducer.maxSizeInFlight 默认值: 48m 参数说明:该参数用于设置 shuffle read task 的 buffer 缓冲大小，而这个buffer 缓冲决定了每次能够拉取多少数据. 调优建议:如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小(比如 96m)，从而减少拉取数据的次数，也就可以减少网络传输的 次数，进而提升性能。在实践中发现，合理调节该参数，性能会有 1%~5% 的提升. spark.shuffle.io.maxRetries 默认值: 3 参数说明:shuffle read task 从 shuffle write task 所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代 表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会 导致作业执行失败。 调优建议:对于那些包含了特别耗时的 shuffle 操作的作业，建议增加重试 最大次数(比如 60 次)，以避免由于 JVM 的 full gc 或者网络不稳定等因素导 致的数据拉取失败。在实践中发现，对于针对超大数据量(数十亿~上百亿)的 shuffle 过程，调节该参数可以大幅度提升稳定性. 解决方案 Kafka数据源, 均匀分布在不同partitio种 Hive中key分布不均衡, 处理成Hive中间表 调整并行度分散同一个Task的不同key SparkSQL: spark.sql.shuffle.partitions, shuffle read task 的并行度，该值默认是 200，对于很多场景来说都有点过小 RDD: reduceByKey(1000) 程序开发调优 RDD相关 避免创建重复的 RDD 尽可能复用同一个 RDD 对多次使用的 RDD 进行持久化: 调用RDD的cache()和persist()方法 cache()方法表示: 使用非序列化的方式将 RDD 中的数据全部尝试持久化到内存中. persist()方法表示: 手动选择持久化级别，并使用指定的方式进行持久化. 使用 MEMORY_ONLY发生OOM, 尝试使 用 MEMORY_ONLY_SER/MEMORY_AND_DISK_SER 级别. 算子调优 尽量避免使用 shuffle 类算子, 尽量使用 map 类的非 shuffle 算子 使用 map-side 预聚合的 shuffle 操作 通常来说，在可能的情况下，建议 使用 reduceByKey 或者 aggregateByKey 算子来替代掉 groupByKey 算子 使用高性能的算子 使用 reduceByKey/aggregateByKey 替代 groupByKey 使用 mapPartitions 替代普通 map, 注意出现OOM的情况(单partition数据量过大) 使用 foreachPartitions 替代 foreach 使用 filter 之后进行 coalesce 操作 使用 repartitionAndSortWithinPartitions 替代 repartition 与 sort 类操作 广播大变量 在算子函数中使用到外部变量时，默认情况下，Spark 会将该变量复制多 个副本，通过网络传输到 task 中，此时每个 task 都有一个变量副本。大量的变量副本在网络中传输的性能开销，以及在各个节点的 Executor 中占用过多内存导致的频繁 GC，都 会极大地影响性能。 广播后的变量，会保证每个 Executor 的内存中，只 驻留一份变量副本，而 Executor 中的 task 执行时共享该 Executor 中的那份变 量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能 开销，并减少对 Executor 内存的占用开销，降低 GC 的频率。 使用 Kryo 优化序列化性能 优化数据结构 三种类型比较耗费内存: 对象, 字符串和集合类型 在 Spark 编码实现中，特别是对于算子函数中的代 码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类 型(比如 Int、Long)替代字符串，使用数组替代集合类型，这样尽可能地减少 内存占用，从而降低 GC 频率，提升性能。 运行资源调优 参数 num-executors 参数调优建议:每个 Spark 作业的运行一般设置 50~100 个左右的 Executor 进程比较合适，设置太少或太多的 Executor 进程都不好。设置的太少，无法充 分利用集群资源;设置的太多的话，大部分队列可能无法给予充分的资源。 executor-memory 参数说明: 该参数用于设置每个 Executor 进程的内存。Executor 内存的大 小，很多时候直接决定了 Spark 作业的性能，而且跟常见的 JVM OOM 异常， 也有直接的关联。 参数调优建议: 每个 Executor 进程的内存设置 4G~8G 较为合适。 但是这 只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看 自己团队的资源队列的最大内存限制是多少，num-executors 乘以 executor- memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共 享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的 1/3~1/2，避免你自己的 Spark 作业占用了队列所有的资源，导致别的同学的作 业无法运行。 executor-cores driver-memory spark.default.parallelism 参数说明: 该参数用于设置每个 stage 的默认 task 数量。这个参数极为重要，如果不设置可能会直接影响你的 Spark 作业性能。 参数调优建议: Spark 作业的默认 task 数量为 500~1000 个较为合适。 官网建议的设置原则是，设置该参数为 num-executors * executor-cores 的 2~3 倍较为合适</content></entry><entry><title>HDFS快速入门</title><url>/post/quickstart-of-hdfs/</url><categories><category>数据科学</category></categories><tags><tag>大数据</tag></tags><content type="html"><![CDATA[  概述 简介: 源自Google的GFS论文的巨大分布式文件系统 特点: 扩展性 容错性 海量数据存储 优缺点: 优点: 数据冗余, 硬件容错 处理流式(一次写入多次读取)的数据访问 适合存储大文件 构建在廉价机器上 缺点: 不满足低延迟的数据访问 不适合小文件的存储 存储: 将文件切分成指定大小的数据块并以多副本的存储在多个机器上 数据的切分,多副本,容错等操作对用户透明 NameNode (Filename, numReplicas, block-ids) 架构 简介: 一个Master(NameNode/NN)带N个Slaves(DataNode/DN) 一个文件会被拆分成多个Block, 默认blocksize:128M NameNode 负责客户端请求的响应 负责元数据(文件的名称、副本系数、Block存放的DN)的管理 DataNode 存储用户的文件对应的数据块(Block) 要定期向NN发送心跳信息, 汇报本身及所有的block信息, 健康状况 副本机制 副本存放策略 HDFS文件读写流程 环境搭建 伪分布式 jdk和ssh安装 免密登录配置: ssh-keygen -t rsa cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys ssh localhost 下载解压 tar -zxvf hadoop-x.x.x-cdhx.x.x -C ~/app 配置文件修改: hadoop_home/etc/hadoop hadoop-env.sh配置: JAVA_HOME=/usr/lib/jvm/java-x.x core-site.xml配置: 默认目录位于/tmp, 重启后会丢失 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;!-- 外网访问改成ip --&gt; &lt;value&gt;hdfs://localhost:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/tmp&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; hdfs-site.xml配置: &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; slavs配置(伪分布式不需要配置) 启动 # 格式化文件系统, 仅第一次执行, 使用hdfs代替hadoop(depressed)) ./bin/hdfs namenode -format # 启动NameNode, DataNode ./sbin/start-dfs.sh # 进程验证 DataNode SecondaryNameNode NameNode jps # 网页方式验证 curl http://localhost:50070/ # 防火墙添加规则 # vim /etc/sysconfig/iptables -A INPUT -p tcp -m tcp --dport 50070 -j ACCEPT 停止: ./sbin/stop-dfs.sh HDFS Shell操作 配置hadoop/bin的环境变量 # vi ~/.bashrc export HADOOP_HOME=/opt/apache/hadoop/hadoop-2.6.0-cdh5.7.0 export PATH=$HADOOP_HOME/bin:$PATH # 使生效 source ~/.bashrc 查看命令参数: hadoop fs(hdfs dfs) 常用操作 ls: # 查看根目录 hadoop fs -ls / # 递归查看 hadoop fs -ls -R / mkdir # 根目录创建test目录 hadoop fs -mkdir /test # 递归创建目录 hadoop fs -mkdir -p /test/a put: 将本地文件传到hdfs # 将hello.txt传到hdfs根目录 hadoop fs -put hello.txt / get # 将hdfs中的/test/a/h.txt拷贝到本地 hadoop fs -get /test/a/h.txt rm: 删除文件 # 删除文件 hadoop fs -rm /hello.txt # 递归删除 hadoop fs -rm -R /test cat: 查看文件内容 hadoop fs -cat /hello.txt # 也可以使用-text hadoop fs -text /hello.txt copyFromLocal: 从本地拷贝到hdfs # 将本地hello.txt文件拷贝到hdfs的/test/a目录下, 文件名为h.txt fs -copyFromLocal hello.txt /test/a/h.txt test hdfs dfs -test -[ezd] URI Java API操作HDFS 默认副本系数为3 步骤 添加依赖 &lt;properties&gt; &lt;cdh.version&gt;2.6.0-cdh5.7.0&lt;/cdh.version&gt; &lt;/properties&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;${cdh.version}&lt;/version&gt; &lt;/dependency&gt; 创建连接 // 该路径为core-site.xml中配置 public static final String HADOOP_PATH = &#34;hdfs://192.168.33.12:8020&#34;; FileSystem fileSystem = null; Configuration configuration = null; configuration = new Configuration(); // 指定用户为hadoop fileSystem = FileSystem.get(new URI(HADOOP_PATH), configuration, &#34;hadoop&#34;); // 创建文件夹 fileSystem.mkdirs(new Path(&#34;/hdfsapi/test&#34;)); //创建文件并写入内容 FSDataOutputStream outputStream = fileSystem.create(new Path(&#34;/hdfsapi/test/a.txt&#34;)); outputStream.write(&#34;hello hdfs&#34;.getBytes()); outputStream.flush(); outputStream.close(); //将内容输出到控制台 FSDataInputStream inputStream = fileSystem.open(new Path(&#34;/hdfsapi/test/a.txt&#34;)); IOUtils.copyBytes(inputStream, System.out, 1024); // IOUtils为hadoop.io包 inputStream.close(); // 重命名 Path srcPath = new Path(&#34;/hdfsapi/test/a.txt&#34;); Path dstPath = new Path(&#34;/hdfsapi/test/b.txt&#34;); boolean rename = fileSystem.rename(srcPath, dstPath); System.out.println(rename); // 本地文件上传到hdfs Path localPath = new Path(&#34;E:\\WorkSpace\\data\\test.txt&#34;); Path hdfsPath = new Path(&#34;/hdfsapi/test&#34;); fileSystem.copyFromLocalFile(localPath, hdfsPath); //带进度的大文件上传 InputStream inputStream = new BufferedInputStream( new FileInputStream(new File(&#34;E:\\WorkSpace\\Web\\bigFile&#34;)) ); FSDataOutputStream fsDataOutputStream = fileSystem.create(new Path(&#34;/hdfsapi/test&#34;), new Progressable() { @Override public void progress() { System.out.println(&#34;.&#34;); } }); IOUtils.copyBytes(inputStream, fsDataOutputStream, 4096); // 下载文件 Path localPath = new Path(&#34;E:\\WorkSpace\\b.txt&#34;); Path hdfsPath = new Path(&#34;/hdfsapi/test/b.txt&#34;); fileSystem.copyToLocalFile(hdfsPath, localPath); // 列出文件信息 FileStatus[] listStatus = fileSystem.listStatus(new Path(&#34;/hdfsapi/test&#34;)); for (FileStatus status : listStatus) { String file = status.isDirectory() ? &#34;文件夹&#34; : &#34;文件&#34;; short replication = status.getReplication(); //副本 long len = status.getLen(); // 文件大小 String path = status.getPath().toString(); System.out.println(file + &#34;\t&#34; + replication + &#34;\t&#34; + len + &#34;\t&#34; + path); } //删除文件 fileSystem.delete(new Path(&#34;/hdfsapi/test/a.txt&#34;), true); HDFS文件读写流程 存储数据 client(divide into blocks)-&gt;NameNode-&gt;DataNode 读取数据 client从NameNode获取元数据信息 client从DataNode请求数据 参考 Pseudo-Distributed Operation Using the CDH 5 Maven Repository hadoop fs   ]]></content></entry><entry><title>大数据集群CDH5.11.0搭建及配置</title><url>/post/install-and-conf-of-cdh5110/</url><categories><category>数据科学</category></categories><tags><tag>大数据</tag></tags><content type="html"><![CDATA[  环境 系统环境 CentOS7 3台 # host 192.168.237.100 hadoop001 192.168.237.110 hadoop002 192.168.237.120 hadoop003 SSH免密登录 关闭防火墙 # 关闭防火墙 systemctl stop firewalld # 关闭开启自启 systemctl disable firewalld SELINUX关闭 setenforce 0 sed -i &#34;s/SELINUX=enforcing/SELINUX=disabled/&#34; /etc/selinux/config iptables --flush reboot #重启生效 软件环境 JDK1.8 # 查看是否安装openjdk rpm -qa | grep jdk # 卸载openjdk, 否则Cloudera 安装parcel hang rpm -e java-1.8.0-openjdk-devel-1.8.0.181-3.b13.el7_5.x86_64 java-1.8.0-openjdk-1.8.0.181-3.b13.el7_5.x86_64 java-1.8.0-openjdk-headless-1.8.0.181-3.b13.el7_5.x86_64 # 安装Oracle JDK 最好将JDK安装在/usr/java/default中，有的版本要求安装在此目录，要不然找不到JAVA_HOME
MySQL5.7 #为hive建库hive mysql&gt;create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci; #为Activity Monitor建库amon mysql&gt;create database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci; #为Oozie建库oozie mysql&gt;create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci; #为Hue建库hue mysql&gt;create database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci; mysql&gt;grant all privileges on *.* to &#39;root&#39;@&#39;hadoop001&#39; identified by &#39;root@pierce&#39; with grant option; mysql&gt;flush privileges; 依赖安装 # pstree yum install psmisc 安装说明 在线 离线(推荐): 系统侵入性小, 便于升级 软件下载与安装 # cm5.11.0下载 wget http://archive.cloudera.com/cm5/cm/5/cloudera-manager-centos7-cm5.11.0_x86_64.tar.gz # cdh5.11.0下载 wget http://archive.cloudera.com/cdh5/parcels/5.11.0/CDH-5.11.0-1.cdh5.11.0.p0.34-el7.parcel wget http://archive.cloudera.com/cdh5/parcels/5.11.0/CDH-5.11.0-1.cdh5.11.0.p0.34-el7.parcel.sha1 安装Cloudera Manager Server 和Agent 解压cm sudo tar -zxvf cloudera-manager-centos7-cm5.11.0_x86_64.tar.gz -C /opt/cm/ 配置 CM Server数据库（master） sudo cp mysql-connector-java-5.1.6-bin.jar /opt/cm/cm-5.11.0/share/cmf/lib/ # 创建数据库scm, 用户scm, 密码scm /opt/cm/cm-5.11.0/share/cmf/schema/scm_prepare_database.sh mysql -hhadoop001 -uroot -proot --scm-host hadoop001 scm scm scm 配置 CM Agent(master) # vi /opt/cm/cm-5.11.0/etc/cloudera-scm-agent/config.ini [General] # Hostname of the CM server. server_host=hadoop001 #修改为主节点主机名 将Agent文件从主节点分发到其他从节点:(master) scp -r /opt/cm/cm-5.11.0 hadoop002:/opt/cm/cm-5.11.0 scp -r /opt/cm/cm-5.11.0 hadoop003:/opt/cm/cm-5.11.0 创建用户cloudera-scm(hadoop001,hadoop002,hadoop003): 每个节点上执行命令创建cloudera-scm用户，因为CM使用cloudera-scm用户管理 useradd --system --home=/opt/cm/cm-5.11.0/run/cloudera-scm-server/ --no-create-home --shell=/bin/false --comment &#34;Cloudera SCM User&#34; cloudera-scm 准备Parcels，用以安装CDH5 在主节点hadoop001(master)上，将cdh parcels安装包放置到/opt/cloudera/parcel-repo目录下，并修改权限: cp /home/hadoop/softwares/cm5.11.0/CDH-5.11.0-1.cdh5.11.0.p0.34-el7.parcel /opt/cm/cloudera/parcel-repo/ cp /home/hadoop/softwares/cm5.11.0/CDH-5.11.0-1.cdh5.11.0.p0.34-el7.parcel.sha1 /opt/cm/cloudera/parcel-repo/ mv /opt/cm/cloudera/parcel-repo/CDH-5.11.0-1.cdh5.11.0.p0.34-el7.parcel.sha1 /opt/cm/cloudera/parcel-repo/CDH-5.11.0-1.cdh5.11.0.p0.34-el7.parcel.sha 在所有cloudera-agent上创建parcels目录（这一步可以不用做） 启动 CM Server和Agent 启动服务 # 主节点启动(hadoop001): /opt/cm/cm-5.11.0/etc/init.d/cloudera-scm-server start # 所有节点启动(hadoop001,hadoop002,hadoop003): /opt/cm/cm-5.11.0/etc/init.d/cloudera-scm-agent start 添加开机自启动 chmod +x /etc/rc.d/rc.local # vi /etc/rc.d/rc.local # 在此文件中添加启动命令即可实现开机启动 #主节点启动(hadoop001) /opt/cm/cm-5.11.0/etc/init.d/cloudera-scm-server start # 所有节点启动(hadoop001,hadoop002,hadoop003): /opt/cm/cm-5.11.0/etc/init.d/cloudera-scm-agent start 登录: http://hadoop001:7180 CDH部署 选择节点 选择本地Parcel 如果未显示, 在更多选项中修改本地Parcel路径, 然后重启server和agent 服务器检查: 修复警告 sysctl -w vm.swappiness=10 echo &#34;vm.swappiness=10&#34; &gt;&gt;/etc/sysctl.conf # 将下面两句添加到系统启动脚本中如`/etc/rc.local`，以便系统重启生效 echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled 选择服务 服务配置 数据库设置 集群设置 # 解决spark找不到JAVA_HOME问题 mkdir -p /usr/java ln -s /usr/local/jdk目录 /usr/java/default # hive mysql驱动 cp mysql-connector-java-5.1.42-bin.jar /opt/cm/cloudera/parcels/CDH/lib/hive/lib/ # oozie mysql驱动 cp /home/hadoop/lib/mysql-connector-java-5.1.6-bin.jar /usr/share/java/mysql-connector-java.jar 启动服务 Kafka安装 查看CDH集群支持的Kafka版本: https://www.cloudera.com/documentation/enterprise/release-notes/topics/rn_consolidated_pcm.html#pcm_kafka 下载csd包: http://archive.cloudera.com/csds/kafka/ 下载kafka parcel包 wget http://archive.cloudera.com/kafka/parcels/2.1.1/KAFKA-2.1.1-1.2.1.1.p0.18-el7.parcel wget http://archive.cloudera.com/kafka/parcels/2.1.1/KAFKA-2.1.1-1.2.1.1.p0.18-el7.parcel.sha1 wget http://archive.cloudera.com/kafka/parcels/2.1.1/manifest.json 拷贝到parcel-repo: mv KAFKA-2.1.1-1.2.1.1.p0.18-el7.parcel.sha1 KAFKA-2.1.1-1.2.1.1.p0.18-el7.parcel.sha 主界面-&gt;主机-&gt;Parcel-&gt;检查新Parcel-&gt;分配/激活 添加服务 配置 bootstrap.servers: node00:9092,node01:9092,node02:9092 source.bootstrap.servers: node00:9092,node01:9092,node02:9092 whitelist: node00:9092,node01:9092,node02:9092 如开启Kafka MirrorMaker需要配置该项, 否则角色日志中会报Error: whitelist must be specified broker_max_heap_size: 1G mirror_maker_max_heap_size: 1G 修改带var的路径 参考 CDH版本 离线安装 Cloudera Manager 5.11.1 和 CDH5.11.1 完全教程 Cloudera CDH新增节点到集群参考步骤 CDH 5.16 安装 Kafka 集群 操作手册   ]]></content></entry><entry><title>ElasticSearch6.3搭建快速入门</title><url>/post/quickstart-of-elasticsearch63/</url><categories><category>数据科学</category></categories><tags><tag>大数据</tag></tags><content type="html"><![CDATA[  快速入门 Elasticsearch是一个实时分布式搜索和分析引擎 环境 java 1.8+ ubuntu 14.04/16.04 版本说明 5.X之后的字段类型不再支持string，由text或keyword取代 6.3支持sql 基本概念 索引(Indices): 同msql的db 每个索引都有多个分片, 每个分片是一个Lucene索引 分片: 默认每个索引5个分片 副本: 默认每个分片1个副本 类型(Type): 同mysql的table 6.x移除了types https://www.elastic.co/guide/en/elasticsearch/reference/6.0/removal-of-types.html 文档(Documents): 同mysql的rows 一条json串 字段(Fields): 同mysql的columns Elasticsearch为对字段类型进行猜测，动态生成了字段和类型的映射关系 可以预先使用mapping来定义 结点和集群 节点(node)是一个运行着的Elasticsearch实例 集群(cluster)是一组具有相同 cluster.name 的节点集合 多节点安装和部署 至少3个结点保证集群可用, 2个结点容易出现脑裂 安装方式 deb/rpm方式 # ubuntu/debain wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.0.deb dpkg -i elasticsearch-6.3.0.deb zip/tar方式: 推荐 便于同一台机器部署多个结点 同一台机器多节点部署 步骤 下载tar包并解压到/opt/elk/es目录 wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.0.tar.gz tar zxvf elasticsearch-6.3.0.tar.gz -C /opt/elk/es/ 创建elsaticsearch用户和elsaticsearch用户组, 修改文件的用户和用户组 # 创建elasticsearch用户设置密码 adduser elasticsearch # 修改文件所属用户 chown -R elasticsearch:elasticsearch elasticsearch-6.3.0 修改配置 # vi /opt/elk/es/elasticsearch-6.3.0/config/elasticsearch.yml # 集群名称，同一集群，名称要设置相同 cluster.name: my-es # 节点名称 node.name: node-0 # 绑定ip和端口 network.host: 192.168.1.101 http.port: 9200 复制2个结点 cp -pR elasticsearch-6.3.0 elasticsearch-6.3.0-node-1 cp -pR elasticsearch-6.3.0 elasticsearch-6.3.0-node-2 修改配置: node-1的端口为9201， node-2的端口为9202 # vi elasticsearch-6.3.0-node-1/config/elasticsearch.yml http.port: 9201 # vi elasticsearch-6.3.0-node-2/config/elasticsearch.yml http.port: 9202 依次启动3个结点: es必须用非root用户启动 # 切换为elasticsearch用户启动 su -elasticsearch # 后台启动 ./elasticsearch-6.3.0/bin/elasticsearch -d ./elasticsearch-6.3.0-node-1/bin/elasticsearch -d ./elasticsearch-6.3.0-node-2/bin/elasticsearch -d 查看ES是否启动成功: jps -l 启动报错 max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536] # vim /etc/security/limits.conf * soft nofile 65536 * hard nofile 131072 * soft nproc 4096 * hard nproc 8192 #退出用户重新登录，使配置生效 ulimit -Hn max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] # vi /etc/sysctl.conf vm.max_map_count=262144 sysctl -p max number of threads [2048] for user [vagrant] is too low, increase to at least [4096] es 5.x Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x000000008a660000, 1973026816, 0) failed; error=&lsquo;Cannot allocate memory&rsquo; (errno=12) # vi config/jvm.options -Xms512m -Xmx512m 不同机器部署: 两台服务器, 3个结点 每个结点都需要配置discovery.zen.ping.unicast.hosts 结点配置 node-0: 192.168.1.100 # 集群名称，同一集群，名称要设置相同 cluster.name: my-es # 节点名称 node.name: node-0 # 绑定ip和端口 network.host: 192.168.1.100 http.port: 9200 transport.tcp.port: 9300 # 该属性是为了形成一个集群，有主节点资格并互相连接的节点的最小数目 discovery.zen.minimum_master_nodes: 2 #关闭多播，用单播。并指定至少一个能接受单播的主机, 每个节点配置均一致 discovery.zen.ping.multicast.enabled: false discovery.zen.ping.unicast.hosts: [&#34;192.168.1.100:9300&#34;, &#34;192.168.2.100:9300&#34;, &#34;192.168.2.100:9301&#34;] node-1: 192.168.2.100 # 集群名称，同一集群，名称要设置相同 cluster.name: my-es # 节点名称 node.name: node-1 # 绑定ip和端口 network.host: 192.168.2.100 http.port: 9200 transport.tcp.port: 9300 # 该属性是为了形成一个集群，有主节点资格并互相连接的节点的最小数目 discovery.zen.minimum_master_nodes: 2 #关闭多播，用单播。并指定至少一个能接受单播的主机, 每个节点配置均一致 discovery.zen.ping.multicast.enabled: false discovery.zen.ping.unicast.hosts: [&#34;192.168.1.100:9300&#34;, &#34;192.168.2.100:9300&#34;, &#34;192.168.2.100:9301&#34;] node-2: 192.168.2.100 # 集群名称，同一集群，名称要设置相同 cluster.name: my-es # 节点名称 node.name: node-2 # 绑定ip和端口 network.host: 192.168.2.100 http.port: 9201 transport.tcp.port: 9301 # 该属性是为了形成一个集群，有主节点资格并互相连接的节点的最小数目 discovery.zen.minimum_master_nodes: 2 #关闭多播，用单播。并指定至少一个能接受单播的主机, 每个节点配置均一致 discovery.zen.ping.multicast.enabled: false discovery.zen.ping.unicast.hosts: [&#34;192.168.1.100:9300&#34;, &#34;192.168.2.100:9300&#34;, &#34;192.168.2.100:9301&#34;] 参考 Elasticsearch Reference [6.3]   ]]></content></entry><entry><title>数据结构基础之图</title><url>/post/graph-of-datastructure/</url><categories><category>数据结构与算法</category></categories><tags><tag>数据结构</tag></tags><content type="html"><![CDATA[  简介 概念: 由顶点的有穷非空集合和顶点之间边的集合组成, 通常表示为: G(V,E), 其中, G表示一个图, V是图G中顶点的集合, E是图G中边的集合 异同: 线性表中把数据元素叫做元素, 树中将数据元素叫结点, 在图中数据元素叫做顶点(Vertex) 线性表中可以没有数据元素,称为空表; 树中可以没有结点,叫做空树; 图中不允许没有顶点,若V是顶点的集合, 则集合V有穷且非空 线性表中, 相邻的数据元素具有线性关系; 树结构中, 相邻两层的结点具有层次关系; 图中, 任意两个顶点之间都可能有关系, 顶点之间的逻辑关系用边来表示, 边集可以是空 相关定义 无向边: 若顶点vi, vj之间的边没有方向, 则称这条边为无向边, 用无序偶对(vi,vj)来表示 无向图: 如果图中任意两个顶点之间的边都是无向边, 则称该图为无向图 无向完全图: 在无向图中, 如果任意两个顶点之间都存在边, 则称该图为无向完全图 有向边: 若从顶点vi到vj之间的边有方向, 则称这条边为有向边, 也称为弧(Arc), 用有序偶&lt;vi,vj&gt;来表示, vi称为狐尾(Tail), vj称为弧头(Head) 有向图(Directed graphs): 如果图中任意两个顶点之间的边都是有向边, 则称该图为有向图 有向完全图: 在有向图中, 如果任意两个顶点之间都存在方向互为相反的两条弧, 则称该图为有向完全图 简单图: 在图中, 若不存在顶点到其自身的边, 且同一条边不重复出现, 则称这样的图为简单图 权(Weight): 与图的边或弧相关的数 网(NetWork): 带权的图 子图: 假设有两个图G=(V,{E}), G&rsquo;=(V&rsquo;,{E&rsquo;}), 如果V&rsquo;包含于V, 且E&rsquo;包含于E, 则称G&rsquo;为G的子图 图的顶点与边间关系 对于无向图G=(V,{E}), 如果边(v,v&rsquo;)属于E, 则称顶点v和v&rsquo;互为邻接点(Adjacent), 即v与v&rsquo;相邻接。边(v,v&rsquo;)依附于顶点v和v&rsquo;, 或者说(v,v&rsquo;)与顶点v和v&rsquo;相关联。顶点v的度是和v相关联的边的数目, 记为TD(v) $e=\frac{1}{2}\sum_{i=1}^nTD(v_i)$ 对于有向图G=(V,{E}), 如果边&lt;v,v&rsquo;&gt;属于E, 则称顶点v邻接到v&rsquo;, 顶点v&rsquo;邻接自v。弧&lt;v,v&rsquo;&gt;和顶点v, v&rsquo;相关联。以顶点v为头的弧的数目, 称为v的入度(InDegree), 记为ID(v); 以v为尾的弧的数目, 称为v的出度(OutDegree), 记为OD(v); 顶点v的度为TD(v) = ID(v) + OD(v) $e=\sum_{i=1}^nID(v_i)=\sum_{i=1}^nOD(v_i)$ 回路或环(Cycle): 第一个顶点到最后一个顶点相同的路径 简单路径: 序列中顶点不重复出现的路径 简单回路或简单环: 除了第一个顶点和最后一个顶点之外, 其余顶点不重复出现的回路 连通图 连通图: 在无向图G中, 如果从顶点v到顶点v&rsquo;有路径, 则称v和v&rsquo;是连通的。 如果对于图中任意两个顶点vi, vj 属于E, vi和vj都是连通的, 则称G是连通图 连通分量: 无向图中的极大连通子图 要是子图 子图要是连通的 连通子图含有极大顶点数 具有极大顶点数的连通子图包含依附于这些顶点的所有边 强连通图: 在有向图G中, 如果对于每一对vi、vj属于V, vi不等于vj, 从vi到vj和从vj到vi都存在路径, 则称G是强连通图 强连通分量: 有向图中的极大强连通子图 连通图的生成树: 是一个极小的连通子图, 它含有图中全部的n个顶点, 但只有足以构成一棵树的n-1条边 有向树: 如果一个有向图恰有一个顶点的入度为0, 其余顶点的入度为1, 则是一棵有向树 图的抽象数据类型 ADT 图(Graph) Data 顶点的有穷非空集合和边的集合 Operation CreateGraph(*G, V, VR): 按照顶点集V和边弧集VR的定义构造图G DestroyGraph(*G): 图G存在则销毁 LocateVex(G, u): 若图G中存在顶点u, 则返回图中的位置 GetVex(G, v): 返回图G中顶点v的值 PutVex(G, v, value): 将图G中顶点v赋值value FirstAdjVex(G, *v): 返回顶点v的一个邻接顶点, 若顶点在G中无邻接顶点返回空 NextAdjVex(G, v, *w): 返回顶点v相对于顶点w的下一个邻接顶点, 若w是v的最后一个邻接点, 则返回空 InsertVex(*G): 在图G中增添新顶点v DeleteVex(*G, v): 删除图G中顶点v及相关的弧 InsertArc(*G, v, w): 在图G中增填弧&lt;v,w&gt;, 若G是无向图, 还需要添加对称弧&lt;w,v&gt; DFSTraverse(G): 对图G中进行深度优先遍历, 在遍历过程对每个顶点调用 HFSTraverse(G): 对图G中进行广度有限遍历, 在遍历过程对每个顶点调用 endADT 存储结构 邻接矩阵(Adjacency Matrix) 用两个数组来表示图。一个一维数组存储图中顶点信息，一个二维数组(称为邻接矩阵)存储图中的边或弧的信息。设图G有n个顶点, 则邻接矩阵是一个n*n的方阵, 定义为 $$arc[i][j] = \begin{cases} 1, \quad 若(v_i,v_j)\in E或\left\langle v_i,v_j \right\rangle \in E\ 0, \quad 反之 \end{cases} $$ 特性: 无向图的边数是一个对称矩阵 有向图中, 入度为列之和, 出度为行之和 优点: 可以快速查到一个顶点和另一顶点之间的关联关系 缺点: 占用了太多的空间 网图表示 定义: 设图G是网图, 有n个顶点, 则邻接矩阵是一个n*n的方阵, 定义为: $$ arc[i][j] = \begin{cases} W_{ij}, \quad 若(v_i,v_j)\in E或\left\langle v_i,v_j \right \rangle \in E\ 0, \quad 若i=j\ \infty, \quad 反之 \end{cases} $$ 说明: $W_{ij}$表示$(v_i,v_j)$或$&lt;v_i,v_j&gt;$上的权值 $\infty$表示一个计算机允许的、大于所有边上的权值的值，也就是一个不可能的极限值 邻接表(Adjacency List) 定义: 为了解决邻接矩阵占用空间的问题, 采用数组与链表相结合的存储方法称为邻接表 特性: 图的每一个顶点都是一个链表的头节点，其后连接着该顶点能够直接达到的相邻顶点。 逆邻接表 有向图的逆邻接表: 对每个顶点vi都建立一个链接为vi为弧头的表 特性: 解决逆向查找的麻烦 与邻接表是正好相反的。逆邻接表每一个顶点作为链表的头节点，后继节点所存储的是能够直接达到该顶点的相邻顶点 十字链表 将邻接表和逆邻接表结合起来 顶点表结构: data|firstin|firstout firstin: 入边表头指针, 指向该顶点的入边表中第一个结点 firstout: 表示出边表头指针, 指向该顶点的出边表中的第一个结点 边表结点结构: tailvex|headvex|headlink|taillink, 如果是网, 还可以增加一个weight域来存储权值 tailvex: 弧起点在顶点表的下标 headvex: 弧终点在顶点表的下标 headlink: 入边表指针域, 指向终点相同的下一条边 taillink: 指向边表指针域, 指向起点相同的下一条边 邻接多重表 重新定义边表结构: ivex|ilink|jvex|jlink ivex和jvex是与某条边依附的两个顶点在顶点表中的下标 ilink指向依附顶点ivex的下一条边 jlink指向依附顶点jvex的下一条边 边集数组 由两个一维数组构成, 一个存储顶点的信息, 另一个存储边的信息。边数组的每个数据元素由一条边的起点下标(begin)、终点下标(end)和权(weight)组成 图的遍历(Traversing Graph) 从图中某一顶点出发访遍图中其余顶点, 且使每一个顶点仅被访问一次的过程 深度优先遍历(Depth_First_Search) 定义: 从图中某个顶点v出发, 访问此顶点, 然后从v的未访问邻接点出发深度优先遍历图, 直至图中所有和v有路径相通的顶点都被访问到, 若图中尚有顶点未被访问到, 则另选图中一个未曾被访问到的顶点作为起点, 重复上述过程, 直至图中所有顶点都被访问到为止 特征: 类似树的前序遍历 不同存储结构比较: 对于n个顶点e条边的图 邻接矩阵: O(n^2) 邻接表: O(n+e) 广度优先遍历(Breadth_First_Search) 定义: 类似于一个分层搜索的过程，广度优先遍历需要使用一个队列以保持访问过的结点的顺序，以便按这个顺序来访问这些结点的邻接结点 特征: 类似树的层序遍历 异同 同: 时间复杂度一样 异: 对顶点的访问顺序不同 深度优先更适合目标比较明确, 以找到目标为主要目的的情况 广度优先更适合在不断扩大遍历范围时找到相对最优解的情况 最小生成树(Minimum Cost Spanning Tree) 定义: 构造连通网的最小代价生成树 普里姆(Prim)算法 定义: 假设N = (P,{E})是连通网, TE是N上最小生成树中边的集合。算法从U = {u0}(u0属于V),TE={}开始。重复执行下述操作：在所有u属于U, v属于V-U的边(u,v)属于E中找一条代价最小的边(u0,v0)并入集合TE, 同时v0并入U, 直至U=V为止。此时TE中必有n-1条边, T=(V,{TE})为N的最小生成树。 时间复杂度: O(n^2) 证明: 反证法 克鲁斯卡尔(Kruskal)算法 定义: 假设N = (V,{E})是连通网, 则令最小生成树的初始状态为只有n个顶点而无边的非连通图T={V,{}}, 图中每个顶点自成一个连通分量。在E中选择代价最小的边, 若该边依附的顶点落在T中不同的连通分量上, 则将此边加入到T中, 否则舍去此边而选择选择下一条代价最小的边。依次类推, 直至T中所有的顶点都在同一连通分量上为止。 时间复杂度：O(eloge) 证明: 归纳法 使用场景 普里姆算法对于稠密图, 即边数非常多的情况更好些 克鲁斯卡尔算法主要针对边来展开, 边数少时效率会非常高, 对于稀疏图有很大优势 最短路径 非网图: 由于边上没有权值, 最短路径指两顶点之间经过的边数最少的路径 网图: 两顶点之间经过的边上权值之和最少的路径, 称路径上第一个顶点是源点, 最后一个顶点是终点。 迪杰斯特拉(Dijkstra)算法 定义概述: 典型的单源最短路径算法，用于计算一个节点到其他所有节点的最短路径，主要特点是以起始点为中心向外层层扩展，直到扩展到终点为止。 时间复杂度: O(n^2) 弗洛伊德(Floyd)算法 定义概述： 解决任意两点间的最短路径的一种算法，可以正确处理有向图或负权的最短路径问题，同时也被用于计算有向图的传递闭包。 时间复杂度: O(n^3) 空间复杂度: O(n^2) 拓扑排序 无环: 图中没有回路 介绍 AOV网(Activity On Vertex NetWork): 在一个表示工程的有向图中, 用顶点表示活动, 用弧表示活动之间的优先关系, 这样的有向图为顶点表示活动的网。 拓扑序列： 设G=(V,E)是一个具有n个顶点的有向图, V中的顶点序列v1,v2,&hellip;,vn, 满足若从顶点vi到vj有一条路径, 则在顶点序列中顶点vi必须在vj之前。 拓扑排序： 对一个有向图构造拓扑序列的过程 拓扑排序算法 解决问题: 工程能否顺利的进行 基本思路：从AOV网中选择一个入度为0的顶点输出, 然后删去此顶点, 并删除以此顶点为尾的弧, 继续重复此步骤, 直到输出全部顶点或者AOV网中不存在入度为0的顶点为止 时间复杂度: O(n+e) 关键路径 解决问题: 工程完成需要的最短时间 介绍 AOE网: 在一个表示工程的带权有向图中, 用顶点表示事件, 用有向边表示活动, 用边上的权值表示活动的持续时间, 这种有向图的边表示活动的网。 路径长度： 路径上各个活动所持续的时间之和 关键路径： 从源点到汇点具有最大长度的路径 关键活动: 关键路径上的活动 算法描述 准备两个数组 a：最早开始时间数组etv b：最迟开始时间数组。（针对顶点即事件而言） 步骤: 从源点V0出发，令etv[0]（源点）=0，按拓扑有序求其余各顶点的最早发生时间etv[i]（1 ≤ i ≤ n-1）。同时按照上一章拓扑排序的方法检测是否有环存在。
顶点Vk的最早发生时间etv[k]公式: $$ etv[k] = \begin{cases} 0, \quad 当 k = 0 时\ max{etv[i] + len&lt;v_i,v_k&gt;}, \quad 当k\neq0且&lt;v_i,v_j&gt;\in P[k] \end{cases} $$ P[k]表示所有到达顶点Vk的弧的集合 $len&lt;v_i,v_k&gt;为弧&lt;v_i,v_k&gt;上的权值$ 从汇点Vn出发，令ltv[n-1] = etv[n-1]，按拓扑排序求各个其余各顶点的最迟发生时间ltv[i]（n-2 ≥ i ≥ 2）;
顶点Vk的最晚发生时间ltv[k]公式: $$ ltv[k] = \begin{cases} etv[k], \quad 当 k = n-1 时\ min{ltv[j] + len&lt;v_k,v_j&gt;}, \quad 当k \lt n-1且&lt;v_k,v_j&gt;\in S[k] \end{cases} $$ S[k]表示所有从顶点Vk出发的弧的集合 $len&lt;v_k,v_j&gt;为弧&lt;v_k,v_j&gt;上的权值$ 其实是把拓扑序列倒过来进行 根据各顶点的etv和ltv数组的值，求出弧（活动）的最早开工时间和最迟开工时间，求每条弧的最早开工时间和最迟开工时间是否相等，若相等，则是关键活动。
注意： 1，2 完成点（事件）的最早和最迟。 3根据事件来计算活动最早和最迟，从而求的该弧（活动）是否为关键活动 参考 深度优先和广度优先遍历及其 Java 实现   ]]></content></entry><entry><title>数据结构基础之树</title><url>/post/tree-of-datastructure/</url><categories><category>数据结构与算法</category></categories><tags><tag>数据结构</tag></tags><content type="html"><![CDATA[  简介 概念: 为n(n&gt;=0)个结点的有限集。n=0时为空树。 特点: 在任意一棵非空树中 有且仅有一个特定的称为根(Root)的结点 当n&gt;1时, 其余结点可以分为m(m&gt;0)个互不相交的有限集T1、T2、&hellip;&hellip;、Tm, 其中每一个集合本身又是一棵树, 并且称为根的子树(SubTree) 结点分类 根结点: 无双亲, 唯一 叶结点: 无孩子, 可以多个 中间结点: 一个双亲多个孩子 相关概念 度: 结点拥有的子树数, 度为零的结点称为叶结点或终端结点, 度不为零的结点称为非终端结点或分支结点 树的度: 树内各节点的度的最大值 结点的层次: 从根开始定义起, 根为第一层, 根的孩子为第二层 树的深度或高度: 树中结点的最大层次 有序树和无序树: 如果将树中结点的各子树看成从左至右是有次序的, 不能互换的, 则称该树为有序树, 否则称为无序树 森林: m(m&gt;=0)棵互不相交的树的集合, 对树中每个结点而言, 其子树的集合即为森林 抽象数据类型定义 ADT 树(tree) Data 由一个根节点和若干棵子树构成。树中结点具有相同数据类型及层次关系 Operation InitTree(*T): 构造空树 DestroyTree(*T): 销毁树 CreateTree(*T, definition): 按definition中给出树的定义来构造树 ClearTree(*T): 若树T存在, 则将树T清为空树 TreeEmpty(T): 若T为空树, 返回true, 否则返回false TreeDepth(T): 返回T的深度 Root(T): 返回T的根结点 Value(T,cur_e): cur_e是树T中一个结点, 返回此结点的值 Assign(T,cur_e,value): 给树T的结点cur_e赋值为value Parent(T,cur_e): 若cur_e是树T的非根结点, 则返回它的双亲, 否则返回空 LeftChild(T,cur_e): 若cur_e是树T的非叶结点, 则返回它的最左孩子, 否则返回空 RightSibling(T,cur_e): 若cur_e有右兄弟, 则返回它的右兄弟, 否则返回空 InsertChild(*T,*p,i,c): 其中p指向树T的某个结点, i为所指结点p的度加上1, 非空树c与T不相交, 操作结果为插入c为树T中p指结点的第i棵子树 DeleteChild(*T,*p,i): 其中p指向树T的某个结点, i为所指结点p的度, 操作结果为删除T中p所指结点的第i课子树 end ADT 存储结构 双亲表示法 说明: data为数据域, parent为指针域, 根结点的位置域设置为-1 双亲域表示法 下标 data parent 0 A -1 1 B 0 2 C 0 3 D 1 4 E 2 5 F 2 6 G 3 7 H 3 8 I 3 9 J 4 长子域表示法 右兄弟域表示法 孩子表示法 引入: 采用多重链表: 每个结点有多个指针域, 其中每个指针指向一棵树的根节点 存储设计: 方案一: 指针域的个数等于树的度, 对于树中各节点度相差很大时, 比较浪费空间 data为数据域, child1~childd为指针域 方案二: 每个结点指针域的个数等于该结点的度, 专门取一个位置来存储结点指针域的个数 data为指针域, degree为度域, child1~childd为指针域 概念: 把每个结点的孩子结点排列起来, 以单链表做存储结构, 则n个结点有n个孩子链表, 如果是叶子结点则此单链表为空。然后n个头指针又组成一个线性表, 采用顺序存储结构, 存放一个一维数组中 结构: 孩子链表的孩子结点: child为数据域,存储某个结点在表头数组中的下标; next为指针域, 存储指向某结点的下一个孩子结点的指针 表头数组的表头结点: data为数据域,存储某结点的数据信息; firstchild为头指针域,存储该节点的孩子链表的头指针 扩展 双亲孩子表示法 孩子兄弟表示法 data为数据域, firstchild为指针域, 储存该结点的第一个孩子结点的储存地址, rightsib为指针域, 存储该节点的右兄弟结点的存储地址 二叉树(Binary Tree) 概念: 是n(n&gt;=0)个结点的有限集合,该集合或者为空集(称为空二叉树), 或者由一个根节点和两颗互不相交的、分别称为根结点的左子树和右子树的二叉树组成 特点: 每个结点最多有两课子树, 即不存在度大于2的结点, 没有子树或者有一棵子树均可 左子树和右子树是有序的 即使某结点只有一棵子树, 也要区分是左子树还是右子树 基本形态(5种): 空二叉树 只有一个根节点 根节点只有左子树 根节点只有右子树 根节点既有左子树又有右子树 特殊二叉树 斜树: 左斜树和右斜树 每层只有一个结点, 结点数与二叉树的深度相同 满二叉树: 所有分支节点都存在左子树和右子树, 并且所有叶子都在同一层上 完全二叉树: 概念: 对一颗具有n个结点的二叉树按层次编号, 如果编号为i(1&lt;=i&lt;=n)的结点与同样深度的满二叉树中编号为i的结点在二叉树中位置完全相同, 则这课二叉树称为完全二叉树 特点: 叶子结点只能出现在最下两层 最下层的叶子一定集中在左部连续位置 倒数第二层,若有叶子结点,一定都在右部连续位置 如果结点度为1, 则该结点只有左子树, 不存在右子树 同样结点的二叉树,完全二叉树的深度最小 二叉树的性质 性质一: 在二叉树的第i层上至多有2^(i-1)个结点(i&gt;=1) 性质二: 深度为k的二叉树至多有2^k - 1 个结点 性质三: 对任何一棵二叉树T, 如果其终端结点数为n0, 度为2的结点数为n2, 则n0 = n2 + 1 性质四: 具有n个结点的完全二叉树的深度为[log2n] + 1 ([x]表示不大于x的最大整数) 性质五: 对于一棵有n个结点的完全二叉树(其深度为[log2n] + 1)的结点按层序编号(从第1层到第[log2n] + 1层, 每层从左到右), 对任一节点i(1&lt;=i&lt;=n): 如果i=1, 则结点i是二叉树的根, 无双亲; 如果i&gt;1, 则其双亲是结点[i/2] 如果2i&gt;n, 则结点i无左孩子(结点i为叶子结点); 否则其孩子是结点2i 如果2i+1&gt;n, 则结点i无右孩子; 否则其右孩子是结点2i+1 二叉树的存储结构 二叉树顺序存储结构 用一维数组存储二叉树中的结点 一般只用于存储完全二叉树 二叉链表 二叉树每个结点最多有两个孩子, 所以为它设计一个数据域和两个指针域, 我们称这样的链表叫做二叉链表 如有需要可以增加一个指向其双亲的指针域, 称之为三叉链表 遍历二叉树 二叉树额遍历(traversing binary tree): 从根节点出发, 按照某种次序依次访问二叉树所有结点, 使得每个结点被访问一次, 且仅被访问一次 A / \ B C / \ / \ D E F G / / \ H I J \ K 二叉树遍历方法: 把树中的结点变成某种意义的线性序列 前序遍历: 先访问根节点, 然后前序遍历左子树, 再前序遍历右子树 中序遍历: 中序遍历根节点的左子树, 然后访问根节点, 最后中序遍历右子树 后序遍历: 从左到右, 先叶子后结点的方式遍历访问左右子树, 最后访问根节点 层序遍历: 同一层按从左到右的顺序对结点逐个访问 前序遍历递归算法 示例: void PreOrderTraverse(BiTree T) { if(T == NULL) return; printf(&#34;%c&#34;, T-&gt;data); //显示结点数据 PreOrderTraverse(T-&gt;lchild); PreOrderTraverse(T-&gt;rchild); } 遍历顺序: ABDHKECFIGJ 中序遍历递归算法(6.8.4) 示例: void InOrderTraverse(BiTree T) { if(T == NULL) return; InOrderTraverse(T-&gt;lchild); //中序遍历左子树 printf(&#34;%c&#34;, T-&gt;data); //显示结点数据 InOrderTraverse(T-&gt;rchild);	//中序遍历右子树 } 遍历顺序: HKDBEAIFCGJ 后序遍历递归算法 示例: void PostOrderTraverse(BiTree T) { if(T == NULL) return; PostOrderTraverse(T-&gt;lchild); PostOrderTraverse(T-&gt;rchild); printf(&#34;%c&#34;, T-&gt;data); } 遍历顺序: KHDEBIFJGCA 遍历总结 总结: 遍历都是从根节点开始 前序遍历是先打印再递归左右 遍历性质 已知前序遍历序列和中序遍历序列, 可以唯一确定一棵二叉树 已知后序遍历序列和中序遍历序列, 可以唯一确定一棵二叉树 二叉树的建立 利用递归原理, 在原来应该打印结点的地方, 改成生成结点, 给结点赋值操作 线索二叉树(Threaded Binary Tree) 引入: n个节点的二叉树存在n+1个空指针域 定义: 指向前驱和后继的指针称为线索, 加上线索的二叉链表称为线索链表, 相应的二叉树称为线索二叉树 特征: 其实线索二叉树, 等于把一棵二叉树转变成了一个双向链表 线索化: 在遍历的过程中修改空指针的过程 使用场景 如果所用的二叉树需经常遍历或查找结点时需要某种遍历序列中的前驱和后继, 那么可以采用线索二叉链表的存储结构 霍夫曼树(Huffman Tree) 相关概念: 路径长度: 从树中一个结点到另一个结点之间的分支构成两个结点之间的路径, 路径上的分支数目称做路径长度 树的路径长度: 从树根到每一结点的路径长度之和 定义: 假设有n个权值{w1,w2,&hellip;,wn}, 构造一棵有n个叶子结点的二叉树, 每个叶子结点带权wk, 每个叶子的路径长度为lk, 我们通常记作, 其中带权路径长度WPL最小的二叉树称做霍夫曼树 算法描述 根据给定n个权值{w1,w2,&hellip;,wn}构成n课二叉树的集合F={T1,T2,&hellip;,Tn}, 其中每颗二叉树Ti中只有一个带权为wi根节点, 其左右子树均为空 在F中选取两棵根节点的权值最小的树作为左右子树构造一棵新的二叉树, 且置新的二叉树的根节点的权值为其左右子树上根节点的权值之和 在F中删除这两棵树, 同时将新得到的二叉树加入F中 重复2和3步骤, 直到F只含一棵树为止。这棵树便是霍夫曼树 霍夫曼编码 构造霍夫曼树主要运用于编码，称为霍夫曼编码。 定义： 一般地，设需要编码的字符集为{d1,d2,&hellip;,dn}, 各个字符在电文中出现的次数或频率的集合为{w1,w2,&hellip;,wn}, 以d1,d2,&hellip;,dn作为叶子结点, 以w1,w2,&hellip;,wn作为相应叶子结点的权值来构造一棵霍夫曼树。规定霍夫曼树的左分支代表0，右分支代表1，则从根节点到叶子结点所经过的路径分支组成0和1的序列便为该节点对应字符的编码，即为霍夫曼编码。 霍夫曼编码实现过程中运用到了贪心算法。霍夫曼树中权值越大的结点距离根结点越近。采用二叉树可以适当降低编码长度，尤其是在编码长度较长，且权值分布不均匀时，采用霍夫曼编码可以大大缩短编码长度。   ]]></content></entry><entry><title>数据结构之线性表</title><url>/post/linerlist-of-datastructure/</url><categories><category>数据结构与算法</category></categories><tags><tag>数据结构</tag></tags><content type="html"><![CDATA[  简介 概念: 零个或多个数据元素的有限序列 抽象数据类型定义 ADT 线性表List Data 线性表的数据对象集合为{a1,a2,...,an}, 每个元素的类型均为DataType, 除第一个元素a1外,每一个元素有且只有一个直接前驱元素, 除了最后一个元素an外, 每一个元素有且只有直接后驱元素。数据元素之间的关系是一对一的关系 Operation InitList(*L): 初始化操作, 构造一个空的线性表L ListEmpty(L): 若线性表L为空表，则返回TRUE，否则返回FALSE ClearList(*L): 将线性表置空 GetElem(L,i,*e): 初始条件：线性表已存在（1≤i≤ListLenght(L)） 操作结果：用e返回线性表L中第i个数据元素的值 locatElem(L,e): 在线性表中查找与给定值e相等的元素, 如果查找成功, 返回该元素在表中的序号表示成功; 否则, 返回0表示失败 ListInsert(*L,i,e) 初始条件：线性表已存在（1≤i≤ListLenght(L)+1） 操作结果：在线性表L中第i个数据元素之前插入新元素e，L长度加1 ListDelete(*L,i,&amp;e) 初始条件：线性表已存在（1≤i≤ListLenght(L)） 操作结果：删除线性表L中第i个数据元素，用e返回其值，L长度减1 ListLenght(L) 初始条件：线性表已存在 操作结果：返回线性表L数据元素个数 end ADT 存储结构 顺序存储结构 定义: 用一段地址连续的存储单元依次存储线性表的数据元素 存储方式: 一维数组 优缺点 优点: 存储和读取数据, 时间复杂度均为O(1) 缺点: 插入和删除数据, 时间复杂度均为O(n) 链式存储结构 结点: 为了表示每个数据元素ai与其直接后续元素ai+1之间的逻辑关系, 对于数据元素ai来说, 除了存储其本身的信息之外, 还需存储一个指示其直接后继的信息(即直接后续的存储位置)。把存储数据元素信息的域称为数据域，把存储直接后继位置的域称为指针域。指针域中存储的信息称做指针或链。这两部分信息组成数据元素ai的存储映像, 称为结点(Node)。 线性链表的最后一个结点指针为空(NULL或&rsquo;^&lsquo;表示) 定义: n个结点(ai的存储映象)链接成一个链表, 即为线性表(a1,a2,&hellip;,an)的链式存储结构, 因为此链表的每个结点中只包含一个指针域, 所以叫做单链表 头指针: 链表中第一个结点的存储位置 头结点: 为了方便操作,会在单链表的第一个结点前附设一个结点称为头结点 其数据域可以不存储任何信息 若线性表为空表, 则头结点的指针域为&quot;空&quot; 头指针与头结点 头指针: 指向链表第一个结点的指针, 若链表有头结点,则是指向头结点的指针 具有标识作用, 所以常用头指针冠以链表的名字 无论链表受否为空, 头指针均不为空, 头指针是链表的必要元素 头结点: 为了操作的统一和方便而设立, 放在第一个元素的结点之前, 其数据域一般无意义(也可以存放链表的长度) 有了头结点, 对第一个元素结点前插入结点和删除第一结点, 其操作与其他结点的操作就统一了 头结点不一定是链表必须要素 单链表的创建和删除 创建: 头插法 尾插法 删除: 声明一结点p和q, 将第一个结点赋值给p 循环: 将下一结点复制给q 释放p 将q赋值给p 头结点指针域为空 数据操作 读取: 使用while循环, 工作指针后移 插入: 时间复杂度均为O(n) //假设插入元素e的结点为s, 注意两者顺序不能颠倒 s-&gt;next = p-&gt;next; p-&gt;next = s; 删除: 时间复杂度均为O(n) //假设将结点q删除 q = p-&gt;next; p-&gt;next = q-&gt;next; 优缺点 优点: 对于插入和删除余额频繁的操作, 单链表的效率优势就越明显 缺点: 查找的时间复杂度为O(n) 两种存储结构对比 存储分配方式: 顺序存储结构用一段连续的存储单元依次存储线性表的数据元素 单链表采用链式存储结构, 用一组任意的存储单元存放线性表的元素 时间性能: 查找: 顺序存储结构O(1), 单链表O(n) 插入和删除: 顺序存储结构要平均移动表长一半的元素, 时间为O(n) 单链表在查出某位置的指针后, 插入和删除时间仅为O(1) 空间性能: 顺序存储结构需要预分配存储空间, 太大浪费, 太小易发生上溢 单链表不需要预分配, 只要有就可以分配, 元素个数也不受限制 其他链式存储结构 静态链表 定义: 用数组描述的链表 循环链表 定义: 将单链表中终端结点的指针端由空指针改为指向头结点, 就使整个单链表形成一个环, 这种头尾相接的单链表称为单循环连链表, 简称循环链表 遍历: 条件为p-&gt;next不等于头结点, 则循环未结束 尾指针: 方便合并操作 双向链表: 使用空间换取时间 定义: 在单链表的每个结点中, 再设置一个指向其前驱结点的指针域 插入 //假设存储元素e的结点为s s-&gt;prior = p; s-&gt;next = p-&gt;next; p-&gt;next-&gt;prior = s; p-&gt;next = s; * 删除 //假设要删除结点p p-&gt;prior-&gt;next = p-&gt;next; p-&gt;next-&gt;prior = p-&gt;prior; 特殊的线性表: 栈和队列 限定了线性表的插入和删除位置 栈 定义: 限定仅在表尾进行插入和删除操作的线性表, 允许插入和删除的称为栈顶(即表尾), 另一端为栈底 特点: 后进先出(LIFO) 抽象数据结构 ADT 栈(stack) Data 同线性表, 元素具有相同的类型, 相邻元素具有前驱和后驱关系 Operation InitStack(*S): 初始化操作, 建立一个空栈S DestroyStack(*S): 若栈存在, 则销毁它 ClearStack(*S): 将栈清空 StackEmpty(S): 若栈为空, 则返回true, 否则返回false GetTop(S,*e): 若栈存在且非空, 用e返回S的栈顶元素 Push(*S,e): 若栈S存在, 插入新元素e到栈S中并成为栈顶元素 Pop(*S,e): 删除栈S中栈顶的元素, 并用e返回其值 StackLength(S): 返回栈S的元素个数 end ADT 顺序存储结构(顺序栈)及实现 数组实现: 下标为0作为栈底 定义top变量指示栈顶元素在数组中的位置, 存在一个元素时为0, 空栈为-1 进栈和出栈操作 进栈(push) //todo判断是否栈满 S-&gt;top ++; S-&gt;data[S-&gt;top] = e; 出栈(pop) //todo判断是否空栈 *e = S-&gt;data[S-&gt;top]; S-&gt;top --; 两栈共享空间 数组实现: 一个栈的栈底为数组的始端, 另一个栈为栈的末端 栈满: top1 + 1 = top2 说明: 通常这样的数据结构是两个栈的控件需求有相反关系 链式存储结构(链栈)及实现 结构: 将栈顶放在单链表的头部, 通常对于链栈来说, 是不需要头结点的 对于链栈来说, 基本不存在栈满的情况(除非系统的内存不够) 实现: 空栈, top=NULL 进栈和出栈操作 进栈 //假设元素值为e的新结点为s, top为栈顶指针 s-&gt;data = e; s-&gt;next = S-&gt;top; S-&gt;top = s; S-&gt;count ++; 出栈 //假设p存储要删除的栈顶结点 //todo判断是否为空栈 *e = S-&gt;top-&gt;data; p = S-&gt;top; S-&gt;top = S-&gt;top-&gt;next; free(p); S-&gt;count --; 使用场景 如果栈的使用过程中元素变化不可预测, 有时很小, 有时非常大, 最好使用链栈; 反之, 如果他的变化在可控范围内, 建议使用顺序栈 递归: 编译器使用栈实现递归 定义: 直接或间接的调用自己的函数 条件: 至少有一个条件, 满足时递归不再进行, 即不再引用自身而是返回值退出 与迭代的区别: 迭代使用的是循环结构, 递归使用的是选择结构 大量的递归调调用会建立函数副本, 会耗费大量的世间和内存 应用: 斐波拉契数列(Fibonacci) 四则表达式求值 括号: 遇左括号进栈, 遇到右括号, 让栈顶的左括号出栈 步骤: 将中缀表达式转化为后缀表达式(栈用来进出运算的符号) 将后缀表达式进行运算得出结果(栈用来进出运算的数字) 队列 定义: 只允许在一端进行插入操作(队尾), 而在另一端进行删除操作(队头)的线性表; 特点: 先进先出(FIFO) 抽象数据结构 ADT 队列(Queue) Data 同线性表, 元素具有相同的类型, 相邻元素具有前驱和后继关系 Operation InitQueue(*Q): 初始化操作, 建立一个空队列Q DestroyQueue(*Q): 若队列存在, 则销毁它 ClearStack(*Q): 将队列清空 QueueEmpty(Q): 若队列为空, 则返回true, 否则返回false GetHead(Q,*e): 若队列存在且非空, 用e返回队列Q的队头元素 EnQueue(*Q,e): 若队列存在, 插入新元素e到队列Q中并成为队尾元素 DeQueue(*Q,e): 删除队列中队头的元素, 并用e返回其值 QueueLength(Q): 返回队列Q的元素个数 end ADT 队列顺序存储: P113 引入两个指针: front指向队头, rear指向队尾的下一个位置, 两者相等为空队列 缺陷: 产生&rsquo;假溢出&rsquo;, 为了避免数组的插入和删除需要移动数据, 引入循环队列 循环队列: P114 定义: 头尾相接的顺序存储结构 队列满的条件: 设队列的最大尺寸为QueueSize, 队列满时保留一个空位 (rear + 1) % QueueSize = front 队列长度计算: (rear - front + QueueSize) % QueueSize 队列链式存储(链队列): P117 线性表中的单链表, 只能尾进头出 头指针指向链队列的头结点, 队尾指向终端结点 空队列front和rear都指向头结点 使用场景 确定队列长度最大值的情况下, 使用循环队列 无法预估队列的长度时, 使用链队列   ]]></content></entry><entry><title>Pyhton中的时间模块</title><url>/post/datetime-module-of-python/</url><categories><category>编程语言</category></categories><tags><tag>Python</tag></tags><content type="html"><![CDATA[  获取当前日期和时间 使用time模块 import time print(time.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;, time.localtime(time.time()))) # Out: 2016-12-08 17:14:29 使用datetime模块 from datetime import datetime from datetime import date today = date.today() print(str(today.year) + &#39;-&#39; + str(today.month) + &#39;-&#39; + str(today.day)) # Out: 2016-12-8 print(today.strftime(&#39;%Y-%m-%d&#39;)) # Out: 2016-12-08 now = datetime.now() print(now.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)) # Out: 2016-12-08 17:14:29 参数说明 strftime(format[, tuple]) -&gt; string 将指定的struct_time(默认为当前时间)，根据指定的格式化字符串输出 python中时间日期格式化符号： 符号 含义 %y 两位数的年份表示（00-99） %Y 四位数的年份表示（000-9999） %m 月份（01-12） %d 月内中的一天（0-31） %H 24小时制小时数（0-23） %I 12小时制小时数（01-12） %M 分钟数（00=59） %S 秒（00-59） %f 微秒 （0,999999） %a 本地简化星期名称 %A 本地完整星期名称 %b 本地简化的月份名称 %B 本地完整的月份名称 %c 本地相应的日期表示和时间表示 %j 年内的一天（001-366） %p 本地A.M.或P.M.的等价符 %U 一年中的星期数（00-53）星期天为星期的开始 %w 星期（0-6），星期天为星期的开始 %W 一年中的星期数（00-53）星期一为星期的开始 %x 本地相应的日期表示 %X 本地相应的时间表示 %Z 当前时区的名称 %% %号本身 时间格式转换 使用time模块 import time # f代表毫秒 struct_time = time.strptime(&#39;161208 17:14:29&#39;, &#39;%y%m%d %H:%M:%S&#39;) # struct_time 为一个时间元组对象 print(time.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;, struct_time)) # Out: &#39;2016-12-08 17:14:29&#39; 使用datetime模块 datetime.strftime(): str_foramt_time接受日期/时间，输出为字符串，即把时间对象格式化为字符串。 datetime.strptime(): str_parse_time接受字符串，输出日期/时间，即把字符串转化为日期/时间。 from datetime import datetime dt = datetime.strptime(&#39;161201 16:14:22&#39;, &#39;%y%m%d %H:%M:%S&#39;) # dt 为 datetime.datetime对象 # 通过调用 timetuple()方法将datetime.datetime对象转化为时间元组 print(dt.strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)) # Out: &#39;2016-12-08 17:14:29&#39; 计算时间间隔 使用datetime模块 datetime.date为日期对象，最小精度为天 datetime.datetime为日期时间对象，最小精度为妙（毫秒也可以） 计算2016-12-1和1990-1-1中间天数和秒数 import datetime # 将字符串转换为datetime.date对象 dt_a = datetime.datetime.strptime(&#39;2016-12-1&#39;, &#39;%Y-%m-%d&#39;) # datetime.date 无 strptime 方法 dt_b = datetime.datetime.strptime(&#39;1990-1-1&#39;, &#39;%Y-%m-%d&#39;) # 虽然格式化时间的时候%m为两位数月份 # 但是反格式化时一位月份也可工作 dt = dt_a - dt_b dt.days # 为 间隔天数 dt.total_seconds() # 为 间隔秒数 dt = datetime.date(2016,12,1) - datetime.date(1990,1,1) dt.days # 间隔天数 计算两个时间戳间隔时间 from datetime import datetime timestamp_a, timestamp_b = 1480582517, 1480472517 dt = datetime.fromtimestamp(timestamp_a) - datetime.fromtimestamp(timestamp_b) print(dt.days, dt.total_seconds()) # Out: 1 110000.0 获取前N天，或者后N天时间 datetime.timedelta([days[, seconds[, microseconds[, milliseconds[, minutes[, hours[, weeks]]]]]]])
计算昨天和明天的日期示例
import datetime #计算昨天和明天的日期, 按格式化输出 today = datetime.date.today() yesterday = today - datetime.timedelta(days=1) tomorrow = today + datetime.timedelta(days=1) print(&#39;today: &#39;, today.strftime(&#39;%Y%m%d&#39;)) # today: 20161208 print(&#39;yesterday: &#39;, yesterday) # yesterday: 2016-12-07 print(&#39;tomorrow: &#39;, tomorrow.strftime(&#39;%y%m%d&#39;)) # tomorrow: 161209 时间戳与日期互转 时间戳计算 时间戳是指格林威治时间1970年01月01日00时00分00秒(北京时间1970年01月01日08时00分00秒)起至现在的总秒数 通过时间戳获取信息 ts = int(time.time()) # ts = 1480583196 print(ts) / 86400 % 7 + 3 # 获取时间戳对应的星期 0 周日 - 6 周六 # Out: 3 # 周四 # =============== timestamp_a, timestamp_b = 1480582517, 1480472517 ts = timestamp_a - timestamp_b print(ts / 86400) # 获取两个时间戳相隔天数 # =============== # 获取当天00:00:00的时间戳 ts = int(time.time()) print(ts / 86400 * 86400) # Out: 1480550400 # =============== # 获取当天23:59:59的时间戳 ts = int(time.time()) print(ts / 86400 * 86400 + 86400 - 1) # Out: 1480636799 日期转时间戳 # python from datetime import datetime, date import time # 将今天日期转为时间戳 today = date.today() ts = int(time.mktime(today.timetuple())) print(ts) # 将指定日期转为时间戳 date_str = &#39;2016-10-17&#39; dt = datetime.strptime(date_str, &#39;%Y-%m-%d&#39;) ts = int(time.mktime(dt.timetuple())) print(ts) date_str = &#39;2016-10-17 10:00:00&#39; dt = datetime.strptime(date_str, &#39;%Y-%m-%d %H:%M:%S&#39;) ts = int(time.mktime(dt.timetuple())) print(ts) 时间戳转日期 python from datetime import datetime datetime.fromtimestamp(float(&#39;1480582517.123&#39;)).strftime(&#39;%Y-%m-%d %H:%M:%S %f&#39;)   ]]></content></entry><entry><title>Android中的WebView使用总结</title><url>/post/summary-of-android-webview/</url><categories><category>APP</category></categories><tags><tag>Android</tag></tags><content type="html"><![CDATA[  用途 加载网页 说明:默认会打开自带的浏览器,使用WebView去加载,则需复写shouldOverrideUrlLoading方法,并返回为true mWebView.setWebViewClient(new WebViewClient(){ @Override public boolean shouldOverrideUrlLoading(WebView view, String url) { view.loadUrl(url); //返回值是true的时候控制去WebView打开，为false调用系统浏览器或第三方浏览器 return true; } }); return值的说明: true: 在打开新的url时WebView就不会再加载这个url了，所有处理都需要在WebView中操作，包含加载 false(默认): 系统就认为上层没有做处理，接下来还是会继续加载这个url的 如果我们拦截了某个url，那么return false 和 return true区别不大，所以一般建议 return false 302问题 通过判断HitTestResult类型, 并return false; WebView webView = (WebView) findViewById(R.id.webview); webView.setWebViewClient(new WebViewClient() { @Override public boolean shouldOverrideUrlLoading(WebView view, String url) { HitTestResult hit = webView.getHitTestResult(); int hitType = hit.getType(); if (hitType == HitTestResult.SRC_ANCHOR_TYPE) {//点击超链接 //这里执行自定义的操作 return true;//返回true浏览器不再执行默认的操作 }else if(hitType == 0){//重定向时hitType为0 return false;//不捕获302重定向 }else{ return false; } } }); 加载本地的html布局 步骤: 将静态的html放在assets目录(与res目录同级)下 调用mWebView.loadUrl(&quot;file:///android_asset/neterror.html&quot;); 获取点击跳转的url地址 自定义WebViewClient,复写shouldOverrideUrlLoading方法,返回为true,用于截获url地址,可以开启新的Activity页面 public boolean shouldOverrideUrlLoading(WebView view, String url) { return false; } 获取标题 自定义WebChromeClient,复写onReceivedTitle 清除之前的view的状态, 释放页面的资源(包括JS), webView.loadUrl(&quot;about:blank&quot;); 播放视频 5.0以下,setWebChromeClient中需添加onShowCustomView 开启硬件加速 android:hardwareAccelerated=&quot;true&quot; 支持插件 webView.getSettings().setPluginState(WebSettings.PluginState.ON); webView.getSettings().setUseWideViewPort(true); 兼容混合模式(https和http) if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.LOLLIPOP) { webView.getSettings().setMixedContentMode(WebSettings.MIXED_CONTENT_ALWAYS_ALLOW); } 全屏 与JS的交互 Android调用JS 调用JS中的方法 调用形式:mWebView.loadUrl(&quot;javascript:callJavaScriptMethod()&quot;); 说明:callJavaScriptMethod()为JS中的方法 调用JS中的alert,confirm和prompt 调用方式: 调用setWebChromeClient方法,复写onJsAlert,onJsConfirm和onJsPrompt方法 JS调用Android 调用形式:&lt;a onClick=&quot;window.obj.invokeAndroid()&quot;&gt; 说明: obj为android中指定的调用名称; 添加mWebView.addJavascriptInterface(new MyJavaScriptInterface(), &quot;obj&quot;); MyJavaScriptInterface为一个类,该类中有invokeAndroid()的方法,且该方法必须为public 双向交互 思路: 将前面的两种方式进行组合 方式： JS 先调用Android，Android再调用JS中的方法 ： 在Android的MyJavaScriptInterface类中调用mWebView.loadUrl(&quot;javascript:wave()&quot;); Android先调用JS，JS再反过来调用Android中MyJavaScriptInterface类的方法 目的：通过JS向Android的方法中传递参数，将该参数保存带全局变量 mWebView.loadUrl(&quot;javascript:window.obj.get&quot;); 使用小技巧 使用物理按键回退上一个网页(如果存在) 复写onKeyDown方法 @Override public boolean onKeyDown(int keyCode, KeyEvent event) { if(keyCode==KeyEvent.KEYCODE_BACK { if(webView.canGoBack()){ webView.goBack();//返回上一页面 return true; } else{ System.exit(0);//退出程序 } } return super.onKeyDown(keyCode, event); } 访问https网站 如果出现某些https的网站无法访问,首先需要添加支持JS WebSettings webSettings = mWebView.getSettings(); webSettings.setJavaScriptEnabled(true); 在2.2及以上系统中处理只需要重载WebViewClient 的 onReceivedSslError即可。 webview.setWebViewClient(new WebViewClient() { @Override public void onReceivedSslError(WebView view, SslErrorHandler handler, SslError error) //handler.cancel(); 默认的处理方式，WebView变成空白页 //handler.process();接受证书 //handleMessage(Message msg); 其他处理 } }); 与SwiperefreshLayout 结合使用 可以调用WebView的reload方法,再次加载页面 通用配置 WebSettings webSettings = mWebView.getSettings(); webSettings.setJavaScriptEnabled(true); webSettings.setJavaScriptCanOpenWindowsAutomatically(true); webSettings.setSupportZoom(false); webSettings.setBuiltInZoomControls(false); webSettings.setAllowFileAccess(true); webSettings.setDatabaseEnabled(true); webSettings.setDomStorageEnabled(true); webSettings.setGeolocationEnabled(true); webSettings.setAppCacheEnabled(true); webSettings.setAppCachePath(getApplicationContext().getCacheDir().getPath()); webSettings.setDefaultTextEncodingName(&#34;UTF-8&#34;); //屏幕自适应 webSettings.setUseWideViewPort(true); webSettings.setLoadWithOverviewMode(true); if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.KITKAT) { webSettings.setCacheMode(WebSettings.LOAD_CACHE_ELSE_NETWORK); } else { webSettings.setCacheMode(WebSettings.LOAD_DEFAULT); } if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.HONEYCOMB) { webSettings.setDisplayZoomControls(false); } if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.KITKAT) { webSettings.setLoadsImagesAutomatically(true); } else { webSettings.setLoadsImagesAutomatically(false); } mWebView.setScrollBarStyle(WDWebView.SCROLLBARS_INSIDE_OVERLAY); mWebView.setHorizontalScrollBarEnabled(false); mWebView.setHorizontalFadingEdgeEnabled(false); mWebView.setVerticalFadingEdgeEnabled(false); 注意事项: 播放视频: 声音消失: 调用pauseTimers(), 需要注意在onResume调用resumeTimers(), 而且同一个进程中的WebView都会收到影响, 需要在onResume中 调用resumeTimers(), 不然会卡住 关于refer 5.0以下手动设置会失效(官方bug) 关于select标签 WebView的初始化不能传入Application的context, 不然调不起来,或者crash Cookie管理 与原生cookie管理 CookieSyncManager.createInstance(mContext); CookieManager cookieManager = CookieManager.getInstance(); cookieManager.setAcceptCookie(true); String[] cookievalues = cookies.split(&#34;;&#34;); for (String value : cookievalues){ cookieManager.setCookie(url,value); } if (Build.VERSION.SDK_INT &lt; 21) { CookieSyncManager.getInstance().sync(); } else { CookieManager.getInstance().flush(); } 文件上传 WebChromeClient中提供了选择文件的方法, 但每个版本的内核的方法不同, 需要进行兼容处理 兼容处理 public ValueCallback&lt;Uri&gt; mUploadMessage; public ValueCallback&lt;Uri[]&gt; mUploadMessageForAndroid5; private final int OPEN_GALLERY_CODE = 110; private final int OPEN_GALLERY_CODE_ANDROID5 = 111; //For Android 3.0+ public void openFileChooser(ValueCallback&lt;Uri&gt; uploadMsg, String acceptType) { openChooserImpl(uploadMsg); } // For Android &gt; 4.1.1 public void openFileChooser(ValueCallback&lt;Uri&gt; uploadMsg, String acceptType, String capture) { openChooserImpl(uploadMsg); } //For Android 5.0+ @Override public boolean onShowFileChooser(WebView webView, ValueCallback&lt;Uri[]&gt; filePathCallback, FileChooserParams fileChooserParams) { openFileChooserImplForAndroid5(filePathCallback); return true; } //3.0+ public void openChooserImpl(ValueCallback&lt;Uri&gt; uploadMsg) { mUploadMessage = uploadMsg; Intent intentImage = new Intent(Intent.ACTION_PICK); intentImage.setDataAndType(MediaStore.Images.Media.INTERNAL_CONTENT_URI, &#34;image/*&#34;); mActivity.startActivityForResult(intentImage, OPEN_GALLERY_CODE); } //5.0+ public void openFileChooserImplForAndroid5(ValueCallback&lt;Uri[]&gt; uploadMsg) { mUploadMessageForAndroid5 = uploadMsg; Intent intentImage = new Intent(Intent.ACTION_PICK); intentImage.setDataAndType(MediaStore.Images.Media.INTERNAL_CONTENT_URI, &#34;image/*&#34;); mActivity.startActivityForResult(intentImage, OPEN_GALLERY_CODE_ANDROID5); } 注意: openFileChooser为hide的方法, 打包混淆需要在proguard-rules.pro中keep -keepclassmembers class com.hinabian.migrate.activity.AtWebView$MyChromeViewClient{ *; } 参考 Android Developer WebView详解与简单实现Android与H5互调 Webview利用CookieSyncManager获取或设置Cookies的策略   ]]></content></entry><entry><title>Java中的常见7种排序算法</title><url>/post/seven-common-sort-algorithm-of-java/</url><categories><category>数据结构与算法</category></categories><tags><tag>Java</tag></tags><content type="html"><![CDATA[  简介 定义: 假设有n个记录的序列为{r1,r2,&amp;hellip;,rn}, 其相应的关键字分别为{k1,k2,&amp;hellip;,kn}, 需确定1,2,&amp;hellip;,n的一种排列p1,p2,&amp;hellip;,pn, 使其相应的关键字满足k_p1 &amp;lt;= k_p2 &amp;lt;= &amp;hellip; &amp;lt;= k_pn (非递减或非递增)关系, 即使得序列成为一个按关键字 有序的序列{r_p1,r_p2,&amp;hellip;,r_pn}, 这样的操作就称为排序 说明: 排序可以看成是线性表的一种操作 排序的依据是关键字之间的大小关系 组合排序可以将主关键字与次关键字拼成字符串, 转化成单关键字排序 排序的稳定性 假设ki=kj (1&amp;lt;=i&amp;lt;=n, 1&amp;lt;=j&amp;lt;=n, i&amp;lt;&amp;gt;j), 且在排序前的序列中ri领先与rj (i&amp;lt;j)。如果排序后ri仍领先rj, 则称所用的排序方法是稳定的; 反之, 若可能使得排序后rj领先ri, 则称所用排序方法是不稳定的。 内排序与外排序 内排序 概述: 在排序的整个过程中, 待排序的所有记录全部被放置在内存中 性能影响: 时间性能: 比较和移动操作尽量少 辅助空间 算法的复杂性: 算法本身的复杂度, 非算法的时间复杂度 分类: 插入排序: 直接插入排序和希尔排序 交换排序: 冒泡排序和快速排序 选择排序: 简单选择排序和堆排序 归并排序: 归并排序 外排序 概述: 由于排序的记录个数太多, 不能同时放置在内存, 整个排序过程需要在内外存之间多次交换数据才能进行 七种排序算法 简单算法: 冒泡排序, 简单选择排序, 直接插入排序 改进算法: 希尔排序, 堆排序, 归并排序和快速排序 冒泡排序(Bubble Sort) 基本思想: 两两比较相邻记录的关键字, 如果反序则交换, 知道没有反序的记录为止 优化: 增加标记变量flag初始值为true, 进入第一层循环后为false, 第二层循环若有数据交换则为false, 判断第一层循环为false则退出 复杂度: 时间复杂度: O(n^2) 最好: O(n) 最坏: O(n^2) 空间复杂度: O(1) 特点: 稳定 实现 public class BubbleSort { /** * 普通冒泡排序 * @param arr * @return */ …  ]]></content></entry><entry><title>Java基础之网络编程</title><url>/post/network-of-java/</url><categories><category>编程语言</category></categories><tags><tag>Java</tag></tags><content type="html"><![CDATA[  网络编程概述 网络编程的本质是两个设备之间的数据交换（把一个设备中的数据发送给两外一个设备，然后接受另外一个设备反馈的数据）。
网络模型 OSI参考模型 TCP/IP 参考模型 网络通讯要素 概述 包括IP地址、端口号和传输协议 Socket 简介 Socket就是为网络服务提供的一种机制。 通信的两端都有Socket。 网络通信其实就是Socket间的通信。 数据在两个Socket间通过IO传输。 常用方法： Socket(InetAddress address, int port) 创建一个流套接字并将其连接到指定 IP 地址的指定端口号。 InputStream getInputStream() 返回此套接字的输入流。 OutputStream getOutputStream() 返回此套接字的输出流。 void shutdownInput() 此套接字的输入流置于“流的末尾”。 void shutdownOutput() 禁用此套接字的输出流。用于客户端告诉服务端，数据发送完毕，让服务端停止读取。 IP地址 网络中设备的标识，不易记忆，可用主机名 本地回环地址：127.0.0.1 主机名：localhost InetAddress 概述： 位于java.net包，表示互联网协议 (IP) 地址的类，构造方法私有，不能直接创建对象。 常见方法： static InetAddress getLocalHost() 返回本地主机。 static InetAddress getByName(String host) 在给定主机名的情况下确定主机的 IP 地址。 static InetAddress[] getAllByName(String host) 在给定主机名的情况下，根据系统上配置的名称服务返回其 IP 地址所组成的数组。 String getHostAddress() 返回 IP 地址字符串（以文本表现形式）。 String getHostName() 获取此 IP 地址的主机名。 端口号 用于标识进程的逻辑地址，不同进程的标识 有效端口：0~65535，其中 0~1024 系统使用或保留端口。 所谓防火墙，其功能就是将发送到某程序端口的数据屏蔽掉以及将从该程序端口发出的数据也屏蔽掉。 传输协议：通讯的规则 常见协议：UDP、TCP。 UDP 将数据及源和目的封装成数据包中，不需要建立连接。 每个数据报的大小在限制在64k内。 因无连接，是不可靠协议。 不需要建立连接，速度快。 应用案例：DNS（域名解析，最先走是本地的hosts（C:\WINDOWS\system32\drivers\etc\hosts）文件，解析失败了，才去访问DNS服务器解析、获取IP地址）、SNMP、QQ、FeiQ聊天、在线视频语音等。 TCP 建立连接，形成传输数据的通道。 在连接中进行大数据量传输。 通过三次握手完成连接，是可靠协议。 必须建立连接，效率会稍低。 应用案例：FTP（File Transfer Protocol）、Telnet、SMTP、HTTP、POP3 等。 UDP协议: 发送端&amp;接收端 概述：DatagramSocket（用来发送和接收数据报包的套接字）与DatagramPacket（数据报包）。
DatagramSocket中的方法：
DatagramSocket(int port) 创建数据报套接字并将其绑定到本地主机上的指定端口。 void send(DatagramPacket p) 从此套接字发送数据报包。 void receive(DatagramPacket p) 从此套接字接收数据报包。 DatagramPacket方法：
DatagramPacket(byte[] buf, int length, InetAddress address, int port) 构造数据报包，用来将长度为 length 的包发送到指定主机上的指定端口号。 DatagramPacket(byte[] buf, int length) 构造 DatagramPacket，用来接收长度为 length 的数据包。 InetAddress getAddress() 返回某台机器的 IP 地址，此数据报将要发往该机器或者是从该机器接收到的。 byte[] getData() 返回数据缓冲区。 int getLength() 返回将要发送或接收到的数据的长度。 UDP发送端：
使用DatagramSocket对象，建立udp socket服务 DatagramSocket ds = new DatagramSocket(6666); 确定数据，并封装成数据包。DatagramPacket(byte[] buf, int length, InetAddress address, int port); 通过DatagramSocket服务的void send(DatagramPacket p); 方法，将数据包发出去 关闭资源 代码示例： class UdpSend { public static void main(String[] args) throws Exception{ //1.创建udp服务。通过DatagramSocket对象。 DatagramSocket ds = new DatagramSocket(6666);//指定端口6666 //2.确定数据，并封装成数据包。DatagramPacket(byte[] buf, int length, InetAddress address, int port) byte[] buf =&#34;udp come test&#34;.getBytes(); DatagramPacket dp = new DatagramPacket(buf,buf.length,InetAddress.getByName(&#34;192.168.0.100&#34;),10000); //3.通过socket服务，将已有数据包发送出去。通过send方法。 ds.send(dp); //4.关闭资源。 ds.close(); } } UDP接收端：
定义udp socket 服务。通常会监听一个端口。其实就是给这个接收网络应用程序定义数字标识。方便于明确哪些数据接收。 定义一个数据包，因为要存储接收到的字节数据。因为数据包对象中有更多功能可以提取字节数据中的不同数据信息。 通过socket服务的receive方法将收到的数据存入已定义好的数据包中。 通过数据包对象的特有功能，将这些不同的数据取出。打印在控制台上。 关闭资源。 代码示例： class UdpRece { public static void main(String[] args) throws Exception{ //1.创建udp socket服务，建立端点。 DatagramSocket ds = new DatagramSocket(10000); //2.定义数据包。用于存储数据。 byte[] buf = new byte[1024]; DatagramPacket dp = new DatagramPacket(buf,buf.length); //3.通过服务的receive方法将接收到数据存入数据包中。 ds.receive(dp);//阻塞式方法。 //4.通过数据包的方法获取其中的数据。 String ip = dp.getAddress().getHostAddress(); String data = new String(dp.getData(),0,dp.getLength()); int port = dp.getPort(); System.out.println(ip+&#34;::&#34;+data+&#34;::&#34;+port); //5.关闭资源 ds.close(); } } 聊天程序示例：
/* 需求：编写一个聊天程序。 分析：有收数据的部分和发数据的部分这两部分需要同时执行； 就需要用到多线程技术。一个线程控制接收，一个线程控制发。 因为收和发动作是不一致的，所以要定义两个run方法。 而且这两个方法要封装到不同的类中。 */ import java.io.*; import java.net.*; //定义发送端类，实现Runnable接口，并覆盖run方法 class Send implements Runnable { private DatagramSocket ds; public Send(DatagramSocket ds) { this.ds = ds; } public void run() { try { //键盘录入 BufferedReader bufr = new BufferedReader(new InputStreamReader(System.in)); String line = null; while ((line=bufr.readLine())!=null) { byte[] buf = line.getBytes();//需将字符串转成字节数组，便于发送 DatagramPacket dp = new DatagramPacket(buf,buf.length,InetAddress.getByName(&#34;192.168.0.255&#34;),10002);//0为网络段，255为发送广播 ds.send(dp); if(&#34;886&#34;.equals(line)) break; } } catch (Exception e) { throw new RuntimeException(&#34;发送端失败&#34;); } } } //定义接收端类，实现Runnable接口，并覆盖run方法 class Rece implements Runnable{ private DatagramSocket ds; public Rece(DatagramSocket ds) { this.ds = ds; } public void run() { try { while (true) { byte[] buf = new byte[1024]; DatagramPacket dp = new DatagramPacket(buf,buf.length); ds.receive(dp); String ip = dp.getAddress().getHostAddress(); String data = new String(dp.getData(),0,dp.getLength()); System.out.println(ip+&#34;::&#34;+data); if(data.equals(&#34;886&#34;)) System.out.println(ip + &#34;...退出聊天室&#34;); } } catch (Exception e) { throw new RuntimeException(&#34;接收端失败&#34;); } } } class ChatDemo { public static void main(String[] args) throws Exception{ DatagramSocket sendScoket = new DatagramSocket(); DatagramSocket receScoket = new DatagramSocket(10002); new Thread(new Send(sendScoket)).start(); new Thread(new Rece(receScoket)).start(); } } TCP协议: 客户端&amp;服务端 概述：客户端（Client）对应的对象是Socket，服务端（Server）对应的对象是ServerSocket。客户端首先与服务端建立连接，形成通道（其实就是IO流），然后，数据就可以在通道之间进行传输，并且单个Server端可以同时与多个Client端建立连接。 建立连接后，通过Socket中的IO流进行数据的传输。关闭socket。同样，客户端与服务器端是两个独立的应用程序。
ServerSocket方法：
ServerSocket(int port) 创建绑定到特定端口的服务器套接字。 Socket accept() 侦听并接受到此套接字的连接。 TCP客户端
客户端需要明确服务器的ip地址以及端口，这样才可以去试着建立连接，如果连接失败，会出现异常。连接成功，说明客户端与服务端建立了通道，那么通过IO流就可以进行数据的传输，而Socket对象已经提供了输入流和输出流对象，通过getInputStream( )和getOutputStream( )获取即可。与服务端通讯结束后，关闭Socket。
代码示例：
class TcpClient { public static void main(String[] args) throws Exception{ //创建客户端的Socket服务。指定目的主机端口 Socket s = new Socket(&#34;192.168.0.100&#34;,10003); //为了发送数据，应该获取Socket流中的输出流。 OutputStream out = s.getOutputStream(); out.write(&#34;tcp come test&#34;.getBytes()); s.close(); } } TCP服务端
需要明确它要处理的数据是从哪个端口进入的。当有客户端访问时，要明确是哪个客户端，可通过accept()获取已连接的客户端对象，并通过该对象与客户端通过IO流进行数据传输。当该客户端访问结束，关闭该客户端。 注：TCP协议传输数据必须先开服务端，再开客户端。否则，客户端根本连接不上服务端。
代码示例： class TcpServer { public static void main(String[] args) throws Exception { //建立服务端Socket服务。并监听一个端口。 ServerSocket ss = new ServerSocket(10003); //通过accept方法获取连接过来的客户端对象 Socket s = ss.accept();//阻塞式 String ip = s.getInetAddress().getHostAddress(); System.out.println(ip+&#34;...connected&#34;); //获取客户端发过来的数据。那么要使用客户端的读取流来读取数据。 InputStream in = s.getInputStream(); byte[] buf = new byte[1024]; int len = in.read(buf); System.out.println(new String(buf,0,len)); s.close();//关闭客户端。 ss.close(); } } TCP协议上传文本文件示例：
import java.io.*; import java.net.*; class TextClient { public static void main(String[] args) throws Exception{ Socket s = new Socket(&#34;192.168.0.100&#34;,10006); //读取要上传文件的数据 BufferedReader bufr = new BufferedReader(new FileReader(&#34;client.txt&#34;)); PrintWriter out = new PrintWriter(s.getOutputStream(),true); String line = null; while ((line=bufr.readLine())!=null) { //向输出流中写数据,并刷新 out.println(line); } //关闭客户端的输出流，告诉服务端，客户端写完了,相当于给流中加入一个结束标记*1. s.shutdownOutput(); //读取并打印服务器发过来的数据 BufferedReader bufIn = new BufferedReader(new InputStreamReader(s.getInputStream())); String str = bufIn.readLine(); System.out.println(str); bufr.close(); s.close(); } } class TextServer { public static void main(String[] args) throws Exception{ ServerSocket ss = new ServerSocket(10006); Socket s = ss.accept(); String ip = s.getInetAddress().getHostAddress(); System.out.println(ip+&#34;...connected!&#34;); //获取客户端的数据，并写入到新文件中 BufferedReader bufIn = new BufferedReader(new InputStreamReader(s.getInputStream())); PrintWriter out = new PrintWriter(new FileWriter(&#34;server.txt&#34;),true); String line = null; while ((line=bufIn.readLine())!=null) { out.println(line); } //当文件数据全部写完后，向客户端发送上传成功的消息 PrintWriter pw = new PrintWriter(s.getOutputStream(),true); pw.println(&#34;上传成功&#34;); out.close(); s.close(); ss.close(); } } URI与URL URI：统一资源标示符。 URL：统一资源定位符，也就是说根据URL能够定位到网络上的某个资源，它是指向互联网“资源”的指针。 每个URL都是URI，但不一定每个URI都是URL。这是因为URI还包括一个子类，即统一资源名称（URN），它命名资源但不指定如何定位资源。 URL类 基本方法 URL(String spec) 根据 String 表示形式创建 URL 对象。 String getFile() 获取此 URL 的文件名。 String getHost() 获取此 URL 的主机名（如果适用）。 String getPath() 获取此 URL 的路径部分。 int getPort() 获取此 URL 的端口号。 String getProtocol() 获取此 URL 的协议名称。 String getQuery() 获取此 URL 的查询部 InputStream openStream() 打开到此 URL 的连接并返回一个用于从该连接读入的 InputStream。   ]]></content></entry><entry><title>Io of Javase Part2</title><url>/post/io-of-javase-part2/</url><categories><category>编程语言</category></categories><tags><tag>Java</tag></tags><content type="html"><![CDATA[  IO包中的其他类 打印流 PrintWriter与PrintStream：可以直接操作输入流和文件，都属于输出流，分别针对字符和字节。 调用 println 方法有自动 flush 功能 PrintWriter 和 PrintStream 重载的 print()和 println()用于多种数据类型的输出。 print()里的参数不能为空；println()可以 PrintStream: 提供了打印方法可以对多种数据类型值进行打印，并保持数据的表示形式 它不抛IOException 打印的所有字符都使用平台的默认字符编码转换为字节。 说明：在需要输出字符而不是输出字节的情况下，应该使用PrintWriter类。
构造函数，接收三种类型的值： 字符串路径 File对象 字节输出流 常见方法：
void write(int b) 将指定的字节写入此流。 void write(byte[] buf, int off, int len) 将 len 字节从指定的初始偏移量为 off 的 byte 数组写入此流。 print方法，可以输出多种数据类型 //write(int b)方法只写最低8位 out.write(97); //a //print方法将97先变成字符串保持原样将数据打印到目的地 out.print(97); //97 格式化输出：
PrintStream format(String format, Object&hellip; args) 使用指定格式字符串和参数将格式化字符串写入此输出流中。
PrintStream printf(String format, Object&hellip; args) 使用指定格式字符串和参数将格式化的字符串写入此输出流的便捷方法。
String name = &#34;小明&#34;; int age = 13; char score = &#39;A&#39;; String format = &#34;姓名 = %s,年龄 = %d,成绩 = %c&#34;; System.out.printf(format, name, age, score);//姓名 = 小明,年龄 = 13,成绩 = A 字符 描述 %s 表示内容是字符串 %d 表示内容是整数 %f 表示内容是小数 %c 表示内容是字符 注：使用“%s”来表示所有的数据类型！ PrintWriter：字符打印流 构造函数参数： 字符串路径
File对象
字节输出流
PrintWriter(OutputStream out, boolean autoFlush) 通过现有的 OutputStream 创建新的 PrintWriter。autoFlush * boolean 变量,如果为 true，则 println、printf 或 format 方法将刷新输出缓冲区
字符输出流
PrintWriter(Writer out, boolean autoFlush) 创建新 PrintWriter。autoFlush * boolean 变量,如果为 true，则 println、printf 或 format 方法将刷新输出缓冲区
文件的分割和合并操作 序列流（合并） SequenceInputStream（InputStream的子类）： 表示其他输入流的逻辑串联，可对多个流进行合并。
构造方法
SequenceInputStream(Emueration&lt;？ extends InputStream &gt; e) 通过记住参数来初始化新创建的 SequenceInputStream，该参数必须是生成运行时类型为 InputStream 对象的 Enumeration 型参数。 SequenceInputStream(InputStream s1, InputStream s2) 通过记住这两个参数来初始化新创建的 SequenceInputStream（将按顺序读取这两个参数，先读取 s1，然后读取 s2），以提供从此 SequenceInputStream 读取的字节。 基本方法：
int available() 返回不受阻塞地从当前底层输入流读取（或跳过）的字节数的估计值，方法是通过下一次调用当前底层输入流的方法。 void close() 关闭此输入流并释放与此流关联的所有系统资源。 int read() 从此输入流中读取下一个数据字节。 int read(byte[] b, int off, int len) 将最多 len 个数据字节从此输入流读入 byte 数组。 代码示例：
import java.io.*; import java.util.*; /* 需求：将1.txt、2.txt、3、txt文件中的数据合并到一个文件中。 分析：使用SequenceInputStream的构造方法 + Vector集合 */ class SequenceDemo { public static void main(String[] args) throws IOException{ /* //定义一个Vector集合 Vector&lt;FileInputStream&gt; v = new Vector&lt;FileInputStream&gt;(); //向集合中添加文件读取流元素 v.add(new FileInputStream(&#34;D:\\Java\\Day20\\SequenceDemo\\1.txt&#34;)); v.add(new FileInputStream(&#34;D:\\Java\\Day20\\SequenceDemo\\2.txt&#34;)); v.add(new FileInputStream(&#34;D:\\Java\\Day20\\SequenceDemo\\3.txt&#34;)); //通过elements方法返回一个文件读取流元素的枚举 Enumeration&lt;FileInputStream&gt; en =v.elements(); */ //改进后的方法：使用ArrayList比Vector效率更高 ArrayList&lt;FileInputStream&gt; al = new ArrayList&lt;FileInputStream&gt;(); for(int x = 1; x &lt;= 3; x++){ al.add( new FileInputStream(&#34;D:\\Java\\Day20\\SequenceDemo\\&#34;+ x + &#34;.txt&#34; )); } //使用集合工具类Collections的enumeration方法返回一个文件读取流元素的枚举 Enumeration&lt;FileInputStream&gt; en = Collections.enumeration(al); //通过传入的文件读取流元素来初始化新创建 SequenceInputStream SequenceInputStream sis = new SequenceInputStream(en); //将合并的数据输出到新的文件中 FileOutputStream fos = new FileOutputStream(&#34;D:\\Java\\Day20\\SequenceDemo\\4.txt&#34;); byte[] buf = new byte[1024]; int len =0; while ((len =sis.read(buf))!=*1) { fos.write(buf,0,len); } fos.close(); sis.close(); } } 操作对象流 ObjectInputStream与ObjectOutputStream
特点： 被操作的对象需要实现Serializable。类通过实现java.io.Serializable接口以启用序列化功能，Serializable只是一个标记接口。 writeObject方法不能写入类及其所有超类型的瞬态（transient关键字修饰）和静态字段的值。 RandomAccessFile 随机访问文件，自身具备读写的方法。不是io体系中的子类。直接继承自Object。通过skipBytes(int x),seek(int x)等方法来达到随机访问。
构造函数：
RandomAccessFile(File file, String mode) 创建从中读取和向其中写入（可选）的随机访问文件流，该文件由 File 参数指定。 RandomAccessFile(String name, String mode) 创建从中读取和向其中写入（可选）的随机访问文件流，该文件具有指定名称。 特点：
该对象即能读，又能写。 该对象内部维护了一个byte数组，并通过指针可以操作数组中的元素。 可以通过getFilePointer方法获取指针的位置，和通过seek方法设置指针的位置。 其实该对象就是将字节输入流和输出流进行了封装。 该对象的源或者目的只能是文件。通过构造函数就可以看出。而且操作文件模式：只读r, 读写rw等。 如果模式为只读r ,不会创建文件，会去读一个已存在的文件，如果该文件不存在，则会出现异常;
如果模式为rw，操作的文件不存在，会自动创建。如果存在不会覆盖。
管道流 简介 PipedInputStream(InputStream子类)和PipedOutputStream：（OutputStream）输入输出可以直接进行连接，通过结合线程使用。 管道输入流应该连接到管道输出流；管道输入流提供要写入管道输出流的所有数据字节。 代码示例： import java.io.*; //实现Runnable接口，覆盖run方法 class Read implements Runnable { private PipedInputStream in; Read(PipedInputStream in) { this.in = in; } public void run() { try { byte[] buf = new byte[1024]; System.out.println(&#34;读取前。。。没有数据阻塞&#34;); int len = in.read(buf); System.out.println(&#34;读到数据。。。阻塞结束&#34;); String s = new String(buf,0,len); System.out.println(s); in.close(); } catch (IOException e) { throw new RuntimeException(&#34;管道流读取失败&#34;); } } } //实现Runnable接口，覆盖run方法 class Write implements Runnable { private PipedOutputStream out; Write(PipedOutputStream out) { this.out = out; } public void run() { try { System.out.println(&#34;开始写入，等待6s后&#34;); Thread.sleep(6000); out.write(&#34;piped come！&#34;.getBytes()); } catch (Exception e) { throw new RuntimeException(&#34;管道输出流失败&#34;); } } } class PipedStreamDemo { public static void main(String[] args) throws IOException{ PipedInputStream in = new PipedInputStream(); PipedOutputStream out = new PipedOutputStream(); //将此管道输入流和输出流相连输入流和输出流相连 in.connect(out); Read r = new Read(in); Write w = new Write(out); //分别建立写和读的线程 new Thread(r).start(); new Thread(w).start(); } } 操作基本数据类型 简介 DataInputStream(FilterInputStream子类)与DataOutputStream（FilterOutputStream子类），可以用于操作基本数据类型的数据的流对象 操作字节数组 简介 ByteArrayInputStream（InputStream子类）：
在构造的时候，需要接收数据源，而且数据源是一个字节数组。 ByteArrayOutputStream（OutputStream子类）：
在构造的时候，不用定义数据目的，因为该对象中已经内部封装了可变长度的字节数组。这就是数据目的地。 因为这两个流对象都操作的数组，并没有使用系统资源。所以，不用进行close关闭。
编码表 简介 将各个国家的文字用数字来表示，并一一对应，形成一张表，这就是编码表。 字符串**&gt;字节数组：编码 字符数组**&gt;字符串：解码 常见的编码表 ASCII：美国标准信息交换码，用一个字节的7位表示。 ISO8859*1：拉丁码表。欧洲码表，用一个字节的8位表示。 GB2312：中国的中文编码表。 GBK：中国的中文编码表升级，融合了更多的中文文字符号。 Unicode：国际标准码，融合了多种文字。所有文字都用两个字节来表示,Java语言使用的就是unicode UTF*8：最多用三个字节来表示一个字符。 &hellip;&hellip; 两种情况 如果编码编错了，解不出来。 如果编对了，解错了，可能有救，也可能没救了。   ]]></content></entry><entry><title>Java基础之IO操作（上）</title><url>/post/io-of-javase-part1/</url><categories><category>编程语言</category></categories><tags><tag>Java</tag></tags><content type="html"> IO流 概述： 用来处理设备之间的数据传输。Java对数据的操作是通过流的方式。Java用于操作流的对象都在IO包中。 分类(按操作数据)： 字符流的抽象基类： Reader BuffedReader(字符读取流缓冲区) InputStreamReader（转换流） FileReader Writer PrintWrite BuffedWriter(字符输出流缓冲区) OutputStreamWriter（转换流） FileWriter 字节流的抽象基类：
InputStream
PipedInputStream（管道流） ObjectInputStream(对象流) SequenceInputStream（合并流） FileInputStream FilterInputStream BufferedInputStream(字节读取流缓冲区) OutputStream
PipedOutputStream（管道流） ObjectOutputStream(对象流) FileOutputStream FilterOutputStream BufferedOutputStream(字节输出流缓冲区) PrintStream 特点：四个类派生出来的子类名称都是以其父类名作为子类名的后缀
字符流 字节流+编码表
字符流两个基类： 字符流中的输出流：Writer 继承体系：
Writer PrintWrite BuffedWriter(字符输出流缓冲区) OutputStreamWriter（转换流） FileWriter 特点：文件不存在，则会自动创建；如果文件存在，则会被覆盖
基本方法：
void write(char[] cbuf) 写入字符数组。 abstract void write(char[] cbuf, int off, int len) 写入字符数组的某一部分。 void write(String str) 写入字符串。 void write(String str, int off, int len) 写入字符串的某一部分。 步骤：
创建一个可以往文件中写入字符数据的字符输出流对象 调用Writer对象中的write方法，写入数据，数据被写入到临时存储缓冲区中 进行刷新，将数据直接写入到目的地中 关闭流，关闭资源，在关闭前会先调用flush刷新缓冲中的数据到目的 …</content></entry><entry><title>java基础之集合（下）</title><url>/post/collection-of-java-part2/</url><categories><category><no value=/></categories><tags><tag><no value=/><tag><no value=/></tags><content type="html"><![CDATA[  Map集合 概述： 该集合存储键值对。一次添加一对，而且要保证键的唯一性。 嵌套类摘要 static interface Map.Entry&amp;lt;K,V&amp;gt; 映射项（键*值对）。 常用方法： 添加 value put( key,value )：返回前一个和key关联的值，如果没有返回null。 说明：添加元素,如果出现相同的键，那么后添加的值会覆盖原有键对应的值，put方法会返回被覆盖的值
删除 void clear():清空map集合。 value remove(Object key):根据指定的key删除这个键值对。 判断 boolean containsKey(key); boolean containsValue(value); boolean isEmpty(); 获取 value get(key):通过键获取值，如果没有该键返回null。当然可以通过返回null，来判断是否包含指定键。 int size():获取键值对个数。 Collection values() 返回此映射中包含的值的 Collection 视图。 Set&amp;lt;K&amp;gt; keySet() 返回此映射中包含的键的 Set 视图。 Set&amp;lt;Map.Entry&amp;lt;K,V&amp;gt;&amp;gt; entrySet() 返回此映射中包含的映射关系的 Set 视图。 map集合的两种取出方式; 第一种：keySet（）方法 原理： 通过keySet方法将map中所有的键存入到Set集合。因为set具备迭代器，所以可以迭代方式取出所有的键，再根据get方法，获取每一个键对应的值。 代码示例： import java.util.*; class MapDemo { public static void main(String[] args) { Map&amp;lt;String,String&amp;gt; map=new HashMap&amp;lt;String,String&amp;gt;(); map.put(&amp;#34;001&amp;#34;,&amp;#34;A1&amp;#34;); map.put(&amp;#34;003&amp;#34;,&amp;#34;C3&amp;#34;); map.put(&amp;#34;002&amp;#34;,&amp;#34;B2&amp;#34;); map.put(&amp;#34;004&amp;#34;,&amp;#34;D4&amp;#34;);	//1.先获取map集合的 …  ]]></content></entry><entry><title>java基础之集合（上）</title><url>/post/collection-of-java-part1/</url><categories><category>编程语言</category></categories><tags><tag>Java</tag></tags><content type="html"><![CDATA[  集合类 面向对象语言对事物的体现都是以对象的形式，所以为了方便对多个对象的操作，就要对对象进行存储，集合就是存储对象最常用的一种方式。 集合特点： 用于存储对象的容器。 集合的长度是可变的。 集合中不可以存储基本数据类型值。 集合容器因为内部的数据结构不同，有多种具体容器。不断的向上抽取，就形成了集合框架。 集合类的构成： 集合框架的构成及分类 Java集合类主要由两个接口（Collection和Map）派生出来：
Collection
List（子接口）：有序（存入和取出的顺序一致），可以重复，有索引
Vector(实现类):
底层数据结构：数组 线程：同步 操作数据：增删，查询都很慢 Enumeration elements(): 返回此向量的组件的枚举。 IO流中SequenceInputStream中用到 Vector&lt;FileInputStream&gt; v = new Vector&lt;FileInputStream&gt;(); Enumeration&lt;FileInputStream&gt; en =v.elements(); SequenceInputStream sis = new SequenceInputStream(en); ArrayList (实现类)： 替代了Vector
底层数据结构：数组 线程：不同步 操作数据：增删稍慢、改查很快 LinkedList (实现类):
底层数据结构：链表(JDK1.7/8 之后取消了循环，修改为双向链表) 线程：不同步 操作数据：增删除很快，改查很慢 Set（子接口） ：无序(存入和取出的顺序不一定一致)，不能重复
HashSet(实现类)：底层数据结构是哈希表，不同步 LinkedHashSet：具有可预知迭代顺序的 Set 接口的哈希表和链接列表实现 TreeSet(实现类)：底层数据结构是二叉树，不同步，可以对Set集合中的元素进行排序 Map：存储键值对，一次添加一对，而且要保证键的唯一性
Hashtable实现类)：
底层数据结构：哈希表，不可以存入null键null值 线程：同步 操作数据：jdk1.0.效率低 Properties：线程同步，用来存储键值对型的配置文件的信息，可以和IO技术相结合 HashMap (实现类)：
底层数据结构：哈希表，允许使用 null值和null键, 底层是基于数组和链表实现 线程：不同步 操作数据：将hashtable替代，jdk1.2.效率高 TreeMap (实现类)：
底层数据结构：二叉树 线程：不同步 操作数据：用于给map集合中的键进行排序 Collection接口: Collection的常见方法： 添加： boolean add(Object obj); boolean addAll(Collection coll); 删除： boolean remove(Object obj); boolean removeAll(Collection coll); void clear( ); 判断： boolean contains(Object obj); boolean containsAll(Collection coll); boolean isEmpty();判断集合中是否有元素。 获取： int size( );返回此 collection 中的元素数。 Iterator iterator( ); 其他： boolean retainAll(Collection coll); 取交集 Object[ ] toArray( ); 将集合转成数组 Iterator接口 概述： 对所有的Collection容器进行元素取出的公共接口，称为迭代器 Collection中有iterator方法，所以每一个子类集合对象都具备迭代器 它替代了Vector类中的Enumeration(枚举)。迭代器的next方法是自动向下取元素，要避免出现NoSuchElementException。 方法： boolean hasNext()：若被迭代的集合元素还没有被遍历，返回true.
Object next():返回集合的下一个元素.
void remove():删除集合上一次next()方法返回的元素。(若集合中有多个相同的元素，都可以删掉)
取出集合的元素代码示例：
//方式一： Iterator it=al.iterator();//获取迭代器，用于取出集合中的元素。 while(it.hasNext()) { System.out.println(it.next()); } //方式二：for循环结束，Iterator变量内存释放，更高效 for (Iterator it=al.iterator();it.hasNext() ; ) { System.out.println(it.next()); } 高级for循环 格式： for(数据类型 变量名：被遍历的集合(Collection)或者数组){ }
对集合进行遍历的特点：只能获取集合元素，但是不能对集合进行操作 迭代器除了遍历，还可以进行remove集合中的动作。如果是用ListIterator,还可以在遍历过程中对集合进行增删改查的动作。 支持迭代器的集合均支持高级for,Map不支持，故需转成Set集合 传统for和高级for区别： 高级for有一个局限性。必须有被遍历的目标。 建议在遍历数组的时候，还是希望用传统for,因为传统for可以定义脚标。 子接口： ListIterator：专门输出List中的元素 方法： boolean hasNext() E next() 返回列表中的下一个元素。 int nextIndex() 返回对 next 的后续调用所返回元素的索引。 void add(E e) 将指定的元素插入列表（可选操作） void set(E e) 用指定元素替换 next 或 previous 返回的最后一个元素（可选操作）。 void remove()从列表中移除由 next 或 previous 返回的最后一个元素（可选操作）。 boolean hasPrevious() 如果以逆向遍历列表，列表迭代器有多个元素，则返回 true。 E previous()返回列表中的前一个元素。 List Collection的子接口，有序（存入和取出的顺序一致），元素都有索引（角标），允许重复元素 特有的常见方法。 特点：都可以操作角标。
添加
void add(index,element); void addAll(index,collection); 删除
Object remove(index); boolean remove(Object o) 从此列表中移除第一次出现的指定元素（如果存在）（可选操作）。 修改
Object set(index,element); 获取：
Object get(index); int indexOf(object); int lastIndexOf(object)： List subList(from,to); list特有的取出元素的方式之一
for(int x = 0; x &lt; list.size(); x++){ System.out.println( &quot;get:&quot; + list.get(x)); } 集合变数组：
Object[ ] toArray() 返回按适当顺序包含列表中的所有元素的数组（从第一个元素到最后一个元素）。 List集合特有的迭代器：ListIterator(列表迭代器)
在迭代时，不可以通过集合对象的方法操作集合中的元素，因为会发生ConcurrentModificationException异常； 所以，在迭代器时，只能用迭代器的方法操作元素，可是Iterator方法是有限的，只能对元素进行判断，取出和删除的操作； 如果想要其他的操作如添加，修改等，就需要使用其子接口，ListIterator.该接口只能通过List集合的listIterator方法获取 。 ArrayList 基于动态数组实现 基本方法：增删改查，与List中的方法基本一致 LinkedList 基本方法： void addFirst(); 1.6以后版本为offerFirst( ) void addLast(); 1.6以后版本为offerLast( ) E getFirst();获取但不移除，如果链表为空，抛出NoSuchElementException。 1.6以后版本为：peekFirst(); 获取但不移除，如果链表为空，返回null E getLast(); 1.6以后版本为：peekLast( ); E removeFirst(); 获取并移除，如果链表为空，抛出NoSuchElementException。 1.6以后版本为：pollFirst( );获取并移除，如果链表为空，返回null E removeLast(); 1.6以后版本为： pollLast( ); Set Collection的子接口,元素不可以重复，是无序的，Set接口中的方法和Collection一致 HashSet： 成员变量: map: 用于存放最终数据的 PRESENT: 是所有写入map的value值 构造函数: 利用了HashMap初始化了map add方法: 将存放的对象当做了 HashMap 的健，value 都是相同的 PRESENT 由于 HashMap 的 key 是不能重复的，所以每当有重复的值写入到 HashSet 时，value 会被覆盖，但 key 不会收到影响，这样就保证了 HashSet 中只能存放不重复的元素 HashMap 会出现的问题 HashSet 依然不能避免 public boolean add(E e) { return map.put(e, PRESENT)==null; } 概述： 内部数据结构是哈希表，是不同步的。
HashSet是如何保证元素唯一性的呢？
是通过元素的两个方法，haseCode和equals来完成的。 如果元素的HashCode值相同，才会判断equals是否为true; 如果元素的HashCode值不同，不会调用equals。 注意：对于判断元素是否存在，以及删除等操作，依赖的方法是元素的hashCode和equals方法。
//HashSet集合中添加重复元素时，返回false System.out.println(hs.add(&#34;java01&#34;)); System.out.println(hs.add(&#34;java01&#34;));//未存储进去为false 代码示例： import java.util.HashSet; import java.util.Iterator; /* * 往HashSet集合中存入自定义对象姓名和年龄相同为同一个人，重复元素。 */ class HashSetDemo { public static void main(String[] args) { System.out.println(&#34;以下为集合中元素存储的比较过程：&#34;); HashSet&lt;Person&gt; hs = new HashSet&lt;Person&gt;(); hs.add(new Person(&#34;a1&#34;, 11)); hs.add(new Person(&#34;a2&#34;, 12)); hs.add(new Person(&#34;a3&#34;, 13)); hs.add(new Person(&#34;a2&#34;, 12)); hs.add(new Person(&#34;a3&#34;, 14)); System.out.println(&#34;去掉重复元素后的结果：&#34;); Iterator&lt;Person&gt; it = hs.iterator(); while (it.hasNext()) { Person p = it.next(); System.out.println(p.getName() + &#34;...&#34; + p.getAge()); } } } class Person { private String name; private int age; Person(String name, int age) { this.name = name; this.age = age; } // 复写hashCode方法，集合底层内部调用 public int hashCode() { System.out.println(this.name + &#34;...hashCode&#34;); return name.hashCode() + age * 3;// *3保证hash值的唯一性 } // 复写equals方法 public boolean equals(Object obj) { if (!(obj instanceof Person)) return false; Person p = (Person) obj; System.out.println(this.name + &#34;:equals:&#34; + p.name); return this.name.equals(p.name) &amp;&amp; this.age == p.age;// 此处equals为Object的方法. } public String getName() { return name; } public int getAge() { return age; } } //运行结果 // 以下为集合中元素存储的比较过程： // a1...hashCode // a2...hashCode // a3...hashCode // a2...hashCode // a2:equals:a2 // a3...hashCode // 去掉重复元素后的结果： // a1...11 // a2...12 // a3...13 // a3...14 LinkedHashSet 具有可预知迭代顺序的 Set 接口的哈希表和链接列表实现，该类为HashSet的子类，可以将无序变有序。 TreeSet 特点： 可以对Set集合中的元素进行排序（默认字母的自然顺序），底层数据结构是二叉树 判断元素唯一性的方式：就是根据保证元素唯一性的依据比较方法（实现Comparable接口中根据覆盖的compareTo方法返回值，实现Comparator接口中根据覆盖的compare方法）的返回结果是否是0，是0，就是相同元素，不存。 自定义排序： 第一种方式：让元素自身具备比较性，元素需要实现Comparable接口，覆盖compareTo方法。这种方式也称为元素的自然顺序，或者叫做默认顺序。
Comparable接口：位于java.lang包中，只有一个方法compareTo int compareTo(T o)：比较此对象与指定对象的顺序。如果该对象小于、等于或大于指定对象，则分别返回负整数、零或正整数。 代码示例：
import java.util.Iterator; import java.util.TreeSet; /** * 需求： 往TreeSet集合中存储自定义对象学生。 年龄和姓名均相同时，视为同一个人， * 并按照学生的年龄进行排序, 当年龄相同时，按照名字的自然顺序排序 */ public class TreeSetDemo { public static void main(String[] args) { TreeSet&lt;Student&gt; ts = new TreeSet&lt;Student&gt;(); ts.add(new Student(&#34;C&#34;, 19)); ts.add(new Student(&#34;D&#34;, 17)); ts.add(new Student(&#34;C&#34;, 19)); ts.add(new Student(&#34;A&#34;, 20)); ts.add(new Student(&#34;E&#34;, 17)); Iterator&lt;Student&gt; it = ts.iterator(); while (it.hasNext()) { Student s = (Student) it.next(); System.out.println(s.getName() + &#34;...&#34; + s.getAge()); } } } class Student implements Comparable&lt;Student&gt; { // 该接口强制让学生具备比较性。 private String name; private int age; Student(String name, int age) { this.name = name; this.age = age; } public int compareTo(Student s) { // 内部底层调用 // 存与取的方式相同,将不能除去重复的对象元素 //return 1; // 存与取的方式刚好相反，将不能除去重复的对象元素 // return *1; // 按自然顺序取出 System.out.println(this.name + &#34;:compareto:&#34; + s.name); if (this.age &gt; s.age) { return 1; // 判断次要条件：当年龄相同时，比较姓名是否相同 } else if (this.age == s.age) { return this.name.compareTo(s.name);// 字符串自有的比较方法。 } else return *1; /*简化书写 int temp = this.age * s.age; return temp == 0? this.name.compareTo(s.name) : temp;*/ } public String getName() { return name; } public int getAge() { return age; } } 第二种方式：当元素自身不具备比较性时，或者具备的比较性不是所需要的，就让集合自身具备比较功能，定义一个类实现Comparator接口(java.util包)，覆盖compare方法。 将该类对象作为参数传递给TreeSet集合的构造函数。
TreeSet(Comparator&lt;? super E&gt; comparator) 构造一个新的空 TreeSet，它根据指定比较器进行排序。
注意：如果自定义类实现了Comparable接口，并且TreeSet的构造函数中也传入了比较器，那么将以比较器的比较规则为准
代码示例：
import java.util.Comparator; import java.util.Iterator; import java.util.TreeSet; /* * 需求：按姓名排序，不修改compareTo方法 */ class TreeSetDemo2 { public static void main(String[] args) { TreeSet&lt;Student&gt; ts = new TreeSet&lt;Student&gt;(new MyCompare());// 将比较器对象作为参数传递给TreeSet集合的构造函数 ts.add(new Student(&#34;A&#34;, 20)); ts.add(new Student(&#34;C&#34;, 19)); ts.add(new Student(&#34;E&#34;, 17)); ts.add(new Student(&#34;D&#34;, 17)); ts.add(new Student(&#34;A&#34;, 18)); ts.add(new Student(&#34;C&#34;, 19)); Iterator&lt;Student&gt; it = ts.iterator(); while (it.hasNext()) { Student s = (Student) it.next(); System.out.println(s.getName() + &#34;...&#34; + s.getAge()); } } } class Student implements Comparable&lt;Student&gt; {	// 该接口强制让学生具备比较性。 private String name; private int age; Student(String name, int age) { this.name = name; this.age = age; } public int compareTo(Student s){	// 内部底层调用 // 按自然顺序取出 System.out.println(this.name + &#34;:compareto:&#34; + s.name); int temp = this.age * s.age; return temp == 0? this.name.compareTo(s.name) : temp; } public String getName() { return name; } public int getAge() { return age; } } // 定义一个类，实现Comparator接口 class MyCompare implements Comparator&lt;Student&gt; { //覆盖compare方法 public int compare(Student s1, Student s2) { int num = s1.getName().compareTo(s2.getName()); if (num == 0) { /*//第一种方法 if(s1.getAge()&gt;s2.getAge()) return 1; if(s1.getAge()==s2.getAge()) return 0; return *1; */ /*//第二种方法 return new Integer(s1.getAge()).compareTo(new Integer(s2.getAge()));*/ // 第三种方法 return s1.getAge() * s2.getAge(); } return num; } }   ]]></content></entry><entry><title>Java基础之多线程</title><url>/post/multi-thread-of-java/</url><categories><category>编程语言</category></categories><tags><tag>java</tag></tags><content type="html"><![CDATA[  进程、线程和多线程 进程：正在进行中的程序（直译）。 线程：进程中一个负责程序执行的控制单元（执行路径）。 多线程：一个进程中可以有多个执行路径，称之为多线程。 好处：解决了多部分代码同时运行的问题。 弊端：线程太多，会导致效率的降低。 创建进程方式 方式一：继承Thread类 步骤： 定义一个类继承Thread类。 覆盖Thread类中的run方法。 直接创建Thread的子类对象创建线程。 调用start方法开启线程并调用线程的任务run方法执行。 构造方法： Thread(Runnable target)分配新的 Thread 对象，target为其run 方法被调用的对象。 继承自Object的方法 void wait( ) / void wait(long timeout) 让线程处于冻结状态，被wait的线程会被存储到线程池中。 void notify( ) 唤醒线程池中的一个线程（任何一个都有可能）。 void notifyAll( ) 唤醒线程池中的所有线程。 注意：该三种方法只能由锁来调用 常用方法 static Thread currentThread( ) 返回对当前正在执行的线程对象的引用
long getId( ) 返回该线程的标识符。
void setName( String name **)**改变线程名称，使之与参数 name 相同。
String getName( ) 返回该线程的名称。
void setPriority( int newPriority ) 更改线程的优先级。
int getPriority( ) 返回线程的优先级。
Thread.State getState( ) 返回该线程的状态。
线程状态（6中）： NEW、RUNNABLE、BLOCKED、WAITING、TIMED_WAITING和TERMINATED已
String toString( ) 返回该线程的字符串表示形式，包括线程名称、优先级和线程组。
void run( ) 如果该线程是使用独立的 Runnable 运行对象构造的，则调用该 Runnable 对象的 run 方法；否则，该方法不执行任何操作并返回。
static void sleep( long millis ) 在指定的毫秒数内让当前正在执行的线程休眠（暂停执行）。
wait和sleep区别 wait可以指定时间也可以不指定。sleep必须指定时间 在同步中，对CPU的执行权和锁的处理不同 wait：释放执行权，释放锁 sleep：释放执行权，不释放锁 void start( ) 使该线程开始执行；Java 虚拟机调用该线程的 run 方法。
void join( ) 等待该线程终止（当A线程执行到B线程的.join()方法时，等B线程都执行完，A才会执行）。注：临时加入一个线程可以使用该方法。
static void yield( ) 暂停当前正在执行的线程对象，并执行其他线程。
void setDaemon( boolean on ) 将该线程标记为守护线程或用户线程。
方式二：实现 Runnable 接口（常用方式：避免了Java单继承的局限性） 步骤： 定义类实现Runnable接口。 覆盖接口中的run方法，将线程的任务代码封装到run方法中。 通过Thread类创建线程对象，并将Runnable接口的子类对象作为Thread类的构造函数的参数进行传递（因为线程的任务都封装在Runnable接口子类对象的run方法中。所以要在线程对象创建时就必须明确要运行的任务）。 调用线程对象的start方法开启线程。 线程安全问题 产生原因 多个线程在操作共享数据 操作共享数据的线程代码有多条 当一个线程在执行操作共享数据的多条代码过程中，其他线程参与了运算，就导致线程安全问题的产生。 解决办法 对多条操作共享数据的语句，只能让一个线程都执行完。在执行过程中，其他线程不可以参与执行。
同步代码块 格式： synchronized(obj) { //obj是一个对象，表示锁(监视器锁) //持有锁的线程可以在同步中执行，没有持有锁的线程即使获取cpu的执行权，也进不去，因为没有获取锁。 } 同步方法(函数) 格式：在方法上加上synchronized修饰符即可。（一般不直接在run方法上添加） 同步方法的同步监听器其实是 this synchronized 返回值类型 方法名（参数列表） ｛ ｝ 同步函数和同步代码块的区别
同步函数的锁是固定的this 同步代码块的锁是任意的对象 建议使用同步代码块 注意：
如果同步函数和同步代码块都使用this作为锁，就可以实现同步 静态同步函数的锁是该函数所属字节码文件对象，可以getClass方法获取，也可以用当前类名.class表示。 死锁： 常见情形为同步的嵌套，当两个线程相互等待对方释放同步监视器时就会发生死锁。一旦出现死锁，程序既不会发生任何异常，也不会出现任何提示。
同步锁 JDK 1.5以后的另一种同步机制，将同步和锁封装成对象
Lock接口（位于java.util.concurrent.locks包）：替代了同步代码块或者同步函数
子类：ReentrantLock 方法摘要： Condition newCondition() 返回绑定到此 Lock 实例的新 Condition 实例,以调用其方法。 void lock( ) 获取锁 void unlock( ) 释放锁 ，为了防止异常出现，导致锁无法被关闭，所以锁的关闭动作要放在fianlly中 Condition接口（位于java.util.concurrent.locks包）：
替代了Object中的wait、notify和notifyAll方法，分别为：await、signal和signalAll方法。 线程间通信 使用时机 多个线程在处理同一资源，但是任务却不同，这时就需要线程间通信。
等待/唤醒机制方法 wait、notify和notifyAll都定义在Object类中，因为这些方法都是监视器（锁）的方法，锁可以是任意的对象，任意的对象调用方式一定在Object类中。
生产者消费者示例（生产一个，消费一个）： //该示例中，实现了本方只唤醒对方的操作 import java.util.concurrent.locks.*; class ProducerConsumerDemo { public static void main(String[] args) { Resource r=new Resource(); //通过已有的锁获取两组监视器，一组监视生产者，一组监视消费者 Producer pro=new Producer(r); Consumer con=new Consumer(r); Thread t1=new Thread(pro); Thread t2=new Thread(con); Thread t3=new Thread(pro); Thread t4=new Thread(con); t1.start(); t2.start(); t3.start(); t4.start(); } } class Resource { private String name; private int count=1; private boolean flag=false; private Lock lock=new ReentrantLock(); private Condition condition_pro =lock.newCondition(); private Condition condition_con =lock.newCondition(); public void set(String name)throws InterruptedException { //加锁 lock.lock(); try { while(flag) condition_pro.await();	//生产方线程被存储到线程池中 this.name=name+&#34;...&#34;+count++; System.out.println(Thread.currentThread().getName()+&#34;.....生产者.....&#34;+this.name); flag=true; condition_con.signal();	//唤醒处于线程池中的消费方线程 } // 使用finally块保证释放锁 finally{ lock.unlock(); } } public void out()throws InterruptedException { //加锁 lock.lock(); try { while(!flag) condition_con.await();	//消费方线程被存储到线程池中 System.out.println(Thread.currentThread().getName()+&#34;...消费者...&#34;+this.name); flag=false; condition_pro.signal();	//唤醒处于线程池中的生产方线程 } // 使用finally块保证释放锁 finally { lock.unlock(); } } } //定义生产者类，实现Runnable接口，覆盖run方法 class Producer implements Runnable { private Resource res; Producer(Resource res) { this.res=res; } public void run(){ while (true) { try { res.set(&#34;+商品+&#34;); } catch (InterruptedException e) { } } } } //定义消费者类，实现Runnable接口，覆盖run方法 class Consumer implements Runnable { private Resource res; Consumer(Resource res) { this.res=res; } public void run() { while (true) { try { res.out(); } catch (InterruptedException e) { } } } } 停止线程 思路： 任务中通常都有循环结构，只要控制循环就可以结束任务 控制循环通常用定义标记来完成 如果线程处于冻结状态，无法读取标记，可以使用interrupt()方法将线程从冻结状态强行恢复到运行状态，让线程具备CPU执行权，该动作会发生InterruptedException，需要处理   ]]></content></entry><entry><title>Java基础之常用类</title><url>/post/the-common-class-of-java/</url><categories><category>编程语言</category></categories><tags><tag>java</tag></tags><content type="html"><![CDATA[  Object类 类 Object 是类层次结构的根类。每个类都使用 Object 作为超类。所有对象（包括数组）都实现这个类的方法。 常用方法 getClass() //Class&lt;?&gt; getClass() 返回此 Object 的运行时类。 public final native Class&lt;?&gt; getClass(); haseCode() // int hashCode() 返回该对象的哈希码值。 public native int hashCode(); toString() 默认返回&quot;对象所属的类名+@+对象的哈希值（十六进制）
//String toString() 返回该对象的字符串表示。 public String toString() { return getClass().getName() + &#34;@&#34; + Integer.toHexStrin(hashCode()); } 复写示例: public String toString() { return &#34;Student [name=&#34; + name + &#34;, age=&#34; + age + &#34;]&#34;; } equals() // boolean equals(Object obj) 指示其他某个对象是否与此对象“相等”。 public boolean equals(Object obj) { return (this == obj); } equals() 方法与 &ldquo;==&rdquo; 区别
共同点: 均用来做比较,返回类型均为boolean类型
区别:
&ldquo;==&ldquo;为基本运算符,即可比较基本类型数据(值比较),也可比较引用数据类型(地址比较); &ldquo;equals()&ldquo;只能比较引用数据类型(地址值),底层依赖&rdquo;==&ldquo;运算符,一般将其复写来比较对象中的属性值. 复写示例:
public boolean equals(Object obj) { if(!(obj instanceof Student)) throw new ClassCastException(&#34;类型错误&#34;); Student s = (Student)obj; return this.name.equals(s.name) &amp;&amp; this.age == s.age; } 注意：当此方法被重写时，通常有必要重写 hashCode 方法，以维护 hashCode 方法的常规协定，**该协定声明相等对象必须具有相等的哈希码。
String类 构造方法：重载形式 String(byte[] bytes) 通过使用平台的默认字符集解码指定的 byte 数组，构造一个新的 String。 String(char[] value) 分配一个新的 String，使其表示字符数组参数中当前包含的字符序列。 String(char[] value, int offset, int count) 分配一个新的String，它包含取自字符数组参数一个子数组的字符。 常用方法： 获取： 长度：
int length() 返回此字符串的长度。 根据位置获取位置上某个字符
char charAt(int index) 根据位置获取字符。 根据字符获取该字符在字符串中的位置
int indexOf(int ch): 返回ch在字符串中第一次出现的位置 ch * 一个字符（Unicode 代码点）。
int indexOf(int ch,int fromIndex): 从fromIndex指定位置开始，获取ch在字符串中出现的位置 int indexOf(String str): 返回str在字符串中第一次出现的位置 int indexOf(String str,int fromIndex): 从fromIndex指定位置开始，获取str在字符串中出现的位置 int lastIndexOf(int ch) 返回指定字符在此字符串中最后一次出现处的索引。 子串：
String substring(begin) String substring(begin,end) 判断： 包含： boolean contains(str)
特殊之处：int indexOf(String str):返回str在字符串中第一次出现的位置，如果返回*1，表示该str不在字符串中存在;所以，也可以用于对指定判断是否包含。if(str.indexOf(&quot;aa&quot;)!=*1)而且该方法既可以又可以获取出现的位置。
开头：boolean startsWith(str)
结尾：boolean endsWith(str) str.endsWith(&quot;.java&quot;)
是否有内容:
boolean ifEmpty(): 原理就是判断长度是否为0 内容是否相同：
boolean equals(Object anObject) 将此字符串与指定的对象比较。 boolean equalsIgnoreCase() 判断内容是否相同，并忽略大小写 String s1 = &#34;abc&#34;;//字符串常量池中创建对象 String s2 = &#34;abc&#34;;//s2直接指向已存在的对象 String s3 = new String(&#34;abc&#34;);//堆内存中创建对象 System.out.println(&#34;s1==s2&#34;);//true System.out.println(&#34;s1==s3&#34;);//false System.out.println(s1.equals(s2));//true(复写了Object方法，用于比较字符串内容) 自然顺序比较：实现了Comparable接口
int compareTo(String)： 相等返回0；调用&gt;参，返回正；调用&lt;参,返回负 转换： 将字符串转换成大写或者小写。 String toUpperCase(); String toLowerCase(); 字符串两端的多个空格去除 String trim() 字符数组&ndash;&gt;字符串 String（char[]）
String(char[],ofset,count):将字符数组中的一部分转换成字符串。
String( )初始化一个新创建的 String 对象，使其表示一个空字符序列。String s = new String();//等效与String s =&quot;&quot;;
static String copyValueOf(char[]data): 返回指定数组中表示该字符序列的 String
static String copyValueOf(char[]data,int offset,int count);
static String valueOf(char[])
字符串**&gt;字符数组 byte[] toCharArray() 字节数组**&gt;字符串 String（byte[]） String(byte[],ofset,count):将字节数组中的一部分转换成字符串。 字符串**&gt;字节数组 char[] getBytes(): 使用平台的默认字符集将此 String 编码为 byte 序列，并将结果存储到一个新的 byte 数组中 基本数据类型&ndash;&gt;字符串 static String ValueOf(int) 3+&quot;&quot;;//String.valueOf(3); static String ValueOf(double) 替换： String replace(oldchar,newchar) 若没有找到，则返回原字符串 连接 String contact（String str） 切割 String[] split(regex) concat效果与&rdquo;+&ldquo;连接符效果一致，但是效果更高一些
其他 String intern() 返回字符串对象的规范化表示形式。 String s1 = &#34;abc&#34;; String s2 = new String(&#34;abc&#34;); String s3 = s1.intern(); //true String s4 = s2.intern(); //false StringBuffer和StringBuilder类 StringBuffer 字符串缓冲区，用于存储数据的容器，字符串缓冲区初始容量为16个字符，其实质还是数组 特点： 长度是可变的。 可以存储不同类型数据。 最终要转成字符串进行使用。 常用方法： 添加
StringBuffer append(data):将指定数据作为参数添加到已有数据结尾处 StringBuffer insert(index,data):可以将数据插入到指定index位置 删除
StringBuffer delete(start,end):删除缓冲区中的数据，包含start,不包含end. StringBuffer deleteCharAt(index):删除指定位置的字符 //清空缓冲区: //sb=new StringBuffer(); sb.delete(0,sb.length()); 修改
StringBuffer replace(start,end,string) void setCharAt(int index,char ch) 查找
char charAt(int index) int indexOf(String str) int lastIndexOf(String str) int length() String substring(int start,int end) 反转
StringBuffer reverse(); 将缓冲区中指定数据存储到指定字符数组中。
void getChars(int srcBegin,int srcEnd,char[]dst,int dstBegin) 其他方法
void setLength(int newLength);设置字符序列的长度 如果小于已有字符序列的长度，相当于清除缓冲区中的一部分内容。 如果大于已有字符序列的长度，相当于扩充缓冲区，扩充部门内容用空格字符填充。
说明：当创建的StringBuffer内容长度大于16，将会新创建一个新数组，长度比旧数组要长。然后把就数组的内容拷贝到新的数组，超出旧数组长度范围的内容将会放在新数组现在内容的后面，也可以通过StringBuffer(int capacity)构造函数指定
StringBuilder 概述 JDK1.5版本之后出现, 与StringBuffer功能一模一样 两者区别 StringBuffer是线程同步的，通常用于多线程。 StringBuilder是线程不同步的，通常用于单线程，它的出现能够提高程序效率。 小结：故StringBuilder多用于多个线程是不安全的，如果需要这样的同步，则建议使用StringBuffer。 升级三个因素： 提高效率 简化书写 提高安全性 Arrays类 基本类型包装类 将基本数据类型封装成了对象，在对象中定义属性和方法 基本数据类型 byte short int long char float double boolean 包装类 Byte Short Integer Long Character Float Double Boolean 基本数据类型和包装类相互转换 基本数据类型 &ndash;&gt; 包装类 除了Character外，其他包装类都可以传入一个字符串参数构建包装类对象 包装类 &ndash;&gt; 基本数据类型 包装类的实例方法 xxx xxxValue( ); // xxx表示包装类对应的基本数据类型 基本类型和String之间的相互转换 String &ndash;&gt; 基本类型 除了Character外所有的包装类提供 xxx parseXxx(String str) 静态方法，用于把一个特定的字符串转换成基本类型变量 int i = Integer.parseInt(&ldquo;16&rdquo;);
基本类型 &ndash;&gt; String String 类有静态方法 valueOf()，用于将基本类型的变量转换成String类型 String s1 = String.valueOf(16);
进制转换方法 十进制 &ndash;&gt; 其他进制方法： String toBinaryString(int i) 以二进制（基数 2）无符号整数形式返回一个整数参数的字符串表示形式。 String toOctalString(int i) 以八进制（基数 8）无符号整数形式返回一个整数参数的字符串表示形式。 String toHexString(int i) 以十六进制（基数 16）无符号整数形式返回一个整数参数的字符串表示形式。 String toString(int i,int radix) 返回用第二个参数指定基数表示的第一个参数的字符串表示形式。 其他进制 &ndash;&gt; 十进制方法： static int parseInt(String s,int radix) 使用第二个参数指定的基数，将字符串参数解析为有符号的整数。 Scanner类 概述： 一个可以使用正则表达式来解析基本类型和字符串的简单文本扫描器，位于java.util包，被final修饰，该类不可以被继承 构造方法 Scanner(InputStream source) Scanner sc = new Scanner(System.in)
一般方法 hasNextXxx() 判断是否还有下一个输入项,其中Xxx可以是Int,Double等。如果需要判断是否包含下一个字符串，则可以省略Xxx xxx nextXxx() 获取下一个输入项。Xxx的含义和上个方法中的Xxx相同,默认情况下，Scanner使用空格，回车等作为分隔符 String nextLine():获取一个String类型的值。可以接收任意类型，只要遇到\r\n就证明一行结束；建议使用该方法 枚举类 概述 实例固定且有限的类就在Java中称作枚举类，Java从JDK1.5以后增加了对枚举类（关键字enum 与class、interface关键字的地位相同）的支持。 特点： 非抽象的枚举默认会使用fianl修饰，因此枚举不能派生子类 枚举类的构造器只能使用private访问控制，若省略默认为private 其所有实例必须在第一行显示列出，最后一个枚举项后的分号是可以省略的，但是如果枚举类有其他的东西，这个分号就不能省略。建议不要省略 在在switch语句中的可以使用 枚举类型成员默认被public static final修饰 手动实现方式 思路： 通过private将构造器隐藏起来； 把这个类的所有可能实例都使用public static final 修饰的类变量来保存； 如果有必要，可以提供一些静态方法，允许其他程序根据特定参数来获取与之匹配的实例 代码示例： public abstract class Week { public static final Week MON = new Week(&#34;星期一&#34;) { public void show() { System.out.println(&#34;星期一&#34;); } }; public static final Week TUE = new Week(&#34;星期二&#34;){ public void show() { System.out.println(&#34;星期二&#34;); } }; public static final Week WED = new Week(&#34;星期三&#34;){ public void show() { System.out.println(&#34;星期三&#34;); } }; private String name; private Week(String name){ this.name = name; }	//私有构造,不让其他类创建本类对象 public String getName() { return name; } public abstract void show(); } 枚举类的常见方法 int ordinal() 返回枚举常量的序数（它在枚举声明中的位置，其中初始常量序数为零）。 int compareTo(E o) 比较此枚举与指定对象的顺序。 String name() 返回此枚举常量的名称，在其枚举声明中对其进行声明。 String toString()返回枚举常量的名称，它包含在声明中。 static T valueOf(Class type,String name)返回带指定名称的指定枚举类型的枚举常量。 values() 此方法虽然在JDK文档中查找不到，但每个枚举类都具有该方法，它遍历枚举类的所有枚举值非常方便 代码示例 enum Week { MON(&#34;星期一&#34;){ public void show() { System.out.println(&#34;星期一&#34;); } },TUE(&#34;星期二&#34;){ public void show() { System.out.println(&#34;星期二&#34;); } },WED(&#34;星期三&#34;){ public void show() { System.out.println(&#34;星期三&#34;); } }; private String name; private Week(String name) { this.name = name; } public String getName() { return name; } public String toString() { return name; } public abstract void show();	} public class EnumDemo { public static void main(String[] args) { Week mon = Week.MON; mon.show(); //遍历枚举类的所有枚举值 Week[] arr = Week.values(); for (Week week : arr) { System.out.println(week); } } } System类 System类中的字段和方法都是静态的 常见方法 有关IO流的方法
static InputStream in “标准”输入流。 static PrintStream out “标准”输出流。 static void setIn(InputStream in) 重新分配“标准”输入流。 static void setOut(PrintStream out) 重新分配“标准”输出流。 System.setIn(new FileInputStream(&#34;1.txt&#34;)); //将源改成文件1.txt System.setOut(new PrintStream(&#34;2.txt&#34;)); //将目的改成文件2.txt long currentTimeMillis();获取当前时间的毫秒值，可以通过此方法检测程序的执行时间。
Properties getProperties();确定当前的系统属性。
Properties集合中存储的都是String类型的键和值。
Set stringPropertyNames() 返回此属性列表中的键集，其中该键及其对应值是字符串，如果在主属性列表中未找到同名的键，则还包括默认属性列表中不同的键。
Runtimie类 每个 Java 应用程序都有一个 Runtime 类实例，使应用程序能够与其运行的环境相连接。应用程序不能创建自己的 Runtime 类实例。
特点 没有构造方法摘要，说明该类不可以创建对象。又发现还有非静态的方法，说明该类应该提供静态的返回该类对象的方法。而且只有一个，说明Runtime类使用了单例设计模式。 常用方法 static Runtime getRuntime() 返回与当前 Java 应用程序相关的运行时对象。 Process类：ProcessBuilder.start() 和 Runtime.exec 方法创建一个本机进程，并返回 Process 子类的一个实例，该实例可用来控制进程并获得相关信息。
Process exec(String command) 在单独的进程中执行指定的字符串命令。 //用记事本打开SystemDemo.java Runtime r = Runtime.getRuntime(); Process p = r.exec(&ldquo;notepad.exe SystemDemo.java&rdquo;);
Math类 提供了操作数学运算的方法，都是静态的。
常用方法： ceil():返回大于参数的最小整数。 floor():返回小于参数的最大整数。 round():返回四舍五入的整数。 pow(a,b):a的b次方。 static double random() 返回带正号的 double 值，该值大于等于 0.0 且小于 1.0。 Radom类（位于java.util）int nextInt(int n) 返回一个伪随机数，它是取自此随机数生成器序列的,在 0（包括）和指定值（不包括）之间均匀分布的 int 值。
//打印1到10的随机数 Random r = new Random(); for(int x=0; x&lt;10; x++) { //int d = (int)(Math.random()*10+1); int d = r.nextInt(10)+1; System.out.println(d); } Date、DateForm类 类 Date 表示特定的瞬间，精确到毫秒。 日期对象和毫秒值之间的转换 毫秒值**&gt;日期对象： 通过Date对象的构造方法 new Date(timeMillis); 还可以通过setTime设置。因为可以通过Date对象的方法对该日期中的各个字段（年月日等）进行操作。 日期对象&ndash;&gt;毫秒值： long getTime():返回自 1970 年 1 月 1 日 00:00:00 GMT 以来此 Date 对象表示的毫秒数。 对日期对象进行格式化： 将日期对象&ndash;&gt;日期格式的字符串。
使用的是DateFormat类（位于java.text包）中的format方法。 Date d =new Date(); //将模式封装到SimpleDateformat对象中。 SimpleDateFormat sdf = new SimpleDateFormat(&#34;yyyy年MM月dd日 E hh:mm:ss&#34;); //调用format方法让模式格式化指定Date对象 String time = sdf.format(d); System.out.println(&#34;time=&#34;+time); 将日期格式的字符串&ndash;&gt;日期对象:
使用DateFormat类中的prase方法。 Calendar类 该类是一个抽象类，它为特定瞬间与一组诸如 YEAR、MONTH、DAY_OF_MONTH、HOUR 等 日历字段之间的转换提供了一些方法，并为操作日历字段（例如获得下星期的日期）提供了一些方法。瞬间可用毫秒值来表示，它是距历元（即格林威治标准时间 1970 年 1 月 1 日的 00:00:00.000，格里高利历）的偏移量。
基本方法 static Calendar getInstance() 使用默认时区和语言环境获得一个日历。 int get(int field) 返回给定日历字段的值。 void set(int year, int month, int date) 设置日历字段 YEAR、MONTH 和 DAY_OF_MONTH 的值。 abstract void add(int field, int amount) 根据日历的规则，为给定的日历字段添加或减去指定的时间量。 例如，要从当前日历时间减去 5 天:add(Calendar.DAY_OF_MONTH, *5);
  ]]></content></entry><entry><title>关于</title><url>/about/</url><categories/><tags/><content type="html"> 博客 最初于2015年在CSDN采用MarkDown格式编写博客, 后续使用Hexo在GitHub Pages上搭建静态博客, 试用一段时间WordPress, 然后迁移到简洁的typecho, 后面基于halo搭建, 先迁移到 Hugo。域名来自万网。
关于我 曾Android开发者, Web后端开发者, 前端新手, Python数分爱好者, 现专注于大数据平台的开发及其数据应用。 爱生活, 爱折腾!</content></entry></search>