<!doctype html><html lang=zh-CN data-theme=dark><head><meta charset=UTF-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.121.1"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="机器学习中的评估指标"><meta itemprop=description content="evaluation-of-machinelearning"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="/img/avatar.jpg"><meta itemprop=keywords content="机器学习"><meta property="og:type" content="article"><meta property="og:title" content="机器学习中的评估指标"><meta property="og:description" content="evaluation-of-machinelearning"><meta property="og:image" content="/img/avatar.jpg"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="/post/evaluation-of-machinelearning/"><meta property="og:site_name" content="Patrick's Blog"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="Patrick"><meta property="article:published_time" content="2019-09-08 22:48:09 +0800 CST"><meta property="article:modified_time" content="2024-04-04 11:29:37 +0800 CST"><link type=text/css rel=stylesheet href=https://unpkg.com/@fortawesome/fontawesome-free@6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://unpkg.com/animate.css@3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://unpkg.com/viewerjs@1.11.0/dist/viewer.min.css><link rel=stylesheet href=/css/main.min.16446798d1ac22e627bb485e0651e58b209fd2ea8ae486329a10a0db9d0fff07.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":true,"isHome":false,"isPage":true,"math":{"css":{"file":"dist/katex.min.css","name":"katex","version":"0.16.0"},"js":[{"file":"dist/katex.min.js","name":"katex","version":"0.16.0"},{"alias_name":"katex","file":"dist/contrib/auto-render.min.js","name":"auto-render","version":"0.16.0"}],"render":"katex"},"path":"evaluation-of-machinelearning","permalink":"/post/evaluation-of-machinelearning/","title":"机器学习中的评估指标","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>机器学习中的评估指标 - Patrick's Blog</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Patrick's Blog</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Once start, goes forward!</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about/ class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档
<span class=badge>33</span></a></li><li class="menu-item menu-item-commonweal"><a href=/404.html class=hvr-icon-pulse rel=section><i class="fa fa-heartbeat hvr-icon"></i>公益 404</a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#分类问题>分类问题</a><ul><li><a href=#混淆矩阵confusion-matrix>混淆矩阵（Confusion Matrix）</a><ul><li><a href=#sklearn相应的包>sklearn相应的包</a></li></ul></li><li><a href=#准确率accuracy>准确率(Accuracy)</a><ul><li><a href=#sklearn相应的包-1>sklearn相应的包</a></li></ul></li><li><a href=#精确率precision-预测正确的正样本占所有预测为正样本的比例>精确率(Precision): 预测正确的正样本占所有预测为正样本的比例</a><ul><li><a href=#sklearn相应的包-2>sklearn相应的包</a></li></ul></li><li><a href=#召回率-recall-预测正确的正样本占所有正样本比例>召回率 (Recall): 预测正确的正样本占所有正样本比例</a><ul><li><a href=#sklearn>sklearn</a></li></ul></li><li><a href=#f1-score>F1 score</a><ul><li><a href=#sklearn相应的包-3>sklearn相应的包</a></li></ul></li><li><a href=#增益gain和提升lift图>增益（Gain）和提升（Lift）图</a></li><li><a href=#roc曲线>ROC曲线</a><ul><li><a href=#sklearn相应的包-4>sklearn相应的包</a></li></ul></li><li><a href=#aucarea-under-curve>AUC（Area Under Curve）</a><ul><li><a href=#sklearn相应的包-5>sklearn相应的包</a></li></ul></li><li><a href=#pr曲线>PR曲线</a><ul><li><a href=#曲线绘制>曲线绘制</a></li></ul></li><li><a href=#多分类>多分类</a></li></ul></li><li><a href=#回归问题>回归问题</a><ul><li><a href=#平均绝对误差mae>平均绝对误差（MAE）</a><ul><li><a href=#sklearn相应包>sklearn相应包</a></li></ul></li><li><a href=#平均平方误差mse>平均平方误差（MSE）</a><ul><li><a href=#sklearn相应包-1>sklearn相应包</a></li></ul></li><li><a href=#均方根误差rmse>均方根误差（RMSE）</a></li><li><a href=#均方对数误差msle>均方对数误差(MSLE)</a><ul><li><a href=#sklearn对应包>sklearn对应包</a></li></ul></li><li><a href=#中值绝对误差medae>中值绝对误差(MedAE)</a><ul><li><a href=#sklearn对应包-1>sklearn对应包</a></li></ul></li><li><a href=#可释方差得分-evs>可释方差得分 (EVS)</a><ul><li><a href=#sklearn相关包>sklearn相关包</a></li></ul></li><li><a href=#决定系数coefficient-of-determination>决定系数(Coefficient of Determination)</a><ul><li><a href=#sklearn相关包-1>sklearn相关包</a></li></ul></li></ul></li><li><a href=#聚类问题>聚类问题</a><ul><li><a href=#外部指标>外部指标</a><ul><li><a href=#常用外部指标>常用外部指标</a></li><li><a href=#互信息mutual-information>互信息（Mutual Information)</a></li><li><a href=#sklearn相应包-2>sklearn相应包</a></li></ul></li><li><a href=#内部指标>内部指标</a><ul><li><a href=#常用内部指标>常用内部指标</a></li><li><a href=#轮廓系数silhouette-coefficient>轮廓系数（Silhouette coefficient）</a></li><li><a href=#sklearn相应包-3>sklearn相应包</a></li></ul></li></ul></li><li><a href=#关联问题>关联问题</a><ul><li><a href=#支持度support>支持度(Support)</a></li><li><a href=#置信度confidence>置信度(Confidence)</a></li><li><a href=#提升度>提升度</a></li></ul></li><li><a href=#参考>参考</a></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=Patrick src=/imgs/img-lazy-loading.gif data-src=/img/avatar.jpg><p class=site-author-name itemprop=name>Patrick</p><div class=site-description itemprop=description>个人学习笔记</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>33</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>4</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>8</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/img-lazy-loading.gif data-src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>
友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://gitee.com/hugo-next/hugo-theme-next title=https://gitee.com/hugo-next/hugo-theme-next target=_blank>Hugo-NexT</a></li></ul></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate=2015-04-05T13:57:58+08:00></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>总访客数：</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>页面浏览：</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=22332></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=121></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate=2023-05-31T21:26:21+08:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=goto-comments class="button goto-comments" title=直达评论><i class="fas fa-comments"></i></div><div id=goto-gtranslate class=button title=多语言翻译><i class="fas fa-globe"></i></div><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><a role=button class="book-mark-link book-mark-link-fixed"></a><script type=text/javascript src=//sidecar.gitter.im/dist/sidecar.v1.js async></script><script type=text/javascript>((window.gitter={}).chat={}).options={room:"hugo-next/community"}</script><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=/post/evaluation-of-machinelearning/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/img/avatar.jpg"><meta itemprop=name content="Patrick"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="Patrick"><meta itemprop=description content="个人学习笔记"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="机器学习中的评估指标"><meta itemprop=description content="evaluation-of-machinelearning"></span><header class=post-header><h1 class=post-title itemprop="name headline">机器学习中的评估指标</h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i>
</span><span class=post-meta-item-text title=发表于>发表于：
</span><time title="创建时间：2019-09-08 22:48:09 +0800 CST" itemprop="dateCreated datePublished" datetime="2019-09-08 22:48:09 +0800 CST">2019-09-08
</time></span><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar-check"></i>
</span><span class=post-meta-item-text title=更新于>更新于：
</span><time title=修改时间：2024-04-04T11:29:37+08:00 itemprop=dateModified datetime=2024-04-04T11:29:37+08:00>2024-04-04</time>
</span><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i>
</span><span class=post-meta-item-text title=分类于>分类于：
</span><span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6 itemprop=url rel=index><span itemprop=name>数据科学</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i>
</span><span class=post-meta-item-text>字数：</span>
<span>848</span>
</span><span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i>
</span><span class=post-meta-item-text>阅读：&ap;</span>
<span>4分钟</span>
</span><span class=post-meta-item title=浏览><span class=post-meta-item-icon><i class="far fa-eye"></i>
</span><span class=post-meta-item-text>浏览：
</span><span id=busuanzi_value_page_pv data-path=/post/evaluation-of-machinelearning/><i class="fa fa-sync fa-spin"></i>
</span></span><span class=post-meta-item title><span class=post-meta-item-icon><i class="far fa-comments"></i>
</span><span class=post-meta-item-text title=评论>评论：
</span><span id=comments-count class=waline-comment-count data-path=/post/evaluation-of-machinelearning/><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class=post-body itemprop=articleBody><h2 id=分类问题>分类问题
<a class=header-anchor href=#%e5%88%86%e7%b1%bb%e9%97%ae%e9%a2%98></a></h2><ul><li>当正负样本分布极不均衡时, 准确率将失去意义, 通常使用AUC作为指标</li></ul><h3 id=混淆矩阵confusion-matrix>混淆矩阵（Confusion Matrix）
<a class=header-anchor href=#%e6%b7%b7%e6%b7%86%e7%9f%a9%e9%98%b5confusion-matrix></a></h3><ul><li>矩阵中的每一行代表实例的预测类别，每一列代表实例的真实类别。
<img src=/imgs/img-lazy-loading.gif data-src=/img/confusion_matrix.png alt=img></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>真正(True Positive , TP)：被模型预测为正的正样本。
</span></span><span style=display:flex><span>假正(False Positive , FP)：被模型预测为正的负样本。
</span></span><span style=display:flex><span>假负(False Negative , FN)：被模型预测为负的正样本。
</span></span><span style=display:flex><span>真负(True Negative , TN)：被模型预测为负的负样本。
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>真正率(True Positive Rate,TPR)：TPR=TP/(TP+FN)，即被预测为正的正样本数 /正样本实际数。 召回率
</span></span><span style=display:flex><span>假正率(False Positive Rate,FPR) ：FPR=FP/(FP+TN)，即被预测为正的负样本数 /负样本实际数。
</span></span><span style=display:flex><span>假负率(False Negative Rate,FNR) ：FNR=FN/(TP+FN)，即被预测为负的正样本数 /正样本实际数。
</span></span><span style=display:flex><span>真负率(True Negative Rate,TNR)：TNR=TN/(TN+FP)，即被预测为负的负样本数 /负样本实际数/2
</span></span></code></pre></div><h4 id=sklearn相应的包>sklearn相应的包
<a class=header-anchor href=#sklearn%e7%9b%b8%e5%ba%94%e7%9a%84%e5%8c%85></a></h4><ul><li><code>sklearn.metrics.confusion_matrix</code></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> confusion_matrix
</span></span><span style=display:flex><span><span style=color:#75715e># y_pred是预测标签</span>
</span></span><span style=display:flex><span>y_pred, y_true <span style=color:#f92672>=</span>[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>], [<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>confusion_matrix(y_true<span style=color:#f92672>=</span>y_true, y_pred<span style=color:#f92672>=</span>y_pred)
</span></span><span style=display:flex><span><span style=color:#75715e># array([[2, 1],</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#        [0, 1]], dtype=int64)</span>
</span></span></code></pre></div><h3 id=准确率accuracy>准确率(Accuracy)
<a class=header-anchor href=#%e5%87%86%e7%a1%ae%e7%8e%87accuracy></a></h3><ul><li>分类正确的样本个数占总样本的比例.</li></ul><h4 id=sklearn相应的包-1>sklearn相应的包
<a class=header-anchor href=#sklearn%e7%9b%b8%e5%ba%94%e7%9a%84%e5%8c%85-1></a></h4><ul><li><code>sklearn.metrics.accuracy_score</code></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> accuracy_score
</span></span><span style=display:flex><span><span style=color:#75715e># y_pred是预测标签</span>
</span></span><span style=display:flex><span>y_pred, y_true<span style=color:#f92672>=</span>[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>], [<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>]
</span></span><span style=display:flex><span>accuracy_score(y_true<span style=color:#f92672>=</span>y_true, y_pred<span style=color:#f92672>=</span>y_pred)
</span></span><span style=display:flex><span><span style=color:#75715e># 0.75</span>
</span></span></code></pre></div><h3 id=精确率precision-预测正确的正样本占所有预测为正样本的比例>精确率(Precision): 预测正确的正样本占所有预测为正样本的比例
<a class=header-anchor href=#%e7%b2%be%e7%a1%ae%e7%8e%87precision-%e9%a2%84%e6%b5%8b%e6%ad%a3%e7%a1%ae%e7%9a%84%e6%ad%a3%e6%a0%b7%e6%9c%ac%e5%8d%a0%e6%89%80%e6%9c%89%e9%a2%84%e6%b5%8b%e4%b8%ba%e6%ad%a3%e6%a0%b7%e6%9c%ac%e7%9a%84%e6%af%94%e4%be%8b></a></h3><ul><li>所有分正确的正样本/所有预测为正类的样本数.
$$Precision = \frac{TP}{TP + FP}$$</li><li>也叫查准率</li></ul><h4 id=sklearn相应的包-2>sklearn相应的包
<a class=header-anchor href=#sklearn%e7%9b%b8%e5%ba%94%e7%9a%84%e5%8c%85-2></a></h4><ul><li><code>sklearn.metrics.precision_score</code></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> precision_score
</span></span><span style=display:flex><span><span style=color:#75715e># y_pred是预测标签</span>
</span></span><span style=display:flex><span>y_pred, y_true <span style=color:#f92672>=</span>[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>], [<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>precision_score(y_true<span style=color:#f92672>=</span>y_true, y_pred<span style=color:#f92672>=</span>y_pred)
</span></span><span style=display:flex><span><span style=color:#75715e># 0.5</span>
</span></span></code></pre></div><h3 id=召回率-recall-预测正确的正样本占所有正样本比例>召回率 (Recall): 预测正确的正样本占所有正样本比例
<a class=header-anchor href=#%e5%8f%ac%e5%9b%9e%e7%8e%87-recall-%e9%a2%84%e6%b5%8b%e6%ad%a3%e7%a1%ae%e7%9a%84%e6%ad%a3%e6%a0%b7%e6%9c%ac%e5%8d%a0%e6%89%80%e6%9c%89%e6%ad%a3%e6%a0%b7%e6%9c%ac%e6%af%94%e4%be%8b></a></h3><ul><li>所有分正确的正样本/所有的正样本数.
$$Recall = \frac{TP}{TP + FN}$$</li><li>也叫查全率</li></ul><h4 id=sklearn>sklearn
<a class=header-anchor href=#sklearn></a></h4><ul><li><code>sklearn.metrics.recall_score</code></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> recall_score
</span></span><span style=display:flex><span><span style=color:#75715e># y_pred是预测标签</span>
</span></span><span style=display:flex><span>y_pred, y_true <span style=color:#f92672>=</span>[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>], [<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>recall_score(y_true<span style=color:#f92672>=</span>y_true, y_pred<span style=color:#f92672>=</span>y_pred)
</span></span><span style=display:flex><span><span style=color:#75715e># 1.0</span>
</span></span></code></pre></div><h3 id=f1-score>F1 score
<a class=header-anchor href=#f1-score></a></h3><ul><li>又称平衡分数, 定义为精确率和召回率的调和平均数
$$F_1 \ score = \frac{2 * Precision * Recall}{Precision + Recall}$$</li></ul><h4 id=sklearn相应的包-3>sklearn相应的包
<a class=header-anchor href=#sklearn%e7%9b%b8%e5%ba%94%e7%9a%84%e5%8c%85-3></a></h4><ul><li><code>sklearn.metrics.f1_score</code></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> f1_score
</span></span><span style=display:flex><span><span style=color:#75715e># y_pred是预测标签</span>
</span></span><span style=display:flex><span>y_pred, y_true <span style=color:#f92672>=</span>[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>], [<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>f1_score(y_true<span style=color:#f92672>=</span>y_true, y_pred<span style=color:#f92672>=</span>y_pred)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># classification_report可以直接输出各个类的precision recall f1-score support</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> classification_report
</span></span><span style=display:flex><span><span style=color:#75715e># y_pred是预测标签</span>
</span></span><span style=display:flex><span>y_pred, y_true <span style=color:#f92672>=</span>[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>], [<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>print(classification_report(y_true<span style=color:#f92672>=</span>y_true, y_pred<span style=color:#f92672>=</span>y_pred))
</span></span></code></pre></div><h3 id=增益gain和提升lift图>增益（Gain）和提升（Lift）图
<a class=header-anchor href=#%e5%a2%9e%e7%9b%8again%e5%92%8c%e6%8f%90%e5%8d%87lift%e5%9b%be></a></h3><h3 id=roc曲线>ROC曲线
<a class=header-anchor href=#roc%e6%9b%b2%e7%ba%bf></a></h3><ul><li>横轴: 负正类率(false postive rate FPR=FP/(FP+TN))特异度, 划分实例中所有负例占所有负例的比例；(1-Specificity)</li><li>纵轴: 真正类率(true postive rate TPR=TP/(TP+FN))灵敏度, Sensitivity(正类覆盖率), 即召回率</li></ul><h4 id=sklearn相应的包-4>sklearn相应的包
<a class=header-anchor href=#sklearn%e7%9b%b8%e5%ba%94%e7%9a%84%e5%8c%85-4></a></h4><ul><li><code>sklearn.metrics.roc_curve</code>, <code>sklearn.metrics.auc</code></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt 
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> roc_curve, auc
</span></span><span style=display:flex><span><span style=color:#75715e># y_test：实际的标签, dataset_pred：预测的概率值。</span>
</span></span><span style=display:flex><span>fpr, tpr, thresholds <span style=color:#f92672>=</span> roc_curve(y_test, dataset_pred)
</span></span><span style=display:flex><span>roc_auc <span style=color:#f92672>=</span> auc(fpr, tpr)  
</span></span><span style=display:flex><span><span style=color:#75715e>#画图，只需要plt.plot(fpr,tpr),变量roc_auc只是记录auc的值，通过auc()函数能计算出来  </span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(fpr, tpr, lw<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;ROC(area = </span><span style=color:#e6db74>%0.2f</span><span style=color:#e6db74>)&#39;</span> <span style=color:#f92672>%</span> (roc_auc))
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#34;FPR (False Positive Rate)&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#34;TPR (True Positive Rate)&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;Receiver Operating Characteristic, ROC(AUC = </span><span style=color:#e6db74>%0.2f</span><span style=color:#e6db74>)&#34;</span><span style=color:#f92672>%</span> (roc_auc))
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><h3 id=aucarea-under-curve>AUC（Area Under Curve）
<a class=header-anchor href=#aucarea-under-curve></a></h3><ul><li>AUC即为ROC曲线下的面积(ROC的积分), 通常大于0.5小于1.</li><li>AUC值(面积)越大的分类器，性能越好.</li></ul><h4 id=sklearn相应的包-5>sklearn相应的包
<a class=header-anchor href=#sklearn%e7%9b%b8%e5%ba%94%e7%9a%84%e5%8c%85-5></a></h4><ul><li><code>sklearn.metrics.roc_auc_score</code></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> roc_auc_score
</span></span><span style=display:flex><span><span style=color:#75715e># y_test：实际的标签, dataset_pred：预测的概率值。</span>
</span></span><span style=display:flex><span>roc_auc_score(y_test, dataset_pred)
</span></span></code></pre></div><h3 id=pr曲线>PR曲线
<a class=header-anchor href=#pr%e6%9b%b2%e7%ba%bf></a></h3><ul><li>横坐标: 精确率P</li><li>纵坐标: 召回率R</li><li>评价标准和ROC一样，先看平滑不平滑（蓝线明显好些）。一般来说，在同一测试集，位于上面的线比下面的好.</li><li>当P和R的值接近时，F1值最大.</li></ul><h4 id=曲线绘制>曲线绘制
<a class=header-anchor href=#%e6%9b%b2%e7%ba%bf%e7%bb%98%e5%88%b6></a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>%</span>matplotlib inline
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> seaborn <span style=color:#66d9ef>as</span> sns
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> precision_recall_fscore_support <span style=color:#66d9ef>as</span> prfs
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>evaluate_pr</span>(res, inf<span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span>, sup<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>):
</span></span><span style=display:flex><span>    y_vali <span style=color:#f92672>=</span> res<span style=color:#f92672>.</span>copy()
</span></span><span style=display:flex><span>    pres <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    recs <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    thrs <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> thred <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>arange(inf, sup, <span style=color:#ae81ff>0.01</span>):
</span></span><span style=display:flex><span>        y_vali[<span style=color:#e6db74>&#39;pred&#39;</span>] <span style=color:#f92672>=</span> y_vali[<span style=color:#e6db74>&#39;proba&#39;</span>] <span style=color:#f92672>&gt;</span> thred
</span></span><span style=display:flex><span>        p, r, f, _ <span style=color:#f92672>=</span> prfs(y_vali[<span style=color:#e6db74>&#39;real&#39;</span>], y_vali[<span style=color:#e6db74>&#39;pred&#39;</span>], average<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;binary&#39;</span>)
</span></span><span style=display:flex><span>        pres<span style=color:#f92672>.</span>append(p)
</span></span><span style=display:flex><span>        recs<span style=color:#f92672>.</span>append(r)
</span></span><span style=display:flex><span>        thrs<span style=color:#f92672>.</span>append(thred)
</span></span><span style=display:flex><span>    sns<span style=color:#f92672>.</span>lineplot(pres, recs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>proba <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict_proba(X)[:,<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>res <span style=color:#f92672>=</span> input_train[[]]<span style=color:#f92672>.</span>copy()
</span></span><span style=display:flex><span>res[<span style=color:#e6db74>&#39;real&#39;</span>] <span style=color:#f92672>=</span> y
</span></span><span style=display:flex><span>res[<span style=color:#e6db74>&#39;proba&#39;</span>] <span style=color:#f92672>=</span> proba
</span></span><span style=display:flex><span>evaluate_pr(res)
</span></span></code></pre></div><h3 id=多分类>多分类
<a class=header-anchor href=#%e5%a4%9a%e5%88%86%e7%b1%bb></a></h3><ul><li><code>precision_recall_fscore_support</code>: 计算每个分类的precision, recall, fscore和support</li></ul><h2 id=回归问题>回归问题
<a class=header-anchor href=#%e5%9b%9e%e5%bd%92%e9%97%ae%e9%a2%98></a></h2><ul><li>在sklearn中, 通常函数以<code>_score</code>结尾返回一个值来最大化, 越高越好; 函数 <code>_error</code>或<code>_loss</code>结尾返回一个值来 minimize（最小化）, 越低越好.</li></ul><h3 id=平均绝对误差mae>平均绝对误差（MAE）
<a class=header-anchor href=#%e5%b9%b3%e5%9d%87%e7%bb%9d%e5%af%b9%e8%af%af%e5%b7%aemae></a></h3><ul><li>平均绝对误差MAE（Mean Absolute Error）又被称为<code>l1</code></li><li>平均绝对误差是非负值，模型越好MAE越接近零.</li><li>公式
$$MAE = \frac{1}{m}\sum_{i=0}^m|y_i - \hat{y}_i|$$</li></ul><h4 id=sklearn相应包>sklearn相应包
<a class=header-anchor href=#sklearn%e7%9b%b8%e5%ba%94%e5%8c%85></a></h4><ul><li><code>sklearn.metrics.mean_absolute_error</code></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> mean_absolute_error
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>y_true, y_pred <span style=color:#f92672>=</span> [<span style=color:#ae81ff>3</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>7</span>], [<span style=color:#ae81ff>2.5</span>, <span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>8</span>]
</span></span><span style=display:flex><span>mean_absolute_error(y_true, y_pred)
</span></span><span style=display:flex><span><span style=color:#75715e># 0.5</span>
</span></span></code></pre></div><h3 id=平均平方误差mse>平均平方误差（MSE）
<a class=header-anchor href=#%e5%b9%b3%e5%9d%87%e5%b9%b3%e6%96%b9%e8%af%af%e5%b7%aemse></a></h3><ul><li>平均平方误差MSE（Mean Squared Error）又被称为<code>l2</code></li><li>本质是在残差平方和(RSS)的基础上除以了样本总量，得到了每个样本量上的平均误差.</li><li>均方误差是非负值，模型越好MSE越接近零.</li><li>公式
$$MSE = \frac{1}{m}\sum_{i=0}^m(y_i - \hat{y}_i)^2$$</li></ul><h4 id=sklearn相应包-1>sklearn相应包
<a class=header-anchor href=#sklearn%e7%9b%b8%e5%ba%94%e5%8c%85-1></a></h4><ul><li><code>sklearn.metrics.mean_squared_error</code></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> mean_squared_error
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>y_true, y_pred <span style=color:#f92672>=</span> [<span style=color:#ae81ff>3</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>7</span>], [<span style=color:#ae81ff>2.5</span>, <span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>8</span>]
</span></span><span style=display:flex><span>mse <span style=color:#f92672>=</span> mean_squared_error(y_true, y_pred)
</span></span><span style=display:flex><span><span style=color:#75715e># 0.375</span>
</span></span><span style=display:flex><span>rmse <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sqrt(mse)
</span></span><span style=display:flex><span><span style=color:#75715e># 0.6123724356957945</span>
</span></span></code></pre></div><h3 id=均方根误差rmse>均方根误差（RMSE）
<a class=header-anchor href=#%e5%9d%87%e6%96%b9%e6%a0%b9%e8%af%af%e5%b7%aermse></a></h3><ul><li>均方根误差RMSE (Root Mean Squared Errort), 即MSE开方.</li><li>公式
$$RMSE = \sqrt{\frac{1}{m}\sum_{i=0}^m(y_i - \hat{y}_i)^2}$$</li></ul><h3 id=均方对数误差msle>均方对数误差(MSLE)
<a class=header-anchor href=#%e5%9d%87%e6%96%b9%e5%af%b9%e6%95%b0%e8%af%af%e5%b7%aemsle></a></h3><ul><li>均方对数误差MSLE (mean squared logarithmic error)</li><li>均方对数误差是非负值，模型越好MSLE越接近零.</li><li>公式
$$MSLE = \frac{1}{m}\sum_{i=1}^m(log(y_i + 1) - log(\hat{y_i + 1})^2$$</li></ul><h4 id=sklearn对应包>sklearn对应包
<a class=header-anchor href=#sklearn%e5%af%b9%e5%ba%94%e5%8c%85></a></h4><ul><li><code>sklearn.metrics.mean_squared_log_error</code></li></ul><h3 id=中值绝对误差medae>中值绝对误差(MedAE)
<a class=header-anchor href=#%e4%b8%ad%e5%80%bc%e7%bb%9d%e5%af%b9%e8%af%af%e5%b7%aemedae></a></h3><ul><li>中值绝对误差MedAE（median absolute error）</li><li>中值绝对误差是非负值，模型越好MSE越接近零.</li><li>公式
$$MedAE = median(|y_i - \hat{y}_i|, \cdots, |y_m - \hat{y}_m|)$$</li></ul><h4 id=sklearn对应包-1>sklearn对应包
<a class=header-anchor href=#sklearn%e5%af%b9%e5%ba%94%e5%8c%85-1></a></h4><ul><li><code>sklearn.metrics.mean_squared_log_error</code></li></ul><h3 id=可释方差得分-evs>可释方差得分 (EVS)
<a class=header-anchor href=#%e5%8f%af%e9%87%8a%e6%96%b9%e5%b7%ae%e5%be%97%e5%88%86-evs></a></h3><ul><li>解释变异（ Explained variance）是根据误差的方差计算得到.</li><li>最佳模型的可释方差分数值为1，模型越差值越小.</li><li>公式:
$$EVS = 1 - \frac{var(y_i - \hat{y_i})}{var(y_i)}$$</li></ul><h4 id=sklearn相关包>sklearn相关包
<a class=header-anchor href=#sklearn%e7%9b%b8%e5%85%b3%e5%8c%85></a></h4><ul><li><code>sklearn.metrics.explained_variance_score</code></li></ul><h3 id=决定系数coefficient-of-determination>决定系数(Coefficient of Determination)
<a class=header-anchor href=#%e5%86%b3%e5%ae%9a%e7%b3%bb%e6%95%b0coefficient-of-determination></a></h3><ul><li>R2 决定系数(r2_score) 判断回归方程的拟合程度.</li><li>最佳模型的R^{2}决定系数分数值为1，常数模型值为0，模型越差值越小.</li><li>公式
$$
R^2 = 1 - \frac{\sum_{i=0}^m(y_i - \hat{y}<em>i)^2}{\sum</em>{i=0}^m(y_i - \bar{y}<em>i)^2} = 1 - \frac{RSS}{\sum</em>{i=0}^m(y_i - \bar{y}_i)^2}
$$</li></ul><h4 id=sklearn相关包-1>sklearn相关包
<a class=header-anchor href=#sklearn%e7%9b%b8%e5%85%b3%e5%8c%85-1></a></h4><ul><li><code>sklearn.metrics.r2_score</code></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> r2_score
</span></span><span style=display:flex><span>y_true, y_pred <span style=color:#f92672>=</span> [<span style=color:#ae81ff>3</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>7</span>], [<span style=color:#ae81ff>2.5</span>, <span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>8</span>]
</span></span><span style=display:flex><span>r2_score(y_true, y_pred)
</span></span></code></pre></div><h2 id=聚类问题>聚类问题
<a class=header-anchor href=#%e8%81%9a%e7%b1%bb%e9%97%ae%e9%a2%98></a></h2><ul><li>聚类结果, 追求"簇内相似度"(intra-cluster similarity)高, 且"簇间相似度"(inter-cluster similarity)低.</li><li>聚类性能度量大致有两类:<ul><li>将聚类结果与某个"参考模型"(reference model)进行比较, 称为"外部指标"(external index).</li><li>直接参考聚类结果而不利用任何参考模型, 称为"内部指标"(internal index).</li></ul></li></ul><h3 id=外部指标>外部指标
<a class=header-anchor href=#%e5%a4%96%e9%83%a8%e6%8c%87%e6%a0%87></a></h3><ul><li>对数据集$D = {x_1, x_2, \cdots, x_m}$, 假定通过聚类给出的簇划分为$C = {C_1, C_2, \cdots, C_k }$, 参考模型给出的簇划分为$<code>$C^* = \{C_1^*, C_2^*, \cdots, C_s^*\}$</code>$. 相应地, 令$\lambda$与$<code>$\lambda^*$</code>$分别表示与$C$和$<code>$C^*$</code>$对应的簇标记向量, 将样本两两配对考虑, 定义</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>$$
</span></span><span style=display:flex><span>  a = |SS|, SS = \{(x_i,x_j) | \lambda_i = \lambda_j, \lambda_i^* = \lambda_j*, i &lt; j \} \\
</span></span><span style=display:flex><span>  b = |SD|, SD = \{(x_i,x_j) | \lambda_i = \lambda_j, \lambda_i* \ne \lambda_j^*, i &lt; j \} \\
</span></span><span style=display:flex><span>  c = |DS|, DS = \{(x_i,x_j) | \lambda_i \ne \lambda_j, \lambda_i^* = \lambda_j^*, i &lt; j \} \\
</span></span><span style=display:flex><span>  d = |DD|, DD = \{(x_i,x_j) | \lambda_i \ne \lambda_j, \lambda_i^* \ne \lambda_j^*, i &lt; j \}
</span></span><span style=display:flex><span>  $$
</span></span></code></pre></div><ul><li>其中<ul><li>集合SS: 包含了在$C$中属于相同簇, 同时在$C^*$中也属于相同簇的样本对</li><li>集合SD: 包含了在$C$中属于相同簇, 同时在$C^*$中属于不同簇的样本对</li><li>集合DS: 包含了在$C$中属于不同簇, 同时在$C^*$中属于相同簇的样本对</li><li>集合DD: 包含了在$C$中属于不同簇, 同时在$C^*$中属于不同簇的样本对</li></ul></li><li>由于每个样本对$(x_i, x_j) (i &lt; j)$仅能出现在一个集合中, 因此有$a + b + c + d = m(m - 1)/2$</li></ul><h4 id=常用外部指标>常用外部指标
<a class=header-anchor href=#%e5%b8%b8%e7%94%a8%e5%a4%96%e9%83%a8%e6%8c%87%e6%a0%87></a></h4><ul><li>Jacccard系数(Jaccard Coeffient, 简称JC)
$$JC = \frac{a}{a + b + c}$$</li><li>FM指数(Fowlkes and Mallows Index, 简称FMI)
$$FMI = \sqrt{\frac{a}{a + b} \cdot \frac{a}{a + c}}$$</li><li>Rand指数(Rand Index, 简称RI)
$$RI = \frac{2(a + d)}{m(m-1)}$$</li><li>上述性能指数的结果值均在$[0, 1]$区间, 值越大越好.</li></ul><h4 id=互信息mutual-information>互信息（Mutual Information)
<a class=header-anchor href=#%e4%ba%92%e4%bf%a1%e6%81%afmutual-information></a></h4><ul><li>两个随机变量的互信息（Mutual Information，简称MI）或转移信息（transinformation）是变量间相互依赖性的量度.</li></ul><h4 id=sklearn相应包-2>sklearn相应包
<a class=header-anchor href=#sklearn%e7%9b%b8%e5%ba%94%e5%8c%85-2></a></h4><ul><li>FMI: <code>fowlkes_mallows_score</code></li><li>RI: <code>sklearn.metrics.adjusted_rand_score</code></li><li>MI: <code>sklearn.metrics.adjusted_mutual_info_score</code></li></ul><h3 id=内部指标>内部指标
<a class=header-anchor href=#%e5%86%85%e9%83%a8%e6%8c%87%e6%a0%87></a></h3><ul><li>考虑聚类结果的簇划分$C = {C_1, C_2, \cdots, C_k}$, 有以下定义<ul><li>$avg(C)$为簇$C$内样本间的平均距离</li><li>$diam(C)$对应簇内样本间的最远距离</li><li>$d_{min}(C_i, C_j)$对应于簇$C_i$与簇$C_j$最近样本间的距离</li><li>$d_{cen(C_i, C_j)}$对应簇$C_i$与簇$C_j$中心点的距离</li></ul></li></ul><h4 id=常用内部指标>常用内部指标
<a class=header-anchor href=#%e5%b8%b8%e7%94%a8%e5%86%85%e9%83%a8%e6%8c%87%e6%a0%87></a></h4><ul><li>DB指数(Davies-Bouldin Index, 简称DBI)
$$DBI = \frac{1}{k}\sum_{i=1}^k\max_{ j\ne i}(\frac{avg(C_i) + avg(C_j)}{d_{cen}(C_i, C_j)})$$
DBI的可能最小值为0, 值越小越好.</li><li>Dunn指数(Dunn Index, 简称DI)
$$DI = \min_{1 \le i \le k}{\min_{j \ne i}(\frac{d_{min}(C_i,C_j)}{\max_{1 \le l \le k} diam(C_l)})}$$
DI值越大越好</li></ul><h4 id=轮廓系数silhouette-coefficient>轮廓系数（Silhouette coefficient）
<a class=header-anchor href=#%e8%bd%ae%e5%bb%93%e7%b3%bb%e6%95%b0silhouette-coefficient></a></h4><ul><li>结合了聚类的凝聚度（Cohesion）和分离度（Separation）, 用于评估聚类的效果。该值处于(-1,1)之间.</li><li>其中值越接近1表示样本与自己所在的簇中的样本很相似，并且与其他簇中的样本不相似;当样本点与簇外的样本更相似的时候，轮廓系数就为负; 当轮廓系数为0时, 则代表两个簇中的样本相 似度一致，两个簇本应该是一个簇.</li><li>公式
$$s(i) = \frac{b(i) - a(i)}{max{a(i) , b(i)}}$$
a(i)为样本i与簇内其它样本的平均距离, b(i)为样本i与其它某簇样本的平均距离, 多个簇b(i)取最小.</li></ul><h4 id=sklearn相应包-3>sklearn相应包
<a class=header-anchor href=#sklearn%e7%9b%b8%e5%ba%94%e5%8c%85-3></a></h4><ul><li>DBI: <code>sklearn.metrics.davies_bouldin_score</code></li><li><code>sklearn.metrics.silhouette_score</code>, 返回是一个数据集中, 所有样本的轮廓系数均值.</li><li><code>sklearn.metrics.silhouette_score_samples</code>，它的参数与轮廓系数一致，但返回的是数据集中每个样本自己的轮廓系数.</li></ul><h2 id=关联问题>关联问题
<a class=header-anchor href=#%e5%85%b3%e8%81%94%e9%97%ae%e9%a2%98></a></h2><ul><li>假设$I={I_{1},I_{2},\ldots ,I_{m}}$, 是项的集合。给定一个交易数据库$D={t_{1},t_{2},\ldots ,t_{n}}$，其中每个事务（Transaction）t是I的非空子集，即$t\subseteq I$，每一个交易都与一个唯一的标识符TID（Transaction ID）对应。关联规则是形如$X \Rightarrow Y$的蕴涵式，其中$X,Y\subseteq I$且$X\cap Y=\emptyset$ ， X和Y分别称为关联规则的先导(antecedent或left-hand-side, LHS)和后继(consequent或right-hand-side, RHS) 。关联规则$X\Rightarrow Y$在D中的支持度（support）是D中事务包含$X\cup Y$的百分比，即概率$P(X\cup Y)$；包含X的事务中同时包含Y的百分比，即条件概率$P\left(Y|X\right)$。</li></ul><h3 id=支持度support>支持度(Support)
<a class=header-anchor href=#%e6%94%af%e6%8c%81%e5%ba%a6support></a></h3><ul><li>表示项目X, Y同时在总数据集中出现的概率, 其计算公式为
$$support(X => Y) = \frac{T(X \cup Y)}{N}$$
指D中N个交易记录中同时出现X和Y的交易记录所占的比例.</li></ul><h3 id=置信度confidence>置信度(Confidence)
<a class=header-anchor href=#%e7%bd%ae%e4%bf%a1%e5%ba%a6confidence></a></h3><ul><li>指在先导项X已经发生的情况下, 后续项Y也发生的概率, 即包含X的交易记录中同时也包含Y的交易记录所占的比例, 计算公式为:
$$confidence(X => Y) = \frac{support(X \cup Y)}{support(X)}$$</li></ul><h3 id=提升度>提升度
<a class=header-anchor href=#%e6%8f%90%e5%8d%87%e5%ba%a6></a></h3><ul><li>表示含有X的条件下同时含有Y的概率, 与无论含不含X, 含有Y的概率之比, 计算公式
$$confidence(X => Y) /support(Y)$$
购买X的情况下, 购买Y的概率大于购买Y的概率, 则具有提升作用.</li></ul><h2 id=参考>参考
<a class=header-anchor href=#%e5%8f%82%e8%80%83></a></h2><ul><li><a href=# title="机器学习 周志华著">机器学习 周志华著</a></li><li><a href=https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation title=model-evaluation rel="noopener external nofollow noreferrer" target=_blank class=exturl>model-evaluation
<i class="fa fa-external-link-alt"></i></a></li></ul></div><footer class=post-footer><div class=post-tags><a href=/tags/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0>机器学习</a></div><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-copyright><img src=/imgs/cc/cc.svg width=75 height=75 align=right alt=共享知识><ul><li class=post-copyright-title><strong>文章标题：</strong>
机器学习中的评估指标</li><li class=post-copyright-author><strong>本文作者： </strong>Patrick</li><li class=post-copyright-link><strong>本文链接：</strong>
<a id=post-cr-link href=/post/evaluation-of-machinelearning/ title=机器学习中的评估指标>/post/evaluation-of-machinelearning/</a></li><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <i class="fab fa-fw fa-creative-commons"></i><a target=_blank href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/post/featureenginering-of-machinelearning/ rel=next title=机器学习中的特征工程><i class="fa fa-chevron-left"></i> 机器学习中的特征工程</a></div><div class="post-nav-prev post-nav-item"><a href=/post/the-modeling-of-data-warehouse/ rel=prev title=数据仓库建模分层设计>数据仓库建模分层设计
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div id=comments class=post-comments><div class=comment-head><div class=comment-headline><i class="fas fa-comments fa-fw"></i>
<span>评论交流</span></div></div><div class=comment-wrap><div><div class=comment-loading><i class="fa fa-sync fa-spin"></i></div><div class=waline-container></div></div></div></div></div></main><footer class=footer><div class=footer-inner><div id=gtranslate class=google-translate><i class="fa fa-language"></i><div id=google_translate_element></div></div><div class=copyright>&copy;
<span itemprop=copyrightYear>2015 - 2024
</span><span class=with-love><i class="fa fa-heart"></i>
</span><span class=author itemprop=copyrightHolder>Patrick</span></div><div class=powered-by>由 <a href=https://gohugo.io title=0.121.1 target=_blank>Hugo</a> & <a href=https://github.com/hugo-next/hugo-theme-next title=4.5.3 target=_blank>Hugo NexT.Gemini</a> 强力驱动</div></div></footer><script type=text/javascript src=https://unpkg.com/animejs@3.2.1/lib/anime.min.js defer></script><script type=text/javascript src=https://unpkg.com/viewerjs@1.11.0/dist/viewer.min.js defer></script><script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":true,"save":"manual"},"copybtn":true,"darkmode":true,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"找到 ${hits} 个搜索结果","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"limit":1e3,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"postmeta":{"comments":{"enable":true,"plugin":"waline"},"views":{"enable":true,"plugin":"busuanzi"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"vendor":{"plugins":"unpkg","router":"https://unpkg.com"},"version":"4.5.3","waline":{"cfg":{"emoji":false,"imguploader":false,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":"cmts.yaoyuehome.com","sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.min.b0c78e5a4df586ee46d02716ffa91a4a322e56763e04e91eb2ba052c9469ed02.js defer></script><script type=text/javascript src=/js/math.min.a6ada19a368d85dad9ead2040d86ae561a867fafef89391d1aa2aa5909366509.js defer></script></body></html>